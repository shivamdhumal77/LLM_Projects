{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac5eebaf-b784-4311-9d24-8e6a9f96bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916dfaef-79e8-45e0-bbfc-16b816d8f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, seq_len, vocab):\n",
    "        self.text = text\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.encoded_text = [vocab[char] for char in text if char in vocab]\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_text) - self.seq_len\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.encoded_text[idx:idx + self.seq_len]\n",
    "        y = self.encoded_text[idx + 1:idx + self.seq_len + 1]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871a66e1-49d7-411b-959c-fa36e2f581e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhuma\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [10/48549], Loss: 3.0485\n",
      "Epoch [1/1], Step [20/48549], Loss: 3.1149\n",
      "Epoch [1/1], Step [30/48549], Loss: 2.8286\n",
      "Epoch [1/1], Step [40/48549], Loss: 2.6576\n",
      "Epoch [1/1], Step [50/48549], Loss: 2.6248\n",
      "Epoch [1/1], Step [60/48549], Loss: 2.6381\n",
      "Epoch [1/1], Step [70/48549], Loss: 2.4880\n",
      "Epoch [1/1], Step [80/48549], Loss: 2.4899\n",
      "Epoch [1/1], Step [90/48549], Loss: 2.5834\n",
      "Epoch [1/1], Step [100/48549], Loss: 2.5388\n",
      "Epoch [1/1], Step [110/48549], Loss: 2.5618\n",
      "Epoch [1/1], Step [120/48549], Loss: 2.7388\n",
      "Epoch [1/1], Step [130/48549], Loss: 2.4991\n",
      "Epoch [1/1], Step [140/48549], Loss: 2.5322\n",
      "Epoch [1/1], Step [150/48549], Loss: 2.4920\n",
      "Epoch [1/1], Step [160/48549], Loss: 2.4629\n",
      "Epoch [1/1], Step [170/48549], Loss: 2.5954\n",
      "Epoch [1/1], Step [180/48549], Loss: 2.4765\n",
      "Epoch [1/1], Step [190/48549], Loss: 2.6072\n",
      "Epoch [1/1], Step [200/48549], Loss: 2.4847\n",
      "Epoch [1/1], Step [210/48549], Loss: 2.5383\n",
      "Epoch [1/1], Step [220/48549], Loss: 2.5423\n",
      "Epoch [1/1], Step [230/48549], Loss: 2.4787\n",
      "Epoch [1/1], Step [240/48549], Loss: 2.5002\n",
      "Epoch [1/1], Step [250/48549], Loss: 2.4703\n",
      "Epoch [1/1], Step [260/48549], Loss: 2.5139\n",
      "Epoch [1/1], Step [270/48549], Loss: 2.5046\n",
      "Epoch [1/1], Step [280/48549], Loss: 2.5181\n",
      "Epoch [1/1], Step [290/48549], Loss: 2.4932\n",
      "Epoch [1/1], Step [300/48549], Loss: 2.4835\n",
      "Epoch [1/1], Step [310/48549], Loss: 2.5242\n",
      "Epoch [1/1], Step [320/48549], Loss: 2.4371\n",
      "Epoch [1/1], Step [330/48549], Loss: 2.4717\n",
      "Epoch [1/1], Step [340/48549], Loss: 2.5151\n",
      "Epoch [1/1], Step [350/48549], Loss: 2.5221\n",
      "Epoch [1/1], Step [360/48549], Loss: 2.5002\n",
      "Epoch [1/1], Step [370/48549], Loss: 2.5387\n",
      "Epoch [1/1], Step [380/48549], Loss: 2.4712\n",
      "Epoch [1/1], Step [390/48549], Loss: 2.5071\n",
      "Epoch [1/1], Step [400/48549], Loss: 2.4757\n",
      "Epoch [1/1], Step [410/48549], Loss: 2.4854\n",
      "Epoch [1/1], Step [420/48549], Loss: 2.5082\n",
      "Epoch [1/1], Step [430/48549], Loss: 2.4388\n",
      "Epoch [1/1], Step [440/48549], Loss: 2.4954\n",
      "Epoch [1/1], Step [450/48549], Loss: 2.4736\n",
      "Epoch [1/1], Step [460/48549], Loss: 2.4628\n",
      "Epoch [1/1], Step [470/48549], Loss: 2.4588\n",
      "Epoch [1/1], Step [480/48549], Loss: 2.4093\n",
      "Epoch [1/1], Step [490/48549], Loss: 2.5256\n",
      "Epoch [1/1], Step [500/48549], Loss: 2.4679\n",
      "Epoch [1/1], Step [510/48549], Loss: 2.4980\n",
      "Epoch [1/1], Step [520/48549], Loss: 2.5478\n",
      "Epoch [1/1], Step [530/48549], Loss: 2.5121\n",
      "Epoch [1/1], Step [540/48549], Loss: 2.5387\n",
      "Epoch [1/1], Step [550/48549], Loss: 2.4517\n",
      "Epoch [1/1], Step [560/48549], Loss: 2.5016\n",
      "Epoch [1/1], Step [570/48549], Loss: 2.4054\n",
      "Epoch [1/1], Step [580/48549], Loss: 2.4488\n",
      "Epoch [1/1], Step [590/48549], Loss: 2.4546\n",
      "Epoch [1/1], Step [600/48549], Loss: 2.7125\n",
      "Epoch [1/1], Step [610/48549], Loss: 2.4798\n",
      "Epoch [1/1], Step [620/48549], Loss: 2.4702\n",
      "Epoch [1/1], Step [630/48549], Loss: 2.4098\n",
      "Epoch [1/1], Step [640/48549], Loss: 2.4365\n",
      "Epoch [1/1], Step [650/48549], Loss: 2.5334\n",
      "Epoch [1/1], Step [660/48549], Loss: 2.4626\n",
      "Epoch [1/1], Step [670/48549], Loss: 2.5799\n",
      "Epoch [1/1], Step [680/48549], Loss: 2.4985\n",
      "Epoch [1/1], Step [690/48549], Loss: 2.4394\n",
      "Epoch [1/1], Step [700/48549], Loss: 2.4381\n",
      "Epoch [1/1], Step [710/48549], Loss: 2.4483\n",
      "Epoch [1/1], Step [720/48549], Loss: 2.5141\n",
      "Epoch [1/1], Step [730/48549], Loss: 2.4689\n",
      "Epoch [1/1], Step [740/48549], Loss: 2.4417\n",
      "Epoch [1/1], Step [750/48549], Loss: 2.4760\n",
      "Epoch [1/1], Step [760/48549], Loss: 2.5291\n",
      "Epoch [1/1], Step [770/48549], Loss: 2.5052\n",
      "Epoch [1/1], Step [780/48549], Loss: 2.4911\n",
      "Epoch [1/1], Step [790/48549], Loss: 2.4540\n",
      "Epoch [1/1], Step [800/48549], Loss: 2.4697\n",
      "Epoch [1/1], Step [810/48549], Loss: 2.4092\n",
      "Epoch [1/1], Step [820/48549], Loss: 2.4543\n",
      "Epoch [1/1], Step [830/48549], Loss: 2.4298\n",
      "Epoch [1/1], Step [840/48549], Loss: 2.4691\n",
      "Epoch [1/1], Step [850/48549], Loss: 2.5055\n",
      "Epoch [1/1], Step [860/48549], Loss: 2.4516\n",
      "Epoch [1/1], Step [870/48549], Loss: 2.4648\n",
      "Epoch [1/1], Step [880/48549], Loss: 2.4811\n",
      "Epoch [1/1], Step [890/48549], Loss: 2.4599\n",
      "Epoch [1/1], Step [900/48549], Loss: 2.4439\n",
      "Epoch [1/1], Step [910/48549], Loss: 2.4985\n",
      "Epoch [1/1], Step [920/48549], Loss: 2.4435\n",
      "Epoch [1/1], Step [930/48549], Loss: 2.4985\n",
      "Epoch [1/1], Step [940/48549], Loss: 2.3994\n",
      "Epoch [1/1], Step [950/48549], Loss: 2.4874\n",
      "Epoch [1/1], Step [960/48549], Loss: 2.4100\n",
      "Epoch [1/1], Step [970/48549], Loss: 2.5508\n",
      "Epoch [1/1], Step [980/48549], Loss: 2.4531\n",
      "Epoch [1/1], Step [990/48549], Loss: 2.4466\n",
      "Epoch [1/1], Step [1000/48549], Loss: 2.4374\n",
      "Epoch [1/1], Step [1010/48549], Loss: 2.4654\n",
      "Epoch [1/1], Step [1020/48549], Loss: 2.4198\n",
      "Epoch [1/1], Step [1030/48549], Loss: 2.5349\n",
      "Epoch [1/1], Step [1040/48549], Loss: 2.3850\n",
      "Epoch [1/1], Step [1050/48549], Loss: 2.4017\n",
      "Epoch [1/1], Step [1060/48549], Loss: 2.4519\n",
      "Epoch [1/1], Step [1070/48549], Loss: 2.5096\n",
      "Epoch [1/1], Step [1080/48549], Loss: 2.5130\n",
      "Epoch [1/1], Step [1090/48549], Loss: 2.4325\n",
      "Epoch [1/1], Step [1100/48549], Loss: 2.4652\n",
      "Epoch [1/1], Step [1110/48549], Loss: 2.4929\n",
      "Epoch [1/1], Step [1120/48549], Loss: 2.6229\n",
      "Epoch [1/1], Step [1130/48549], Loss: 2.4365\n",
      "Epoch [1/1], Step [1140/48549], Loss: 2.4705\n",
      "Epoch [1/1], Step [1150/48549], Loss: 2.4303\n",
      "Epoch [1/1], Step [1160/48549], Loss: 2.5100\n",
      "Epoch [1/1], Step [1170/48549], Loss: 2.4598\n",
      "Epoch [1/1], Step [1180/48549], Loss: 2.4555\n",
      "Epoch [1/1], Step [1190/48549], Loss: 2.4877\n",
      "Epoch [1/1], Step [1200/48549], Loss: 2.4995\n",
      "Epoch [1/1], Step [1210/48549], Loss: 2.4385\n",
      "Epoch [1/1], Step [1220/48549], Loss: 2.5392\n",
      "Epoch [1/1], Step [1230/48549], Loss: 2.4842\n",
      "Epoch [1/1], Step [1240/48549], Loss: 2.4174\n",
      "Epoch [1/1], Step [1250/48549], Loss: 2.4432\n",
      "Epoch [1/1], Step [1260/48549], Loss: 2.4812\n",
      "Epoch [1/1], Step [1270/48549], Loss: 2.4195\n",
      "Epoch [1/1], Step [1280/48549], Loss: 2.4548\n",
      "Epoch [1/1], Step [1290/48549], Loss: 2.4927\n",
      "Epoch [1/1], Step [1300/48549], Loss: 2.5000\n",
      "Epoch [1/1], Step [1310/48549], Loss: 2.4335\n",
      "Epoch [1/1], Step [1320/48549], Loss: 2.4429\n",
      "Epoch [1/1], Step [1330/48549], Loss: 2.5321\n",
      "Epoch [1/1], Step [1340/48549], Loss: 2.4523\n",
      "Epoch [1/1], Step [1350/48549], Loss: 2.4914\n",
      "Epoch [1/1], Step [1360/48549], Loss: 2.4530\n",
      "Epoch [1/1], Step [1370/48549], Loss: 2.5795\n",
      "Epoch [1/1], Step [1380/48549], Loss: 2.4150\n",
      "Epoch [1/1], Step [1390/48549], Loss: 2.4772\n",
      "Epoch [1/1], Step [1400/48549], Loss: 2.5104\n",
      "Epoch [1/1], Step [1410/48549], Loss: 2.4792\n",
      "Epoch [1/1], Step [1420/48549], Loss: 2.4758\n",
      "Epoch [1/1], Step [1430/48549], Loss: 2.4968\n",
      "Epoch [1/1], Step [1440/48549], Loss: 2.4427\n",
      "Epoch [1/1], Step [1450/48549], Loss: 2.4837\n",
      "Epoch [1/1], Step [1460/48549], Loss: 2.5268\n",
      "Epoch [1/1], Step [1470/48549], Loss: 2.4796\n",
      "Epoch [1/1], Step [1480/48549], Loss: 2.4461\n",
      "Epoch [1/1], Step [1490/48549], Loss: 2.4593\n",
      "Epoch [1/1], Step [1500/48549], Loss: 2.4355\n",
      "Epoch [1/1], Step [1510/48549], Loss: 2.4598\n",
      "Epoch [1/1], Step [1520/48549], Loss: 2.4836\n",
      "Epoch [1/1], Step [1530/48549], Loss: 2.4533\n",
      "Epoch [1/1], Step [1540/48549], Loss: 2.4718\n",
      "Epoch [1/1], Step [1550/48549], Loss: 2.4780\n",
      "Epoch [1/1], Step [1560/48549], Loss: 2.4465\n",
      "Epoch [1/1], Step [1570/48549], Loss: 2.4941\n",
      "Epoch [1/1], Step [1580/48549], Loss: 2.4755\n",
      "Epoch [1/1], Step [1590/48549], Loss: 2.5517\n",
      "Epoch [1/1], Step [1600/48549], Loss: 2.4739\n",
      "Epoch [1/1], Step [1610/48549], Loss: 2.4298\n",
      "Epoch [1/1], Step [1620/48549], Loss: 2.4302\n",
      "Epoch [1/1], Step [1630/48549], Loss: 2.4495\n",
      "Epoch [1/1], Step [1640/48549], Loss: 2.4556\n",
      "Epoch [1/1], Step [1650/48549], Loss: 2.4800\n",
      "Epoch [1/1], Step [1660/48549], Loss: 2.5083\n",
      "Epoch [1/1], Step [1670/48549], Loss: 2.5155\n",
      "Epoch [1/1], Step [1680/48549], Loss: 2.4450\n",
      "Epoch [1/1], Step [1690/48549], Loss: 2.4726\n",
      "Epoch [1/1], Step [1700/48549], Loss: 2.3979\n",
      "Epoch [1/1], Step [1710/48549], Loss: 2.4798\n",
      "Epoch [1/1], Step [1720/48549], Loss: 2.4380\n",
      "Epoch [1/1], Step [1730/48549], Loss: 2.4739\n",
      "Epoch [1/1], Step [1740/48549], Loss: 2.4492\n",
      "Epoch [1/1], Step [1750/48549], Loss: 2.4172\n",
      "Epoch [1/1], Step [1760/48549], Loss: 2.4537\n",
      "Epoch [1/1], Step [1770/48549], Loss: 2.4569\n",
      "Epoch [1/1], Step [1780/48549], Loss: 2.4285\n",
      "Epoch [1/1], Step [1790/48549], Loss: 2.4376\n",
      "Epoch [1/1], Step [1800/48549], Loss: 2.4968\n",
      "Epoch [1/1], Step [1810/48549], Loss: 2.4325\n",
      "Epoch [1/1], Step [1820/48549], Loss: 2.4204\n",
      "Epoch [1/1], Step [1830/48549], Loss: 2.4669\n",
      "Epoch [1/1], Step [1840/48549], Loss: 2.4772\n",
      "Epoch [1/1], Step [1850/48549], Loss: 2.4763\n",
      "Epoch [1/1], Step [1860/48549], Loss: 2.4736\n",
      "Epoch [1/1], Step [1870/48549], Loss: 2.6432\n",
      "Epoch [1/1], Step [1880/48549], Loss: 2.4291\n",
      "Epoch [1/1], Step [1890/48549], Loss: 2.4015\n",
      "Epoch [1/1], Step [1900/48549], Loss: 2.4865\n",
      "Epoch [1/1], Step [1910/48549], Loss: 2.4576\n",
      "Epoch [1/1], Step [1920/48549], Loss: 2.4827\n",
      "Epoch [1/1], Step [1930/48549], Loss: 2.4736\n",
      "Epoch [1/1], Step [1940/48549], Loss: 2.4209\n",
      "Epoch [1/1], Step [1950/48549], Loss: 2.4573\n",
      "Epoch [1/1], Step [1960/48549], Loss: 2.4014\n",
      "Epoch [1/1], Step [1970/48549], Loss: 2.4594\n",
      "Epoch [1/1], Step [1980/48549], Loss: 2.4565\n",
      "Epoch [1/1], Step [1990/48549], Loss: 2.5009\n",
      "Epoch [1/1], Step [2000/48549], Loss: 2.5045\n",
      "Epoch [1/1], Step [2010/48549], Loss: 2.4721\n",
      "Epoch [1/1], Step [2020/48549], Loss: 2.4244\n",
      "Epoch [1/1], Step [2030/48549], Loss: 2.4241\n",
      "Epoch [1/1], Step [2040/48549], Loss: 2.4548\n",
      "Epoch [1/1], Step [2050/48549], Loss: 2.4537\n",
      "Epoch [1/1], Step [2060/48549], Loss: 2.4315\n",
      "Epoch [1/1], Step [2070/48549], Loss: 2.5046\n",
      "Epoch [1/1], Step [2080/48549], Loss: 2.4746\n",
      "Epoch [1/1], Step [2090/48549], Loss: 2.4560\n",
      "Epoch [1/1], Step [2100/48549], Loss: 2.5611\n",
      "Epoch [1/1], Step [2110/48549], Loss: 2.4701\n",
      "Epoch [1/1], Step [2120/48549], Loss: 2.5007\n",
      "Epoch [1/1], Step [2130/48549], Loss: 2.4374\n",
      "Epoch [1/1], Step [2140/48549], Loss: 2.5416\n",
      "Epoch [1/1], Step [2150/48549], Loss: 2.4707\n",
      "Epoch [1/1], Step [2160/48549], Loss: 2.4588\n",
      "Epoch [1/1], Step [2170/48549], Loss: 2.4023\n",
      "Epoch [1/1], Step [2180/48549], Loss: 2.4753\n",
      "Epoch [1/1], Step [2190/48549], Loss: 2.4626\n",
      "Epoch [1/1], Step [2200/48549], Loss: 2.4566\n",
      "Epoch [1/1], Step [2210/48549], Loss: 2.4272\n",
      "Epoch [1/1], Step [2220/48549], Loss: 2.4629\n",
      "Epoch [1/1], Step [2230/48549], Loss: 2.5820\n",
      "Epoch [1/1], Step [2240/48549], Loss: 2.4343\n",
      "Epoch [1/1], Step [2250/48549], Loss: 2.4200\n",
      "Epoch [1/1], Step [2260/48549], Loss: 2.4384\n",
      "Epoch [1/1], Step [2270/48549], Loss: 2.5342\n",
      "Epoch [1/1], Step [2280/48549], Loss: 2.4799\n",
      "Epoch [1/1], Step [2290/48549], Loss: 2.4724\n",
      "Epoch [1/1], Step [2300/48549], Loss: 2.4714\n",
      "Epoch [1/1], Step [2310/48549], Loss: 2.4612\n",
      "Epoch [1/1], Step [2320/48549], Loss: 2.3975\n",
      "Epoch [1/1], Step [2330/48549], Loss: 2.4154\n",
      "Epoch [1/1], Step [2340/48549], Loss: 2.4669\n",
      "Epoch [1/1], Step [2350/48549], Loss: 2.4812\n",
      "Epoch [1/1], Step [2360/48549], Loss: 2.4529\n",
      "Epoch [1/1], Step [2370/48549], Loss: 2.4100\n",
      "Epoch [1/1], Step [2380/48549], Loss: 2.4838\n",
      "Epoch [1/1], Step [2390/48549], Loss: 2.4326\n",
      "Epoch [1/1], Step [2400/48549], Loss: 2.4344\n",
      "Epoch [1/1], Step [2410/48549], Loss: 2.5195\n",
      "Epoch [1/1], Step [2420/48549], Loss: 2.4691\n",
      "Epoch [1/1], Step [2430/48549], Loss: 2.3712\n",
      "Epoch [1/1], Step [2440/48549], Loss: 2.4404\n",
      "Epoch [1/1], Step [2450/48549], Loss: 2.4039\n",
      "Epoch [1/1], Step [2460/48549], Loss: 2.4792\n",
      "Epoch [1/1], Step [2470/48549], Loss: 2.4380\n",
      "Epoch [1/1], Step [2480/48549], Loss: 2.4322\n",
      "Epoch [1/1], Step [2490/48549], Loss: 2.4972\n",
      "Epoch [1/1], Step [2500/48549], Loss: 2.4629\n",
      "Epoch [1/1], Step [2510/48549], Loss: 2.4348\n",
      "Epoch [1/1], Step [2520/48549], Loss: 2.5179\n",
      "Epoch [1/1], Step [2530/48549], Loss: 2.4076\n",
      "Epoch [1/1], Step [2540/48549], Loss: 2.5692\n",
      "Epoch [1/1], Step [2550/48549], Loss: 2.4497\n",
      "Epoch [1/1], Step [2560/48549], Loss: 2.5006\n",
      "Epoch [1/1], Step [2570/48549], Loss: 2.4538\n",
      "Epoch [1/1], Step [2580/48549], Loss: 2.4697\n",
      "Epoch [1/1], Step [2590/48549], Loss: 2.4427\n",
      "Epoch [1/1], Step [2600/48549], Loss: 2.5137\n",
      "Epoch [1/1], Step [2610/48549], Loss: 2.5419\n",
      "Epoch [1/1], Step [2620/48549], Loss: 2.4628\n",
      "Epoch [1/1], Step [2630/48549], Loss: 2.4165\n",
      "Epoch [1/1], Step [2640/48549], Loss: 2.4435\n",
      "Epoch [1/1], Step [2650/48549], Loss: 2.4273\n",
      "Epoch [1/1], Step [2660/48549], Loss: 2.4247\n",
      "Epoch [1/1], Step [2670/48549], Loss: 2.4726\n",
      "Epoch [1/1], Step [2680/48549], Loss: 2.5238\n",
      "Epoch [1/1], Step [2690/48549], Loss: 2.6095\n",
      "Epoch [1/1], Step [2700/48549], Loss: 2.5048\n",
      "Epoch [1/1], Step [2710/48549], Loss: 2.4497\n",
      "Epoch [1/1], Step [2720/48549], Loss: 2.4718\n",
      "Epoch [1/1], Step [2730/48549], Loss: 2.4322\n",
      "Epoch [1/1], Step [2740/48549], Loss: 2.4496\n",
      "Epoch [1/1], Step [2750/48549], Loss: 2.4450\n",
      "Epoch [1/1], Step [2760/48549], Loss: 2.4346\n",
      "Epoch [1/1], Step [2770/48549], Loss: 2.4673\n",
      "Epoch [1/1], Step [2780/48549], Loss: 2.4231\n",
      "Epoch [1/1], Step [2790/48549], Loss: 2.4377\n",
      "Epoch [1/1], Step [2800/48549], Loss: 2.4815\n",
      "Epoch [1/1], Step [2810/48549], Loss: 2.4910\n",
      "Epoch [1/1], Step [2820/48549], Loss: 2.4648\n",
      "Epoch [1/1], Step [2830/48549], Loss: 2.4787\n",
      "Epoch [1/1], Step [2840/48549], Loss: 2.4411\n",
      "Epoch [1/1], Step [2850/48549], Loss: 2.4324\n",
      "Epoch [1/1], Step [2860/48549], Loss: 2.4611\n",
      "Epoch [1/1], Step [2870/48549], Loss: 2.5360\n",
      "Epoch [1/1], Step [2880/48549], Loss: 2.5275\n",
      "Epoch [1/1], Step [2890/48549], Loss: 2.4817\n",
      "Epoch [1/1], Step [2900/48549], Loss: 2.4292\n",
      "Epoch [1/1], Step [2910/48549], Loss: 2.5087\n",
      "Epoch [1/1], Step [2920/48549], Loss: 2.4425\n",
      "Epoch [1/1], Step [2930/48549], Loss: 2.4456\n",
      "Epoch [1/1], Step [2940/48549], Loss: 2.4726\n",
      "Epoch [1/1], Step [2950/48549], Loss: 2.4385\n",
      "Epoch [1/1], Step [2960/48549], Loss: 2.4955\n",
      "Epoch [1/1], Step [2970/48549], Loss: 2.4449\n",
      "Epoch [1/1], Step [2980/48549], Loss: 2.4137\n",
      "Epoch [1/1], Step [2990/48549], Loss: 2.4615\n",
      "Epoch [1/1], Step [3000/48549], Loss: 2.4321\n",
      "Epoch [1/1], Step [3010/48549], Loss: 2.4350\n",
      "Epoch [1/1], Step [3020/48549], Loss: 2.4299\n",
      "Epoch [1/1], Step [3030/48549], Loss: 2.5936\n",
      "Epoch [1/1], Step [3040/48549], Loss: 2.5013\n",
      "Epoch [1/1], Step [3050/48549], Loss: 2.4114\n",
      "Epoch [1/1], Step [3060/48549], Loss: 2.4462\n",
      "Epoch [1/1], Step [3070/48549], Loss: 2.4552\n",
      "Epoch [1/1], Step [3080/48549], Loss: 2.4355\n",
      "Epoch [1/1], Step [3090/48549], Loss: 2.4417\n",
      "Epoch [1/1], Step [3100/48549], Loss: 2.4895\n",
      "Epoch [1/1], Step [3110/48549], Loss: 2.5314\n",
      "Epoch [1/1], Step [3120/48549], Loss: 2.4153\n",
      "Epoch [1/1], Step [3130/48549], Loss: 2.4752\n",
      "Epoch [1/1], Step [3140/48549], Loss: 2.3739\n",
      "Epoch [1/1], Step [3150/48549], Loss: 2.4444\n",
      "Epoch [1/1], Step [3160/48549], Loss: 2.4774\n",
      "Epoch [1/1], Step [3170/48549], Loss: 2.5178\n",
      "Epoch [1/1], Step [3180/48549], Loss: 2.4802\n",
      "Epoch [1/1], Step [3190/48549], Loss: 2.4137\n",
      "Epoch [1/1], Step [3200/48549], Loss: 2.4697\n",
      "Epoch [1/1], Step [3210/48549], Loss: 2.4754\n",
      "Epoch [1/1], Step [3220/48549], Loss: 2.4142\n",
      "Epoch [1/1], Step [3230/48549], Loss: 2.4503\n",
      "Epoch [1/1], Step [3240/48549], Loss: 2.4411\n",
      "Epoch [1/1], Step [3250/48549], Loss: 2.4838\n",
      "Epoch [1/1], Step [3260/48549], Loss: 2.4284\n",
      "Epoch [1/1], Step [3270/48549], Loss: 2.4527\n",
      "Epoch [1/1], Step [3280/48549], Loss: 2.4630\n",
      "Epoch [1/1], Step [3290/48549], Loss: 2.4769\n",
      "Epoch [1/1], Step [3300/48549], Loss: 2.4416\n",
      "Epoch [1/1], Step [3310/48549], Loss: 2.4446\n",
      "Epoch [1/1], Step [3320/48549], Loss: 2.4768\n",
      "Epoch [1/1], Step [3330/48549], Loss: 2.5014\n",
      "Epoch [1/1], Step [3340/48549], Loss: 2.6455\n",
      "Epoch [1/1], Step [3350/48549], Loss: 2.4639\n",
      "Epoch [1/1], Step [3360/48549], Loss: 2.4463\n",
      "Epoch [1/1], Step [3370/48549], Loss: 2.4628\n",
      "Epoch [1/1], Step [3380/48549], Loss: 2.4325\n",
      "Epoch [1/1], Step [3390/48549], Loss: 2.4318\n",
      "Epoch [1/1], Step [3400/48549], Loss: 2.4273\n",
      "Epoch [1/1], Step [3410/48549], Loss: 2.4523\n",
      "Epoch [1/1], Step [3420/48549], Loss: 2.4597\n",
      "Epoch [1/1], Step [3430/48549], Loss: 2.4892\n",
      "Epoch [1/1], Step [3440/48549], Loss: 2.4174\n",
      "Epoch [1/1], Step [3450/48549], Loss: 2.4362\n",
      "Epoch [1/1], Step [3460/48549], Loss: 2.4828\n",
      "Epoch [1/1], Step [3470/48549], Loss: 2.4855\n",
      "Epoch [1/1], Step [3480/48549], Loss: 2.4807\n",
      "Epoch [1/1], Step [3490/48549], Loss: 2.4304\n",
      "Epoch [1/1], Step [3500/48549], Loss: 2.5131\n",
      "Epoch [1/1], Step [3510/48549], Loss: 2.4799\n",
      "Epoch [1/1], Step [3520/48549], Loss: 2.4611\n",
      "Epoch [1/1], Step [3530/48549], Loss: 2.4420\n",
      "Epoch [1/1], Step [3540/48549], Loss: 2.4982\n",
      "Epoch [1/1], Step [3550/48549], Loss: 2.4275\n",
      "Epoch [1/1], Step [3560/48549], Loss: 2.5616\n",
      "Epoch [1/1], Step [3570/48549], Loss: 2.4469\n",
      "Epoch [1/1], Step [3580/48549], Loss: 2.5192\n",
      "Epoch [1/1], Step [3590/48549], Loss: 2.4569\n",
      "Epoch [1/1], Step [3600/48549], Loss: 2.5030\n",
      "Epoch [1/1], Step [3610/48549], Loss: 2.4015\n",
      "Epoch [1/1], Step [3620/48549], Loss: 2.4398\n",
      "Epoch [1/1], Step [3630/48549], Loss: 2.4232\n",
      "Epoch [1/1], Step [3640/48549], Loss: 2.4427\n",
      "Epoch [1/1], Step [3650/48549], Loss: 2.5335\n",
      "Epoch [1/1], Step [3660/48549], Loss: 2.4674\n",
      "Epoch [1/1], Step [3670/48549], Loss: 2.4514\n",
      "Epoch [1/1], Step [3680/48549], Loss: 2.4621\n",
      "Epoch [1/1], Step [3690/48549], Loss: 2.4764\n",
      "Epoch [1/1], Step [3700/48549], Loss: 2.4431\n",
      "Epoch [1/1], Step [3710/48549], Loss: 2.4233\n",
      "Epoch [1/1], Step [3720/48549], Loss: 2.4350\n",
      "Epoch [1/1], Step [3730/48549], Loss: 2.4438\n",
      "Epoch [1/1], Step [3740/48549], Loss: 2.4818\n",
      "Epoch [1/1], Step [3750/48549], Loss: 2.4486\n",
      "Epoch [1/1], Step [3760/48549], Loss: 2.4252\n",
      "Epoch [1/1], Step [3770/48549], Loss: 2.4528\n",
      "Epoch [1/1], Step [3780/48549], Loss: 2.4349\n",
      "Epoch [1/1], Step [3790/48549], Loss: 2.4515\n",
      "Epoch [1/1], Step [3800/48549], Loss: 2.4892\n",
      "Epoch [1/1], Step [3810/48549], Loss: 2.4626\n",
      "Epoch [1/1], Step [3820/48549], Loss: 2.5082\n",
      "Epoch [1/1], Step [3830/48549], Loss: 2.4569\n",
      "Epoch [1/1], Step [3840/48549], Loss: 2.4543\n",
      "Epoch [1/1], Step [3850/48549], Loss: 2.4841\n",
      "Epoch [1/1], Step [3860/48549], Loss: 2.4339\n",
      "Epoch [1/1], Step [3870/48549], Loss: 2.4578\n",
      "Epoch [1/1], Step [3880/48549], Loss: 2.5285\n",
      "Epoch [1/1], Step [3890/48549], Loss: 2.4842\n",
      "Epoch [1/1], Step [3900/48549], Loss: 2.4348\n",
      "Epoch [1/1], Step [3910/48549], Loss: 2.4021\n",
      "Epoch [1/1], Step [3920/48549], Loss: 2.4568\n",
      "Epoch [1/1], Step [3930/48549], Loss: 2.4695\n",
      "Epoch [1/1], Step [3940/48549], Loss: 2.4285\n",
      "Epoch [1/1], Step [3950/48549], Loss: 2.4407\n",
      "Epoch [1/1], Step [3960/48549], Loss: 2.4323\n",
      "Epoch [1/1], Step [3970/48549], Loss: 2.4970\n",
      "Epoch [1/1], Step [3980/48549], Loss: 2.4970\n",
      "Epoch [1/1], Step [3990/48549], Loss: 2.4362\n",
      "Epoch [1/1], Step [4000/48549], Loss: 2.4102\n",
      "Epoch [1/1], Step [4010/48549], Loss: 2.4080\n",
      "Epoch [1/1], Step [4020/48549], Loss: 2.4657\n",
      "Epoch [1/1], Step [4030/48549], Loss: 2.4436\n",
      "Epoch [1/1], Step [4040/48549], Loss: 2.4224\n",
      "Epoch [1/1], Step [4050/48549], Loss: 2.4767\n",
      "Epoch [1/1], Step [4060/48549], Loss: 2.4230\n",
      "Epoch [1/1], Step [4070/48549], Loss: 2.4918\n",
      "Epoch [1/1], Step [4080/48549], Loss: 2.4176\n",
      "Epoch [1/1], Step [4090/48549], Loss: 2.4791\n",
      "Epoch [1/1], Step [4100/48549], Loss: 2.5092\n",
      "Epoch [1/1], Step [4110/48549], Loss: 2.4507\n",
      "Epoch [1/1], Step [4120/48549], Loss: 2.4738\n",
      "Epoch [1/1], Step [4130/48549], Loss: 2.4597\n",
      "Epoch [1/1], Step [4140/48549], Loss: 2.4507\n",
      "Epoch [1/1], Step [4150/48549], Loss: 2.4712\n",
      "Epoch [1/1], Step [4160/48549], Loss: 2.4438\n",
      "Epoch [1/1], Step [4170/48549], Loss: 2.4375\n",
      "Epoch [1/1], Step [4180/48549], Loss: 2.5402\n",
      "Epoch [1/1], Step [4190/48549], Loss: 2.4700\n",
      "Epoch [1/1], Step [4200/48549], Loss: 2.4543\n",
      "Epoch [1/1], Step [4210/48549], Loss: 2.4720\n",
      "Epoch [1/1], Step [4220/48549], Loss: 2.4832\n",
      "Epoch [1/1], Step [4230/48549], Loss: 2.4293\n",
      "Epoch [1/1], Step [4240/48549], Loss: 2.4886\n",
      "Epoch [1/1], Step [4250/48549], Loss: 2.4633\n",
      "Epoch [1/1], Step [4260/48549], Loss: 2.4340\n",
      "Epoch [1/1], Step [4270/48549], Loss: 2.4409\n",
      "Epoch [1/1], Step [4280/48549], Loss: 2.4499\n",
      "Epoch [1/1], Step [4290/48549], Loss: 2.4172\n",
      "Epoch [1/1], Step [4300/48549], Loss: 2.5299\n",
      "Epoch [1/1], Step [4310/48549], Loss: 2.5656\n",
      "Epoch [1/1], Step [4320/48549], Loss: 2.4119\n",
      "Epoch [1/1], Step [4330/48549], Loss: 2.5741\n",
      "Epoch [1/1], Step [4340/48549], Loss: 2.4341\n",
      "Epoch [1/1], Step [4350/48549], Loss: 2.4291\n",
      "Epoch [1/1], Step [4360/48549], Loss: 2.4588\n",
      "Epoch [1/1], Step [4370/48549], Loss: 2.4820\n",
      "Epoch [1/1], Step [4380/48549], Loss: 2.4378\n",
      "Epoch [1/1], Step [4390/48549], Loss: 2.4423\n",
      "Epoch [1/1], Step [4400/48549], Loss: 2.4340\n",
      "Epoch [1/1], Step [4410/48549], Loss: 2.4096\n",
      "Epoch [1/1], Step [4420/48549], Loss: 2.4278\n",
      "Epoch [1/1], Step [4430/48549], Loss: 2.4486\n",
      "Epoch [1/1], Step [4440/48549], Loss: 2.4329\n",
      "Epoch [1/1], Step [4450/48549], Loss: 2.5964\n",
      "Epoch [1/1], Step [4460/48549], Loss: 2.4150\n",
      "Epoch [1/1], Step [4470/48549], Loss: 2.4362\n",
      "Epoch [1/1], Step [4480/48549], Loss: 2.4757\n",
      "Epoch [1/1], Step [4490/48549], Loss: 2.4604\n",
      "Epoch [1/1], Step [4500/48549], Loss: 2.3935\n",
      "Epoch [1/1], Step [4510/48549], Loss: 2.4396\n",
      "Epoch [1/1], Step [4520/48549], Loss: 2.4690\n",
      "Epoch [1/1], Step [4530/48549], Loss: 2.4081\n",
      "Epoch [1/1], Step [4540/48549], Loss: 2.4096\n",
      "Epoch [1/1], Step [4550/48549], Loss: 2.4476\n",
      "Epoch [1/1], Step [4560/48549], Loss: 2.5419\n",
      "Epoch [1/1], Step [4570/48549], Loss: 2.4544\n",
      "Epoch [1/1], Step [4580/48549], Loss: 2.4279\n",
      "Epoch [1/1], Step [4590/48549], Loss: 2.4368\n",
      "Epoch [1/1], Step [4600/48549], Loss: 2.4353\n",
      "Epoch [1/1], Step [4610/48549], Loss: 2.4686\n",
      "Epoch [1/1], Step [4620/48549], Loss: 2.5989\n",
      "Epoch [1/1], Step [4630/48549], Loss: 2.4310\n",
      "Epoch [1/1], Step [4640/48549], Loss: 2.4505\n",
      "Epoch [1/1], Step [4650/48549], Loss: 2.4168\n",
      "Epoch [1/1], Step [4660/48549], Loss: 2.4496\n",
      "Epoch [1/1], Step [4670/48549], Loss: 2.4461\n",
      "Epoch [1/1], Step [4680/48549], Loss: 2.4400\n",
      "Epoch [1/1], Step [4690/48549], Loss: 2.5696\n",
      "Epoch [1/1], Step [4700/48549], Loss: 2.4709\n",
      "Epoch [1/1], Step [4710/48549], Loss: 2.3901\n",
      "Epoch [1/1], Step [4720/48549], Loss: 2.5047\n",
      "Epoch [1/1], Step [4730/48549], Loss: 2.5840\n",
      "Epoch [1/1], Step [4740/48549], Loss: 2.5994\n",
      "Epoch [1/1], Step [4750/48549], Loss: 2.4234\n",
      "Epoch [1/1], Step [4760/48549], Loss: 2.4725\n",
      "Epoch [1/1], Step [4770/48549], Loss: 2.4394\n",
      "Epoch [1/1], Step [4780/48549], Loss: 2.3463\n",
      "Epoch [1/1], Step [4790/48549], Loss: 2.4131\n",
      "Epoch [1/1], Step [4800/48549], Loss: 2.4247\n",
      "Epoch [1/1], Step [4810/48549], Loss: 2.4720\n",
      "Epoch [1/1], Step [4820/48549], Loss: 2.4243\n",
      "Epoch [1/1], Step [4830/48549], Loss: 2.4456\n",
      "Epoch [1/1], Step [4840/48549], Loss: 2.4265\n",
      "Epoch [1/1], Step [4850/48549], Loss: 2.5198\n",
      "Epoch [1/1], Step [4860/48549], Loss: 2.4428\n",
      "Epoch [1/1], Step [4870/48549], Loss: 2.4127\n",
      "Epoch [1/1], Step [4880/48549], Loss: 2.4679\n",
      "Epoch [1/1], Step [4890/48549], Loss: 2.4665\n",
      "Epoch [1/1], Step [4900/48549], Loss: 2.4569\n",
      "Epoch [1/1], Step [4910/48549], Loss: 2.4484\n",
      "Epoch [1/1], Step [4920/48549], Loss: 2.5086\n",
      "Epoch [1/1], Step [4930/48549], Loss: 2.5498\n",
      "Epoch [1/1], Step [4940/48549], Loss: 2.4586\n",
      "Epoch [1/1], Step [4950/48549], Loss: 2.4523\n",
      "Epoch [1/1], Step [4960/48549], Loss: 2.4329\n",
      "Epoch [1/1], Step [4970/48549], Loss: 2.4806\n",
      "Epoch [1/1], Step [4980/48549], Loss: 2.4892\n",
      "Epoch [1/1], Step [4990/48549], Loss: 2.4193\n",
      "Epoch [1/1], Step [5000/48549], Loss: 2.5102\n",
      "Epoch [1/1], Step [5010/48549], Loss: 2.4436\n",
      "Epoch [1/1], Step [5020/48549], Loss: 2.4960\n",
      "Epoch [1/1], Step [5030/48549], Loss: 2.4711\n",
      "Epoch [1/1], Step [5040/48549], Loss: 2.3955\n",
      "Epoch [1/1], Step [5050/48549], Loss: 2.4634\n",
      "Epoch [1/1], Step [5060/48549], Loss: 2.4433\n",
      "Epoch [1/1], Step [5070/48549], Loss: 2.4500\n",
      "Epoch [1/1], Step [5080/48549], Loss: 2.4178\n",
      "Epoch [1/1], Step [5090/48549], Loss: 2.4268\n",
      "Epoch [1/1], Step [5100/48549], Loss: 2.4302\n",
      "Epoch [1/1], Step [5110/48549], Loss: 2.5711\n",
      "Epoch [1/1], Step [5120/48549], Loss: 2.3940\n",
      "Epoch [1/1], Step [5130/48549], Loss: 2.4708\n",
      "Epoch [1/1], Step [5140/48549], Loss: 2.4568\n",
      "Epoch [1/1], Step [5150/48549], Loss: 2.4323\n",
      "Epoch [1/1], Step [5160/48549], Loss: 2.4827\n",
      "Epoch [1/1], Step [5170/48549], Loss: 2.4419\n",
      "Epoch [1/1], Step [5180/48549], Loss: 2.4530\n",
      "Epoch [1/1], Step [5190/48549], Loss: 2.4487\n",
      "Epoch [1/1], Step [5200/48549], Loss: 2.5311\n",
      "Epoch [1/1], Step [5210/48549], Loss: 2.4454\n",
      "Epoch [1/1], Step [5220/48549], Loss: 2.4734\n",
      "Epoch [1/1], Step [5230/48549], Loss: 2.4686\n",
      "Epoch [1/1], Step [5240/48549], Loss: 2.4744\n",
      "Epoch [1/1], Step [5250/48549], Loss: 2.4279\n",
      "Epoch [1/1], Step [5260/48549], Loss: 2.4708\n",
      "Epoch [1/1], Step [5270/48549], Loss: 2.4121\n",
      "Epoch [1/1], Step [5280/48549], Loss: 2.4886\n",
      "Epoch [1/1], Step [5290/48549], Loss: 2.4306\n",
      "Epoch [1/1], Step [5300/48549], Loss: 2.4037\n",
      "Epoch [1/1], Step [5310/48549], Loss: 2.4229\n",
      "Epoch [1/1], Step [5320/48549], Loss: 2.4417\n",
      "Epoch [1/1], Step [5330/48549], Loss: 2.5807\n",
      "Epoch [1/1], Step [5340/48549], Loss: 2.4078\n",
      "Epoch [1/1], Step [5350/48549], Loss: 2.4507\n",
      "Epoch [1/1], Step [5360/48549], Loss: 2.4200\n",
      "Epoch [1/1], Step [5370/48549], Loss: 2.4145\n",
      "Epoch [1/1], Step [5380/48549], Loss: 2.4020\n",
      "Epoch [1/1], Step [5390/48549], Loss: 2.4909\n",
      "Epoch [1/1], Step [5400/48549], Loss: 2.4809\n",
      "Epoch [1/1], Step [5410/48549], Loss: 2.4990\n",
      "Epoch [1/1], Step [5420/48549], Loss: 2.5435\n",
      "Epoch [1/1], Step [5430/48549], Loss: 2.4993\n",
      "Epoch [1/1], Step [5440/48549], Loss: 2.4303\n",
      "Epoch [1/1], Step [5450/48549], Loss: 2.4150\n",
      "Epoch [1/1], Step [5460/48549], Loss: 2.5404\n",
      "Epoch [1/1], Step [5470/48549], Loss: 2.4157\n",
      "Epoch [1/1], Step [5480/48549], Loss: 2.4740\n",
      "Epoch [1/1], Step [5490/48549], Loss: 2.4833\n",
      "Epoch [1/1], Step [5500/48549], Loss: 2.4295\n",
      "Epoch [1/1], Step [5510/48549], Loss: 2.4657\n",
      "Epoch [1/1], Step [5520/48549], Loss: 2.4097\n",
      "Epoch [1/1], Step [5530/48549], Loss: 2.4541\n",
      "Epoch [1/1], Step [5540/48549], Loss: 2.4688\n",
      "Epoch [1/1], Step [5550/48549], Loss: 2.4880\n",
      "Epoch [1/1], Step [5560/48549], Loss: 2.4479\n",
      "Epoch [1/1], Step [5570/48549], Loss: 2.5630\n",
      "Epoch [1/1], Step [5580/48549], Loss: 2.4758\n",
      "Epoch [1/1], Step [5590/48549], Loss: 2.5455\n",
      "Epoch [1/1], Step [5600/48549], Loss: 2.4634\n",
      "Epoch [1/1], Step [5610/48549], Loss: 2.4367\n",
      "Epoch [1/1], Step [5620/48549], Loss: 2.4550\n",
      "Epoch [1/1], Step [5630/48549], Loss: 2.4580\n",
      "Epoch [1/1], Step [5640/48549], Loss: 2.4260\n",
      "Epoch [1/1], Step [5650/48549], Loss: 2.4764\n",
      "Epoch [1/1], Step [5660/48549], Loss: 2.4827\n",
      "Epoch [1/1], Step [5670/48549], Loss: 2.4010\n",
      "Epoch [1/1], Step [5680/48549], Loss: 2.4650\n",
      "Epoch [1/1], Step [5690/48549], Loss: 2.4394\n",
      "Epoch [1/1], Step [5700/48549], Loss: 2.4538\n",
      "Epoch [1/1], Step [5710/48549], Loss: 2.4758\n",
      "Epoch [1/1], Step [5720/48549], Loss: 2.3980\n",
      "Epoch [1/1], Step [5730/48549], Loss: 2.4165\n",
      "Epoch [1/1], Step [5740/48549], Loss: 2.4859\n",
      "Epoch [1/1], Step [5750/48549], Loss: 2.4518\n",
      "Epoch [1/1], Step [5760/48549], Loss: 2.4413\n",
      "Epoch [1/1], Step [5770/48549], Loss: 2.5388\n",
      "Epoch [1/1], Step [5780/48549], Loss: 2.4793\n",
      "Epoch [1/1], Step [5790/48549], Loss: 2.3712\n",
      "Epoch [1/1], Step [5800/48549], Loss: 2.4566\n",
      "Epoch [1/1], Step [5810/48549], Loss: 2.4989\n",
      "Epoch [1/1], Step [5820/48549], Loss: 2.4795\n",
      "Epoch [1/1], Step [5830/48549], Loss: 2.4333\n",
      "Epoch [1/1], Step [5840/48549], Loss: 2.4308\n",
      "Epoch [1/1], Step [5850/48549], Loss: 2.4360\n",
      "Epoch [1/1], Step [5860/48549], Loss: 2.3866\n",
      "Epoch [1/1], Step [5870/48549], Loss: 2.4596\n",
      "Epoch [1/1], Step [5880/48549], Loss: 2.4536\n",
      "Epoch [1/1], Step [5890/48549], Loss: 2.4371\n",
      "Epoch [1/1], Step [5900/48549], Loss: 2.4290\n",
      "Epoch [1/1], Step [5910/48549], Loss: 2.4614\n",
      "Epoch [1/1], Step [5920/48549], Loss: 2.4422\n",
      "Epoch [1/1], Step [5930/48549], Loss: 2.4262\n",
      "Epoch [1/1], Step [5940/48549], Loss: 2.5518\n",
      "Epoch [1/1], Step [5950/48549], Loss: 2.5269\n",
      "Epoch [1/1], Step [5960/48549], Loss: 2.4936\n",
      "Epoch [1/1], Step [5970/48549], Loss: 2.4190\n",
      "Epoch [1/1], Step [5980/48549], Loss: 2.5135\n",
      "Epoch [1/1], Step [5990/48549], Loss: 2.4313\n",
      "Epoch [1/1], Step [6000/48549], Loss: 2.4404\n",
      "Epoch [1/1], Step [6010/48549], Loss: 2.4318\n",
      "Epoch [1/1], Step [6020/48549], Loss: 2.5392\n",
      "Epoch [1/1], Step [6030/48549], Loss: 2.4514\n",
      "Epoch [1/1], Step [6040/48549], Loss: 2.4613\n",
      "Epoch [1/1], Step [6050/48549], Loss: 2.4334\n",
      "Epoch [1/1], Step [6060/48549], Loss: 2.4337\n",
      "Epoch [1/1], Step [6070/48549], Loss: 2.5024\n",
      "Epoch [1/1], Step [6080/48549], Loss: 2.4402\n",
      "Epoch [1/1], Step [6090/48549], Loss: 2.4397\n",
      "Epoch [1/1], Step [6100/48549], Loss: 2.4981\n",
      "Epoch [1/1], Step [6110/48549], Loss: 2.4092\n",
      "Epoch [1/1], Step [6120/48549], Loss: 2.4296\n",
      "Epoch [1/1], Step [6130/48549], Loss: 2.4319\n",
      "Epoch [1/1], Step [6140/48549], Loss: 2.5305\n",
      "Epoch [1/1], Step [6150/48549], Loss: 2.4610\n",
      "Epoch [1/1], Step [6160/48549], Loss: 2.3843\n",
      "Epoch [1/1], Step [6170/48549], Loss: 2.4525\n",
      "Epoch [1/1], Step [6180/48549], Loss: 2.4322\n",
      "Epoch [1/1], Step [6190/48549], Loss: 2.4421\n",
      "Epoch [1/1], Step [6200/48549], Loss: 2.4602\n",
      "Epoch [1/1], Step [6210/48549], Loss: 2.4002\n",
      "Epoch [1/1], Step [6220/48549], Loss: 2.4313\n",
      "Epoch [1/1], Step [6230/48549], Loss: 2.4728\n",
      "Epoch [1/1], Step [6240/48549], Loss: 2.4084\n",
      "Epoch [1/1], Step [6250/48549], Loss: 2.4532\n",
      "Epoch [1/1], Step [6260/48549], Loss: 2.4072\n",
      "Epoch [1/1], Step [6270/48549], Loss: 2.5342\n",
      "Epoch [1/1], Step [6280/48549], Loss: 2.4913\n",
      "Epoch [1/1], Step [6290/48549], Loss: 2.5563\n",
      "Epoch [1/1], Step [6300/48549], Loss: 2.4668\n",
      "Epoch [1/1], Step [6310/48549], Loss: 2.4208\n",
      "Epoch [1/1], Step [6320/48549], Loss: 2.4124\n",
      "Epoch [1/1], Step [6330/48549], Loss: 2.4673\n",
      "Epoch [1/1], Step [6340/48549], Loss: 2.4979\n",
      "Epoch [1/1], Step [6350/48549], Loss: 2.4176\n",
      "Epoch [1/1], Step [6360/48549], Loss: 2.4475\n",
      "Epoch [1/1], Step [6370/48549], Loss: 2.3970\n",
      "Epoch [1/1], Step [6380/48549], Loss: 2.4762\n",
      "Epoch [1/1], Step [6390/48549], Loss: 2.4705\n",
      "Epoch [1/1], Step [6400/48549], Loss: 2.4349\n",
      "Epoch [1/1], Step [6410/48549], Loss: 2.4422\n",
      "Epoch [1/1], Step [6420/48549], Loss: 2.4775\n",
      "Epoch [1/1], Step [6430/48549], Loss: 2.4644\n",
      "Epoch [1/1], Step [6440/48549], Loss: 2.4776\n",
      "Epoch [1/1], Step [6450/48549], Loss: 2.4590\n",
      "Epoch [1/1], Step [6460/48549], Loss: 2.4551\n",
      "Epoch [1/1], Step [6470/48549], Loss: 2.4543\n",
      "Epoch [1/1], Step [6480/48549], Loss: 2.4692\n",
      "Epoch [1/1], Step [6490/48549], Loss: 2.4819\n",
      "Epoch [1/1], Step [6500/48549], Loss: 2.4089\n",
      "Epoch [1/1], Step [6510/48549], Loss: 2.4443\n",
      "Epoch [1/1], Step [6520/48549], Loss: 2.4751\n",
      "Epoch [1/1], Step [6530/48549], Loss: 2.4281\n",
      "Epoch [1/1], Step [6540/48549], Loss: 2.4042\n",
      "Epoch [1/1], Step [6550/48549], Loss: 2.3894\n",
      "Epoch [1/1], Step [6560/48549], Loss: 2.4367\n",
      "Epoch [1/1], Step [6570/48549], Loss: 2.4311\n",
      "Epoch [1/1], Step [6580/48549], Loss: 2.4373\n",
      "Epoch [1/1], Step [6590/48549], Loss: 2.4767\n",
      "Epoch [1/1], Step [6600/48549], Loss: 2.4213\n",
      "Epoch [1/1], Step [6610/48549], Loss: 2.4673\n",
      "Epoch [1/1], Step [6620/48549], Loss: 2.4079\n",
      "Epoch [1/1], Step [6630/48549], Loss: 2.4778\n",
      "Epoch [1/1], Step [6640/48549], Loss: 2.4364\n",
      "Epoch [1/1], Step [6650/48549], Loss: 2.4605\n",
      "Epoch [1/1], Step [6660/48549], Loss: 2.4527\n",
      "Epoch [1/1], Step [6670/48549], Loss: 2.4182\n",
      "Epoch [1/1], Step [6680/48549], Loss: 2.4896\n",
      "Epoch [1/1], Step [6690/48549], Loss: 2.4195\n",
      "Epoch [1/1], Step [6700/48549], Loss: 2.4240\n",
      "Epoch [1/1], Step [6710/48549], Loss: 2.4689\n",
      "Epoch [1/1], Step [6720/48549], Loss: 2.4774\n",
      "Epoch [1/1], Step [6730/48549], Loss: 2.4374\n",
      "Epoch [1/1], Step [6740/48549], Loss: 2.4304\n",
      "Epoch [1/1], Step [6750/48549], Loss: 2.4310\n",
      "Epoch [1/1], Step [6760/48549], Loss: 2.4711\n",
      "Epoch [1/1], Step [6770/48549], Loss: 2.5124\n",
      "Epoch [1/1], Step [6780/48549], Loss: 2.4719\n",
      "Epoch [1/1], Step [6790/48549], Loss: 2.4048\n",
      "Epoch [1/1], Step [6800/48549], Loss: 2.4366\n",
      "Epoch [1/1], Step [6810/48549], Loss: 2.4905\n",
      "Epoch [1/1], Step [6820/48549], Loss: 2.3984\n",
      "Epoch [1/1], Step [6830/48549], Loss: 2.4159\n",
      "Epoch [1/1], Step [6840/48549], Loss: 2.4583\n",
      "Epoch [1/1], Step [6850/48549], Loss: 2.4630\n",
      "Epoch [1/1], Step [6860/48549], Loss: 2.4266\n",
      "Epoch [1/1], Step [6870/48549], Loss: 2.4641\n",
      "Epoch [1/1], Step [6880/48549], Loss: 2.4445\n",
      "Epoch [1/1], Step [6890/48549], Loss: 2.4467\n",
      "Epoch [1/1], Step [6900/48549], Loss: 2.5101\n",
      "Epoch [1/1], Step [6910/48549], Loss: 2.4177\n",
      "Epoch [1/1], Step [6920/48549], Loss: 2.3962\n",
      "Epoch [1/1], Step [6930/48549], Loss: 2.4502\n",
      "Epoch [1/1], Step [6940/48549], Loss: 2.4326\n",
      "Epoch [1/1], Step [6950/48549], Loss: 2.4405\n",
      "Epoch [1/1], Step [6960/48549], Loss: 2.4277\n",
      "Epoch [1/1], Step [6970/48549], Loss: 2.4313\n",
      "Epoch [1/1], Step [6980/48549], Loss: 2.4411\n",
      "Epoch [1/1], Step [6990/48549], Loss: 2.4690\n",
      "Epoch [1/1], Step [7000/48549], Loss: 2.5042\n",
      "Epoch [1/1], Step [7010/48549], Loss: 2.3879\n",
      "Epoch [1/1], Step [7020/48549], Loss: 2.4253\n",
      "Epoch [1/1], Step [7030/48549], Loss: 2.4439\n",
      "Epoch [1/1], Step [7040/48549], Loss: 2.4555\n",
      "Epoch [1/1], Step [7050/48549], Loss: 2.4208\n",
      "Epoch [1/1], Step [7060/48549], Loss: 2.3798\n",
      "Epoch [1/1], Step [7070/48549], Loss: 2.3979\n",
      "Epoch [1/1], Step [7080/48549], Loss: 2.4694\n",
      "Epoch [1/1], Step [7090/48549], Loss: 2.3969\n",
      "Epoch [1/1], Step [7100/48549], Loss: 2.4802\n",
      "Epoch [1/1], Step [7110/48549], Loss: 2.4545\n",
      "Epoch [1/1], Step [7120/48549], Loss: 2.4616\n",
      "Epoch [1/1], Step [7130/48549], Loss: 2.4293\n",
      "Epoch [1/1], Step [7140/48549], Loss: 2.4329\n",
      "Epoch [1/1], Step [7150/48549], Loss: 2.4765\n",
      "Epoch [1/1], Step [7160/48549], Loss: 2.4180\n",
      "Epoch [1/1], Step [7170/48549], Loss: 2.4910\n",
      "Epoch [1/1], Step [7180/48549], Loss: 2.4297\n",
      "Epoch [1/1], Step [7190/48549], Loss: 2.5063\n",
      "Epoch [1/1], Step [7200/48549], Loss: 2.4155\n",
      "Epoch [1/1], Step [7210/48549], Loss: 2.4310\n",
      "Epoch [1/1], Step [7220/48549], Loss: 2.4544\n",
      "Epoch [1/1], Step [7230/48549], Loss: 2.4371\n",
      "Epoch [1/1], Step [7240/48549], Loss: 2.4672\n",
      "Epoch [1/1], Step [7250/48549], Loss: 2.3989\n",
      "Epoch [1/1], Step [7260/48549], Loss: 2.4999\n",
      "Epoch [1/1], Step [7270/48549], Loss: 2.4714\n",
      "Epoch [1/1], Step [7280/48549], Loss: 2.4785\n",
      "Epoch [1/1], Step [7290/48549], Loss: 2.3854\n",
      "Epoch [1/1], Step [7300/48549], Loss: 2.4396\n",
      "Epoch [1/1], Step [7310/48549], Loss: 2.4567\n",
      "Epoch [1/1], Step [7320/48549], Loss: 2.4488\n",
      "Epoch [1/1], Step [7330/48549], Loss: 2.4710\n",
      "Epoch [1/1], Step [7340/48549], Loss: 2.4797\n",
      "Epoch [1/1], Step [7350/48549], Loss: 2.5076\n",
      "Epoch [1/1], Step [7360/48549], Loss: 2.6270\n",
      "Epoch [1/1], Step [7370/48549], Loss: 2.4339\n",
      "Epoch [1/1], Step [7380/48549], Loss: 2.4766\n",
      "Epoch [1/1], Step [7390/48549], Loss: 2.4336\n",
      "Epoch [1/1], Step [7400/48549], Loss: 2.4471\n",
      "Epoch [1/1], Step [7410/48549], Loss: 2.4829\n",
      "Epoch [1/1], Step [7420/48549], Loss: 2.4357\n",
      "Epoch [1/1], Step [7430/48549], Loss: 2.3913\n",
      "Epoch [1/1], Step [7440/48549], Loss: 2.4026\n",
      "Epoch [1/1], Step [7450/48549], Loss: 2.4023\n",
      "Epoch [1/1], Step [7460/48549], Loss: 2.4054\n",
      "Epoch [1/1], Step [7470/48549], Loss: 2.3739\n",
      "Epoch [1/1], Step [7480/48549], Loss: 2.5288\n",
      "Epoch [1/1], Step [7490/48549], Loss: 2.4213\n",
      "Epoch [1/1], Step [7500/48549], Loss: 2.4698\n",
      "Epoch [1/1], Step [7510/48549], Loss: 2.4252\n",
      "Epoch [1/1], Step [7520/48549], Loss: 2.4802\n",
      "Epoch [1/1], Step [7530/48549], Loss: 2.4563\n",
      "Epoch [1/1], Step [7540/48549], Loss: 2.4757\n",
      "Epoch [1/1], Step [7550/48549], Loss: 2.5771\n",
      "Epoch [1/1], Step [7560/48549], Loss: 2.5159\n",
      "Epoch [1/1], Step [7570/48549], Loss: 2.4304\n",
      "Epoch [1/1], Step [7580/48549], Loss: 2.4437\n",
      "Epoch [1/1], Step [7590/48549], Loss: 2.4589\n",
      "Epoch [1/1], Step [7600/48549], Loss: 2.4233\n",
      "Epoch [1/1], Step [7610/48549], Loss: 2.5087\n",
      "Epoch [1/1], Step [7620/48549], Loss: 2.4737\n",
      "Epoch [1/1], Step [7630/48549], Loss: 2.4511\n",
      "Epoch [1/1], Step [7640/48549], Loss: 2.4790\n",
      "Epoch [1/1], Step [7650/48549], Loss: 2.4510\n",
      "Epoch [1/1], Step [7660/48549], Loss: 2.4466\n",
      "Epoch [1/1], Step [7670/48549], Loss: 2.4403\n",
      "Epoch [1/1], Step [7680/48549], Loss: 2.4203\n",
      "Epoch [1/1], Step [7690/48549], Loss: 2.5100\n",
      "Epoch [1/1], Step [7700/48549], Loss: 2.4649\n",
      "Epoch [1/1], Step [7710/48549], Loss: 2.4753\n",
      "Epoch [1/1], Step [7720/48549], Loss: 2.5644\n",
      "Epoch [1/1], Step [7730/48549], Loss: 2.4600\n",
      "Epoch [1/1], Step [7740/48549], Loss: 2.4340\n",
      "Epoch [1/1], Step [7750/48549], Loss: 2.4398\n",
      "Epoch [1/1], Step [7760/48549], Loss: 2.4468\n",
      "Epoch [1/1], Step [7770/48549], Loss: 2.4566\n",
      "Epoch [1/1], Step [7780/48549], Loss: 2.4525\n",
      "Epoch [1/1], Step [7790/48549], Loss: 2.4464\n",
      "Epoch [1/1], Step [7800/48549], Loss: 2.4373\n",
      "Epoch [1/1], Step [7810/48549], Loss: 2.4854\n",
      "Epoch [1/1], Step [7820/48549], Loss: 2.4789\n",
      "Epoch [1/1], Step [7830/48549], Loss: 2.4282\n",
      "Epoch [1/1], Step [7840/48549], Loss: 2.4651\n",
      "Epoch [1/1], Step [7850/48549], Loss: 2.4503\n",
      "Epoch [1/1], Step [7860/48549], Loss: 2.4568\n",
      "Epoch [1/1], Step [7870/48549], Loss: 2.4511\n",
      "Epoch [1/1], Step [7880/48549], Loss: 2.4316\n",
      "Epoch [1/1], Step [7890/48549], Loss: 2.4977\n",
      "Epoch [1/1], Step [7900/48549], Loss: 2.5203\n",
      "Epoch [1/1], Step [7910/48549], Loss: 2.4908\n",
      "Epoch [1/1], Step [7920/48549], Loss: 2.4410\n",
      "Epoch [1/1], Step [7930/48549], Loss: 2.4257\n",
      "Epoch [1/1], Step [7940/48549], Loss: 2.4652\n",
      "Epoch [1/1], Step [7950/48549], Loss: 2.3955\n",
      "Epoch [1/1], Step [7960/48549], Loss: 2.3921\n",
      "Epoch [1/1], Step [7970/48549], Loss: 2.3854\n",
      "Epoch [1/1], Step [7980/48549], Loss: 2.4285\n",
      "Epoch [1/1], Step [7990/48549], Loss: 2.4440\n",
      "Epoch [1/1], Step [8000/48549], Loss: 2.4828\n",
      "Epoch [1/1], Step [8010/48549], Loss: 2.4683\n",
      "Epoch [1/1], Step [8020/48549], Loss: 2.4393\n",
      "Epoch [1/1], Step [8030/48549], Loss: 2.4881\n",
      "Epoch [1/1], Step [8040/48549], Loss: 2.5053\n",
      "Epoch [1/1], Step [8050/48549], Loss: 2.5126\n",
      "Epoch [1/1], Step [8060/48549], Loss: 2.4224\n",
      "Epoch [1/1], Step [8070/48549], Loss: 2.5378\n",
      "Epoch [1/1], Step [8080/48549], Loss: 2.3935\n",
      "Epoch [1/1], Step [8090/48549], Loss: 2.4372\n",
      "Epoch [1/1], Step [8100/48549], Loss: 2.4630\n",
      "Epoch [1/1], Step [8110/48549], Loss: 2.4343\n",
      "Epoch [1/1], Step [8120/48549], Loss: 2.4557\n",
      "Epoch [1/1], Step [8130/48549], Loss: 2.5141\n",
      "Epoch [1/1], Step [8140/48549], Loss: 2.4439\n",
      "Epoch [1/1], Step [8150/48549], Loss: 2.4495\n",
      "Epoch [1/1], Step [8160/48549], Loss: 2.4621\n",
      "Epoch [1/1], Step [8170/48549], Loss: 2.4485\n",
      "Epoch [1/1], Step [8180/48549], Loss: 2.4251\n",
      "Epoch [1/1], Step [8190/48549], Loss: 2.4104\n",
      "Epoch [1/1], Step [8200/48549], Loss: 2.4498\n",
      "Epoch [1/1], Step [8210/48549], Loss: 2.4502\n",
      "Epoch [1/1], Step [8220/48549], Loss: 2.4408\n",
      "Epoch [1/1], Step [8230/48549], Loss: 2.4335\n",
      "Epoch [1/1], Step [8240/48549], Loss: 2.4802\n",
      "Epoch [1/1], Step [8250/48549], Loss: 2.4076\n",
      "Epoch [1/1], Step [8260/48549], Loss: 2.4286\n",
      "Epoch [1/1], Step [8270/48549], Loss: 2.3469\n",
      "Epoch [1/1], Step [8280/48549], Loss: 2.4822\n",
      "Epoch [1/1], Step [8290/48549], Loss: 2.4133\n",
      "Epoch [1/1], Step [8300/48549], Loss: 2.4175\n",
      "Epoch [1/1], Step [8310/48549], Loss: 2.4910\n",
      "Epoch [1/1], Step [8320/48549], Loss: 2.4173\n",
      "Epoch [1/1], Step [8330/48549], Loss: 2.4430\n",
      "Epoch [1/1], Step [8340/48549], Loss: 2.4290\n",
      "Epoch [1/1], Step [8350/48549], Loss: 2.4120\n",
      "Epoch [1/1], Step [8360/48549], Loss: 2.4413\n",
      "Epoch [1/1], Step [8370/48549], Loss: 2.4537\n",
      "Epoch [1/1], Step [8380/48549], Loss: 2.4746\n",
      "Epoch [1/1], Step [8390/48549], Loss: 2.4225\n",
      "Epoch [1/1], Step [8400/48549], Loss: 2.4626\n",
      "Epoch [1/1], Step [8410/48549], Loss: 2.4485\n",
      "Epoch [1/1], Step [8420/48549], Loss: 2.4386\n",
      "Epoch [1/1], Step [8430/48549], Loss: 2.4592\n",
      "Epoch [1/1], Step [8440/48549], Loss: 2.5037\n",
      "Epoch [1/1], Step [8450/48549], Loss: 2.4569\n",
      "Epoch [1/1], Step [8460/48549], Loss: 2.4308\n",
      "Epoch [1/1], Step [8470/48549], Loss: 2.4688\n",
      "Epoch [1/1], Step [8480/48549], Loss: 2.5082\n",
      "Epoch [1/1], Step [8490/48549], Loss: 2.4323\n",
      "Epoch [1/1], Step [8500/48549], Loss: 2.4916\n",
      "Epoch [1/1], Step [8510/48549], Loss: 2.4688\n",
      "Epoch [1/1], Step [8520/48549], Loss: 2.4447\n",
      "Epoch [1/1], Step [8530/48549], Loss: 2.4654\n",
      "Epoch [1/1], Step [8540/48549], Loss: 2.4430\n",
      "Epoch [1/1], Step [8550/48549], Loss: 2.4286\n",
      "Epoch [1/1], Step [8560/48549], Loss: 2.4763\n",
      "Epoch [1/1], Step [8570/48549], Loss: 2.4468\n",
      "Epoch [1/1], Step [8580/48549], Loss: 2.3884\n",
      "Epoch [1/1], Step [8590/48549], Loss: 2.3710\n",
      "Epoch [1/1], Step [8600/48549], Loss: 2.4652\n",
      "Epoch [1/1], Step [8610/48549], Loss: 2.4436\n",
      "Epoch [1/1], Step [8620/48549], Loss: 2.4331\n",
      "Epoch [1/1], Step [8630/48549], Loss: 2.3863\n",
      "Epoch [1/1], Step [8640/48549], Loss: 2.5084\n",
      "Epoch [1/1], Step [8650/48549], Loss: 2.4728\n",
      "Epoch [1/1], Step [8660/48549], Loss: 2.4173\n",
      "Epoch [1/1], Step [8670/48549], Loss: 2.4569\n",
      "Epoch [1/1], Step [8680/48549], Loss: 2.4665\n",
      "Epoch [1/1], Step [8690/48549], Loss: 2.4618\n",
      "Epoch [1/1], Step [8700/48549], Loss: 2.3737\n",
      "Epoch [1/1], Step [8710/48549], Loss: 2.4616\n",
      "Epoch [1/1], Step [8720/48549], Loss: 2.4468\n",
      "Epoch [1/1], Step [8730/48549], Loss: 2.4654\n",
      "Epoch [1/1], Step [8740/48549], Loss: 2.4569\n",
      "Epoch [1/1], Step [8750/48549], Loss: 2.4425\n",
      "Epoch [1/1], Step [8760/48549], Loss: 2.4811\n",
      "Epoch [1/1], Step [8770/48549], Loss: 2.4753\n",
      "Epoch [1/1], Step [8780/48549], Loss: 2.4347\n",
      "Epoch [1/1], Step [8790/48549], Loss: 2.4696\n",
      "Epoch [1/1], Step [8800/48549], Loss: 2.3679\n",
      "Epoch [1/1], Step [8810/48549], Loss: 2.5123\n",
      "Epoch [1/1], Step [8820/48549], Loss: 2.5579\n",
      "Epoch [1/1], Step [8830/48549], Loss: 2.4267\n",
      "Epoch [1/1], Step [8840/48549], Loss: 2.4488\n",
      "Epoch [1/1], Step [8850/48549], Loss: 2.4446\n",
      "Epoch [1/1], Step [8860/48549], Loss: 2.4662\n",
      "Epoch [1/1], Step [8870/48549], Loss: 2.4311\n",
      "Epoch [1/1], Step [8880/48549], Loss: 2.4106\n",
      "Epoch [1/1], Step [8890/48549], Loss: 2.4700\n",
      "Epoch [1/1], Step [8900/48549], Loss: 2.5290\n",
      "Epoch [1/1], Step [8910/48549], Loss: 2.4795\n",
      "Epoch [1/1], Step [8920/48549], Loss: 2.4366\n",
      "Epoch [1/1], Step [8930/48549], Loss: 2.6340\n",
      "Epoch [1/1], Step [8940/48549], Loss: 2.4738\n",
      "Epoch [1/1], Step [8950/48549], Loss: 2.4355\n",
      "Epoch [1/1], Step [8960/48549], Loss: 2.4650\n",
      "Epoch [1/1], Step [8970/48549], Loss: 2.4727\n",
      "Epoch [1/1], Step [8980/48549], Loss: 2.5215\n",
      "Epoch [1/1], Step [8990/48549], Loss: 2.3951\n",
      "Epoch [1/1], Step [9000/48549], Loss: 2.4220\n",
      "Epoch [1/1], Step [9010/48549], Loss: 2.4219\n",
      "Epoch [1/1], Step [9020/48549], Loss: 2.4681\n",
      "Epoch [1/1], Step [9030/48549], Loss: 2.4125\n",
      "Epoch [1/1], Step [9040/48549], Loss: 2.4875\n",
      "Epoch [1/1], Step [9050/48549], Loss: 2.4239\n",
      "Epoch [1/1], Step [9060/48549], Loss: 2.4990\n",
      "Epoch [1/1], Step [9070/48549], Loss: 2.4380\n",
      "Epoch [1/1], Step [9080/48549], Loss: 2.4600\n",
      "Epoch [1/1], Step [9090/48549], Loss: 2.3773\n",
      "Epoch [1/1], Step [9100/48549], Loss: 2.4833\n",
      "Epoch [1/1], Step [9110/48549], Loss: 2.4416\n",
      "Epoch [1/1], Step [9120/48549], Loss: 2.4217\n",
      "Epoch [1/1], Step [9130/48549], Loss: 2.4763\n",
      "Epoch [1/1], Step [9140/48549], Loss: 2.4415\n",
      "Epoch [1/1], Step [9150/48549], Loss: 2.4741\n",
      "Epoch [1/1], Step [9160/48549], Loss: 2.4625\n",
      "Epoch [1/1], Step [9170/48549], Loss: 2.5153\n",
      "Epoch [1/1], Step [9180/48549], Loss: 2.4014\n",
      "Epoch [1/1], Step [9190/48549], Loss: 2.4517\n",
      "Epoch [1/1], Step [9200/48549], Loss: 2.4713\n",
      "Epoch [1/1], Step [9210/48549], Loss: 2.5335\n",
      "Epoch [1/1], Step [9220/48549], Loss: 2.4322\n",
      "Epoch [1/1], Step [9230/48549], Loss: 2.4285\n",
      "Epoch [1/1], Step [9240/48549], Loss: 2.4443\n",
      "Epoch [1/1], Step [9250/48549], Loss: 2.4279\n",
      "Epoch [1/1], Step [9260/48549], Loss: 2.4467\n",
      "Epoch [1/1], Step [9270/48549], Loss: 2.4326\n",
      "Epoch [1/1], Step [9280/48549], Loss: 2.4427\n",
      "Epoch [1/1], Step [9290/48549], Loss: 2.4379\n",
      "Epoch [1/1], Step [9300/48549], Loss: 2.4516\n",
      "Epoch [1/1], Step [9310/48549], Loss: 2.4383\n",
      "Epoch [1/1], Step [9320/48549], Loss: 2.5476\n",
      "Epoch [1/1], Step [9330/48549], Loss: 2.4843\n",
      "Epoch [1/1], Step [9340/48549], Loss: 2.4812\n",
      "Epoch [1/1], Step [9350/48549], Loss: 2.4147\n",
      "Epoch [1/1], Step [9360/48549], Loss: 2.4423\n",
      "Epoch [1/1], Step [9370/48549], Loss: 2.4998\n",
      "Epoch [1/1], Step [9380/48549], Loss: 2.3895\n",
      "Epoch [1/1], Step [9390/48549], Loss: 2.5056\n",
      "Epoch [1/1], Step [9400/48549], Loss: 2.4619\n",
      "Epoch [1/1], Step [9410/48549], Loss: 2.4109\n",
      "Epoch [1/1], Step [9420/48549], Loss: 2.4256\n",
      "Epoch [1/1], Step [9430/48549], Loss: 2.4255\n",
      "Epoch [1/1], Step [9440/48549], Loss: 2.4285\n",
      "Epoch [1/1], Step [9450/48549], Loss: 2.4767\n",
      "Epoch [1/1], Step [9460/48549], Loss: 2.4696\n",
      "Epoch [1/1], Step [9470/48549], Loss: 2.4415\n",
      "Epoch [1/1], Step [9480/48549], Loss: 2.4203\n",
      "Epoch [1/1], Step [9490/48549], Loss: 2.4269\n",
      "Epoch [1/1], Step [9500/48549], Loss: 2.4285\n",
      "Epoch [1/1], Step [9510/48549], Loss: 2.4125\n",
      "Epoch [1/1], Step [9520/48549], Loss: 2.4008\n",
      "Epoch [1/1], Step [9530/48549], Loss: 2.4046\n",
      "Epoch [1/1], Step [9540/48549], Loss: 2.4036\n",
      "Epoch [1/1], Step [9550/48549], Loss: 2.4140\n",
      "Epoch [1/1], Step [9560/48549], Loss: 2.4289\n",
      "Epoch [1/1], Step [9570/48549], Loss: 2.5208\n",
      "Epoch [1/1], Step [9580/48549], Loss: 2.4947\n",
      "Epoch [1/1], Step [9590/48549], Loss: 2.4350\n",
      "Epoch [1/1], Step [9600/48549], Loss: 2.4515\n",
      "Epoch [1/1], Step [9610/48549], Loss: 2.4491\n",
      "Epoch [1/1], Step [9620/48549], Loss: 2.4518\n",
      "Epoch [1/1], Step [9630/48549], Loss: 2.4243\n",
      "Epoch [1/1], Step [9640/48549], Loss: 2.4020\n",
      "Epoch [1/1], Step [9650/48549], Loss: 2.4149\n",
      "Epoch [1/1], Step [9660/48549], Loss: 2.4016\n",
      "Epoch [1/1], Step [9670/48549], Loss: 2.4792\n",
      "Epoch [1/1], Step [9680/48549], Loss: 2.4724\n",
      "Epoch [1/1], Step [9690/48549], Loss: 2.3890\n",
      "Epoch [1/1], Step [9700/48549], Loss: 2.4286\n",
      "Epoch [1/1], Step [9710/48549], Loss: 2.3974\n",
      "Epoch [1/1], Step [9720/48549], Loss: 2.4489\n",
      "Epoch [1/1], Step [9730/48549], Loss: 2.3799\n",
      "Epoch [1/1], Step [9740/48549], Loss: 2.4204\n",
      "Epoch [1/1], Step [9750/48549], Loss: 2.4733\n",
      "Epoch [1/1], Step [9760/48549], Loss: 2.5130\n",
      "Epoch [1/1], Step [9770/48549], Loss: 2.4377\n",
      "Epoch [1/1], Step [9780/48549], Loss: 2.4597\n",
      "Epoch [1/1], Step [9790/48549], Loss: 2.4720\n",
      "Epoch [1/1], Step [9800/48549], Loss: 2.4577\n",
      "Epoch [1/1], Step [9810/48549], Loss: 2.4825\n",
      "Epoch [1/1], Step [9820/48549], Loss: 2.4536\n",
      "Epoch [1/1], Step [9830/48549], Loss: 2.4820\n",
      "Epoch [1/1], Step [9840/48549], Loss: 2.4540\n",
      "Epoch [1/1], Step [9850/48549], Loss: 2.4861\n",
      "Epoch [1/1], Step [9860/48549], Loss: 2.4421\n",
      "Epoch [1/1], Step [9870/48549], Loss: 2.4468\n",
      "Epoch [1/1], Step [9880/48549], Loss: 2.5137\n",
      "Epoch [1/1], Step [9890/48549], Loss: 2.5681\n",
      "Epoch [1/1], Step [9900/48549], Loss: 2.4217\n",
      "Epoch [1/1], Step [9910/48549], Loss: 2.5104\n",
      "Epoch [1/1], Step [9920/48549], Loss: 2.5717\n",
      "Epoch [1/1], Step [9930/48549], Loss: 2.4288\n",
      "Epoch [1/1], Step [9940/48549], Loss: 2.4336\n",
      "Epoch [1/1], Step [9950/48549], Loss: 2.4891\n",
      "Epoch [1/1], Step [9960/48549], Loss: 2.6536\n",
      "Epoch [1/1], Step [9970/48549], Loss: 2.4734\n",
      "Epoch [1/1], Step [9980/48549], Loss: 2.3902\n",
      "Epoch [1/1], Step [9990/48549], Loss: 2.4630\n",
      "Epoch [1/1], Step [10000/48549], Loss: 2.5287\n",
      "Epoch [1/1], Step [10010/48549], Loss: 2.4207\n",
      "Epoch [1/1], Step [10020/48549], Loss: 2.4022\n",
      "Epoch [1/1], Step [10030/48549], Loss: 2.4837\n",
      "Epoch [1/1], Step [10040/48549], Loss: 2.5462\n",
      "Epoch [1/1], Step [10050/48549], Loss: 2.4647\n",
      "Epoch [1/1], Step [10060/48549], Loss: 2.4011\n",
      "Epoch [1/1], Step [10070/48549], Loss: 2.5079\n",
      "Epoch [1/1], Step [10080/48549], Loss: 2.4840\n",
      "Epoch [1/1], Step [10090/48549], Loss: 2.4276\n",
      "Epoch [1/1], Step [10100/48549], Loss: 2.4366\n",
      "Epoch [1/1], Step [10110/48549], Loss: 2.4553\n",
      "Epoch [1/1], Step [10120/48549], Loss: 2.4424\n",
      "Epoch [1/1], Step [10130/48549], Loss: 2.4444\n",
      "Epoch [1/1], Step [10140/48549], Loss: 2.4387\n",
      "Epoch [1/1], Step [10150/48549], Loss: 2.4322\n",
      "Epoch [1/1], Step [10160/48549], Loss: 2.4028\n",
      "Epoch [1/1], Step [10170/48549], Loss: 2.4180\n",
      "Epoch [1/1], Step [10180/48549], Loss: 2.4694\n",
      "Epoch [1/1], Step [10190/48549], Loss: 2.4732\n",
      "Epoch [1/1], Step [10200/48549], Loss: 2.4588\n",
      "Epoch [1/1], Step [10210/48549], Loss: 2.4480\n",
      "Epoch [1/1], Step [10220/48549], Loss: 2.4657\n",
      "Epoch [1/1], Step [10230/48549], Loss: 2.3953\n",
      "Epoch [1/1], Step [10240/48549], Loss: 2.4332\n",
      "Epoch [1/1], Step [10250/48549], Loss: 2.4281\n",
      "Epoch [1/1], Step [10260/48549], Loss: 2.5414\n",
      "Epoch [1/1], Step [10270/48549], Loss: 2.4932\n",
      "Epoch [1/1], Step [10280/48549], Loss: 2.4036\n",
      "Epoch [1/1], Step [10290/48549], Loss: 2.4105\n",
      "Epoch [1/1], Step [10300/48549], Loss: 2.4362\n",
      "Epoch [1/1], Step [10310/48549], Loss: 2.4223\n",
      "Epoch [1/1], Step [10320/48549], Loss: 2.4386\n",
      "Epoch [1/1], Step [10330/48549], Loss: 2.4507\n",
      "Epoch [1/1], Step [10340/48549], Loss: 2.4377\n",
      "Epoch [1/1], Step [10350/48549], Loss: 2.3916\n",
      "Epoch [1/1], Step [10360/48549], Loss: 2.4212\n",
      "Epoch [1/1], Step [10370/48549], Loss: 2.4466\n",
      "Epoch [1/1], Step [10380/48549], Loss: 2.4913\n",
      "Epoch [1/1], Step [10390/48549], Loss: 2.4392\n",
      "Epoch [1/1], Step [10400/48549], Loss: 2.3727\n",
      "Epoch [1/1], Step [10410/48549], Loss: 2.4217\n",
      "Epoch [1/1], Step [10420/48549], Loss: 2.4507\n",
      "Epoch [1/1], Step [10430/48549], Loss: 2.4140\n",
      "Epoch [1/1], Step [10440/48549], Loss: 2.4822\n",
      "Epoch [1/1], Step [10450/48549], Loss: 2.4527\n",
      "Epoch [1/1], Step [10460/48549], Loss: 2.4864\n",
      "Epoch [1/1], Step [10470/48549], Loss: 2.3941\n",
      "Epoch [1/1], Step [10480/48549], Loss: 2.4238\n",
      "Epoch [1/1], Step [10490/48549], Loss: 2.4505\n",
      "Epoch [1/1], Step [10500/48549], Loss: 2.4881\n",
      "Epoch [1/1], Step [10510/48549], Loss: 2.4688\n",
      "Epoch [1/1], Step [10520/48549], Loss: 2.4489\n",
      "Epoch [1/1], Step [10530/48549], Loss: 2.4209\n",
      "Epoch [1/1], Step [10540/48549], Loss: 2.5101\n",
      "Epoch [1/1], Step [10550/48549], Loss: 2.4802\n",
      "Epoch [1/1], Step [10560/48549], Loss: 2.4187\n",
      "Epoch [1/1], Step [10570/48549], Loss: 2.4983\n",
      "Epoch [1/1], Step [10580/48549], Loss: 2.4299\n",
      "Epoch [1/1], Step [10590/48549], Loss: 2.4078\n",
      "Epoch [1/1], Step [10600/48549], Loss: 2.5222\n",
      "Epoch [1/1], Step [10610/48549], Loss: 2.4466\n",
      "Epoch [1/1], Step [10620/48549], Loss: 2.3667\n",
      "Epoch [1/1], Step [10630/48549], Loss: 2.4542\n",
      "Epoch [1/1], Step [10640/48549], Loss: 2.4489\n",
      "Epoch [1/1], Step [10650/48549], Loss: 2.4945\n",
      "Epoch [1/1], Step [10660/48549], Loss: 2.4432\n",
      "Epoch [1/1], Step [10670/48549], Loss: 2.4501\n",
      "Epoch [1/1], Step [10680/48549], Loss: 2.4048\n",
      "Epoch [1/1], Step [10690/48549], Loss: 2.4103\n",
      "Epoch [1/1], Step [10700/48549], Loss: 2.4914\n",
      "Epoch [1/1], Step [10710/48549], Loss: 2.4166\n",
      "Epoch [1/1], Step [10720/48549], Loss: 2.4184\n",
      "Epoch [1/1], Step [10730/48549], Loss: 2.4293\n",
      "Epoch [1/1], Step [10740/48549], Loss: 2.4935\n",
      "Epoch [1/1], Step [10750/48549], Loss: 2.4525\n",
      "Epoch [1/1], Step [10760/48549], Loss: 2.4554\n",
      "Epoch [1/1], Step [10770/48549], Loss: 2.4280\n",
      "Epoch [1/1], Step [10780/48549], Loss: 2.3877\n",
      "Epoch [1/1], Step [10790/48549], Loss: 2.4437\n",
      "Epoch [1/1], Step [10800/48549], Loss: 2.4452\n",
      "Epoch [1/1], Step [10810/48549], Loss: 2.4819\n",
      "Epoch [1/1], Step [10820/48549], Loss: 2.4992\n",
      "Epoch [1/1], Step [10830/48549], Loss: 2.4542\n",
      "Epoch [1/1], Step [10840/48549], Loss: 2.4889\n",
      "Epoch [1/1], Step [10850/48549], Loss: 2.4345\n",
      "Epoch [1/1], Step [10860/48549], Loss: 2.4173\n",
      "Epoch [1/1], Step [10870/48549], Loss: 2.4743\n",
      "Epoch [1/1], Step [10880/48549], Loss: 2.4623\n",
      "Epoch [1/1], Step [10890/48549], Loss: 2.5291\n",
      "Epoch [1/1], Step [10900/48549], Loss: 2.4162\n",
      "Epoch [1/1], Step [10910/48549], Loss: 2.4444\n",
      "Epoch [1/1], Step [10920/48549], Loss: 2.5039\n",
      "Epoch [1/1], Step [10930/48549], Loss: 2.4439\n",
      "Epoch [1/1], Step [10940/48549], Loss: 2.4660\n",
      "Epoch [1/1], Step [10950/48549], Loss: 2.4903\n",
      "Epoch [1/1], Step [10960/48549], Loss: 2.4392\n",
      "Epoch [1/1], Step [10970/48549], Loss: 2.4033\n",
      "Epoch [1/1], Step [10980/48549], Loss: 2.4358\n",
      "Epoch [1/1], Step [10990/48549], Loss: 2.4077\n",
      "Epoch [1/1], Step [11000/48549], Loss: 2.4825\n",
      "Epoch [1/1], Step [11010/48549], Loss: 2.4476\n",
      "Epoch [1/1], Step [11020/48549], Loss: 2.4347\n",
      "Epoch [1/1], Step [11030/48549], Loss: 2.4926\n",
      "Epoch [1/1], Step [11040/48549], Loss: 2.4246\n",
      "Epoch [1/1], Step [11050/48549], Loss: 2.4633\n",
      "Epoch [1/1], Step [11060/48549], Loss: 2.4271\n",
      "Epoch [1/1], Step [11070/48549], Loss: 2.4107\n",
      "Epoch [1/1], Step [11080/48549], Loss: 2.4273\n",
      "Epoch [1/1], Step [11090/48549], Loss: 2.4156\n",
      "Epoch [1/1], Step [11100/48549], Loss: 2.4599\n",
      "Epoch [1/1], Step [11110/48549], Loss: 2.4497\n",
      "Epoch [1/1], Step [11120/48549], Loss: 2.4206\n",
      "Epoch [1/1], Step [11130/48549], Loss: 2.4699\n",
      "Epoch [1/1], Step [11140/48549], Loss: 2.4482\n",
      "Epoch [1/1], Step [11150/48549], Loss: 2.4297\n",
      "Epoch [1/1], Step [11160/48549], Loss: 2.4040\n",
      "Epoch [1/1], Step [11170/48549], Loss: 2.4582\n",
      "Epoch [1/1], Step [11180/48549], Loss: 2.4303\n",
      "Epoch [1/1], Step [11190/48549], Loss: 2.5284\n",
      "Epoch [1/1], Step [11200/48549], Loss: 2.4595\n",
      "Epoch [1/1], Step [11210/48549], Loss: 2.4766\n",
      "Epoch [1/1], Step [11220/48549], Loss: 2.4684\n",
      "Epoch [1/1], Step [11230/48549], Loss: 2.4042\n",
      "Epoch [1/1], Step [11240/48549], Loss: 2.4862\n",
      "Epoch [1/1], Step [11250/48549], Loss: 2.4538\n",
      "Epoch [1/1], Step [11260/48549], Loss: 2.4088\n",
      "Epoch [1/1], Step [11270/48549], Loss: 2.4475\n",
      "Epoch [1/1], Step [11280/48549], Loss: 2.5011\n",
      "Epoch [1/1], Step [11290/48549], Loss: 2.4238\n",
      "Epoch [1/1], Step [11300/48549], Loss: 2.4344\n",
      "Epoch [1/1], Step [11310/48549], Loss: 2.4630\n",
      "Epoch [1/1], Step [11320/48549], Loss: 2.4641\n",
      "Epoch [1/1], Step [11330/48549], Loss: 2.4654\n",
      "Epoch [1/1], Step [11340/48549], Loss: 2.4526\n",
      "Epoch [1/1], Step [11350/48549], Loss: 2.4777\n",
      "Epoch [1/1], Step [11360/48549], Loss: 2.6655\n",
      "Epoch [1/1], Step [11370/48549], Loss: 2.5322\n",
      "Epoch [1/1], Step [11380/48549], Loss: 2.4180\n",
      "Epoch [1/1], Step [11390/48549], Loss: 2.4097\n",
      "Epoch [1/1], Step [11400/48549], Loss: 2.4368\n",
      "Epoch [1/1], Step [11410/48549], Loss: 2.4933\n",
      "Epoch [1/1], Step [11420/48549], Loss: 2.4340\n",
      "Epoch [1/1], Step [11430/48549], Loss: 2.5377\n",
      "Epoch [1/1], Step [11440/48549], Loss: 2.4831\n",
      "Epoch [1/1], Step [11450/48549], Loss: 2.4137\n",
      "Epoch [1/1], Step [11460/48549], Loss: 2.4258\n",
      "Epoch [1/1], Step [11470/48549], Loss: 2.4592\n",
      "Epoch [1/1], Step [11480/48549], Loss: 2.4374\n",
      "Epoch [1/1], Step [11490/48549], Loss: 2.4538\n",
      "Epoch [1/1], Step [11500/48549], Loss: 2.4532\n",
      "Epoch [1/1], Step [11510/48549], Loss: 2.4570\n",
      "Epoch [1/1], Step [11520/48549], Loss: 2.5274\n",
      "Epoch [1/1], Step [11530/48549], Loss: 2.4340\n",
      "Epoch [1/1], Step [11540/48549], Loss: 2.5234\n",
      "Epoch [1/1], Step [11550/48549], Loss: 2.4276\n",
      "Epoch [1/1], Step [11560/48549], Loss: 2.4758\n",
      "Epoch [1/1], Step [11570/48549], Loss: 2.4925\n",
      "Epoch [1/1], Step [11580/48549], Loss: 2.4616\n",
      "Epoch [1/1], Step [11590/48549], Loss: 2.4623\n",
      "Epoch [1/1], Step [11600/48549], Loss: 2.4740\n",
      "Epoch [1/1], Step [11610/48549], Loss: 2.4392\n",
      "Epoch [1/1], Step [11620/48549], Loss: 2.4435\n",
      "Epoch [1/1], Step [11630/48549], Loss: 2.4015\n",
      "Epoch [1/1], Step [11640/48549], Loss: 2.4826\n",
      "Epoch [1/1], Step [11650/48549], Loss: 2.4479\n",
      "Epoch [1/1], Step [11660/48549], Loss: 2.3861\n",
      "Epoch [1/1], Step [11670/48549], Loss: 2.6586\n",
      "Epoch [1/1], Step [11680/48549], Loss: 2.4789\n",
      "Epoch [1/1], Step [11690/48549], Loss: 2.4005\n",
      "Epoch [1/1], Step [11700/48549], Loss: 2.4441\n",
      "Epoch [1/1], Step [11710/48549], Loss: 2.4840\n",
      "Epoch [1/1], Step [11720/48549], Loss: 2.4036\n",
      "Epoch [1/1], Step [11730/48549], Loss: 2.4653\n",
      "Epoch [1/1], Step [11740/48549], Loss: 2.4808\n",
      "Epoch [1/1], Step [11750/48549], Loss: 2.4440\n",
      "Epoch [1/1], Step [11760/48549], Loss: 2.3929\n",
      "Epoch [1/1], Step [11770/48549], Loss: 2.4326\n",
      "Epoch [1/1], Step [11780/48549], Loss: 2.4582\n",
      "Epoch [1/1], Step [11790/48549], Loss: 2.4284\n",
      "Epoch [1/1], Step [11800/48549], Loss: 2.4558\n",
      "Epoch [1/1], Step [11810/48549], Loss: 2.4506\n",
      "Epoch [1/1], Step [11820/48549], Loss: 2.4261\n",
      "Epoch [1/1], Step [11830/48549], Loss: 2.4516\n",
      "Epoch [1/1], Step [11840/48549], Loss: 2.5369\n",
      "Epoch [1/1], Step [11850/48549], Loss: 2.4444\n",
      "Epoch [1/1], Step [11860/48549], Loss: 2.4773\n",
      "Epoch [1/1], Step [11870/48549], Loss: 2.4036\n",
      "Epoch [1/1], Step [11880/48549], Loss: 2.4242\n",
      "Epoch [1/1], Step [11890/48549], Loss: 2.4356\n",
      "Epoch [1/1], Step [11900/48549], Loss: 2.4860\n",
      "Epoch [1/1], Step [11910/48549], Loss: 2.3421\n",
      "Epoch [1/1], Step [11920/48549], Loss: 2.4200\n",
      "Epoch [1/1], Step [11930/48549], Loss: 2.4144\n",
      "Epoch [1/1], Step [11940/48549], Loss: 2.4010\n",
      "Epoch [1/1], Step [11950/48549], Loss: 2.4364\n",
      "Epoch [1/1], Step [11960/48549], Loss: 2.4245\n",
      "Epoch [1/1], Step [11970/48549], Loss: 2.5693\n",
      "Epoch [1/1], Step [11980/48549], Loss: 2.4065\n",
      "Epoch [1/1], Step [11990/48549], Loss: 2.5055\n",
      "Epoch [1/1], Step [12000/48549], Loss: 2.4649\n",
      "Epoch [1/1], Step [12010/48549], Loss: 2.4265\n",
      "Epoch [1/1], Step [12020/48549], Loss: 2.4406\n",
      "Epoch [1/1], Step [12030/48549], Loss: 2.4669\n",
      "Epoch [1/1], Step [12040/48549], Loss: 2.4342\n",
      "Epoch [1/1], Step [12050/48549], Loss: 2.4380\n",
      "Epoch [1/1], Step [12060/48549], Loss: 2.4822\n",
      "Epoch [1/1], Step [12070/48549], Loss: 2.4852\n",
      "Epoch [1/1], Step [12080/48549], Loss: 2.4382\n",
      "Epoch [1/1], Step [12090/48549], Loss: 2.4724\n",
      "Epoch [1/1], Step [12100/48549], Loss: 2.3899\n",
      "Epoch [1/1], Step [12110/48549], Loss: 2.4663\n",
      "Epoch [1/1], Step [12120/48549], Loss: 2.4260\n",
      "Epoch [1/1], Step [12130/48549], Loss: 2.4480\n",
      "Epoch [1/1], Step [12140/48549], Loss: 2.4258\n",
      "Epoch [1/1], Step [12150/48549], Loss: 2.4762\n",
      "Epoch [1/1], Step [12160/48549], Loss: 2.4561\n",
      "Epoch [1/1], Step [12170/48549], Loss: 2.4328\n",
      "Epoch [1/1], Step [12180/48549], Loss: 2.4265\n",
      "Epoch [1/1], Step [12190/48549], Loss: 2.4203\n",
      "Epoch [1/1], Step [12200/48549], Loss: 2.4138\n",
      "Epoch [1/1], Step [12210/48549], Loss: 2.4810\n",
      "Epoch [1/1], Step [12220/48549], Loss: 2.4826\n",
      "Epoch [1/1], Step [12230/48549], Loss: 2.4556\n",
      "Epoch [1/1], Step [12240/48549], Loss: 2.4247\n",
      "Epoch [1/1], Step [12250/48549], Loss: 2.5509\n",
      "Epoch [1/1], Step [12260/48549], Loss: 2.4149\n",
      "Epoch [1/1], Step [12270/48549], Loss: 2.4086\n",
      "Epoch [1/1], Step [12280/48549], Loss: 2.4791\n",
      "Epoch [1/1], Step [12290/48549], Loss: 2.4469\n",
      "Epoch [1/1], Step [12300/48549], Loss: 2.4534\n",
      "Epoch [1/1], Step [12310/48549], Loss: 2.4132\n",
      "Epoch [1/1], Step [12320/48549], Loss: 2.4919\n",
      "Epoch [1/1], Step [12330/48549], Loss: 2.4374\n",
      "Epoch [1/1], Step [12340/48549], Loss: 2.4268\n",
      "Epoch [1/1], Step [12350/48549], Loss: 2.4382\n",
      "Epoch [1/1], Step [12360/48549], Loss: 2.4742\n",
      "Epoch [1/1], Step [12370/48549], Loss: 2.4481\n",
      "Epoch [1/1], Step [12380/48549], Loss: 2.4648\n",
      "Epoch [1/1], Step [12390/48549], Loss: 2.3917\n",
      "Epoch [1/1], Step [12400/48549], Loss: 2.4873\n",
      "Epoch [1/1], Step [12410/48549], Loss: 2.4278\n",
      "Epoch [1/1], Step [12420/48549], Loss: 2.4288\n",
      "Epoch [1/1], Step [12430/48549], Loss: 2.4891\n",
      "Epoch [1/1], Step [12440/48549], Loss: 2.4316\n",
      "Epoch [1/1], Step [12450/48549], Loss: 2.3997\n",
      "Epoch [1/1], Step [12460/48549], Loss: 2.4295\n",
      "Epoch [1/1], Step [12470/48549], Loss: 2.4617\n",
      "Epoch [1/1], Step [12480/48549], Loss: 2.4596\n",
      "Epoch [1/1], Step [12490/48549], Loss: 2.4998\n",
      "Epoch [1/1], Step [12500/48549], Loss: 2.4858\n",
      "Epoch [1/1], Step [12510/48549], Loss: 2.4847\n",
      "Epoch [1/1], Step [12520/48549], Loss: 2.4329\n",
      "Epoch [1/1], Step [12530/48549], Loss: 2.4394\n",
      "Epoch [1/1], Step [12540/48549], Loss: 2.4069\n",
      "Epoch [1/1], Step [12550/48549], Loss: 2.4544\n",
      "Epoch [1/1], Step [12560/48549], Loss: 2.4184\n",
      "Epoch [1/1], Step [12570/48549], Loss: 2.4305\n",
      "Epoch [1/1], Step [12580/48549], Loss: 2.4391\n",
      "Epoch [1/1], Step [12590/48549], Loss: 2.4364\n",
      "Epoch [1/1], Step [12600/48549], Loss: 2.5372\n",
      "Epoch [1/1], Step [12610/48549], Loss: 2.5020\n",
      "Epoch [1/1], Step [12620/48549], Loss: 2.4838\n",
      "Epoch [1/1], Step [12630/48549], Loss: 2.4728\n",
      "Epoch [1/1], Step [12640/48549], Loss: 2.3911\n",
      "Epoch [1/1], Step [12650/48549], Loss: 2.3967\n",
      "Epoch [1/1], Step [12660/48549], Loss: 2.4532\n",
      "Epoch [1/1], Step [12670/48549], Loss: 2.4379\n",
      "Epoch [1/1], Step [12680/48549], Loss: 2.4322\n",
      "Epoch [1/1], Step [12690/48549], Loss: 2.5073\n",
      "Epoch [1/1], Step [12700/48549], Loss: 2.5310\n",
      "Epoch [1/1], Step [12710/48549], Loss: 2.4136\n",
      "Epoch [1/1], Step [12720/48549], Loss: 2.3725\n",
      "Epoch [1/1], Step [12730/48549], Loss: 2.3910\n",
      "Epoch [1/1], Step [12740/48549], Loss: 2.4592\n",
      "Epoch [1/1], Step [12750/48549], Loss: 2.4459\n",
      "Epoch [1/1], Step [12760/48549], Loss: 2.4132\n",
      "Epoch [1/1], Step [12770/48549], Loss: 2.4490\n",
      "Epoch [1/1], Step [12780/48549], Loss: 2.4462\n",
      "Epoch [1/1], Step [12790/48549], Loss: 2.4777\n",
      "Epoch [1/1], Step [12800/48549], Loss: 2.3850\n",
      "Epoch [1/1], Step [12810/48549], Loss: 2.4436\n",
      "Epoch [1/1], Step [12820/48549], Loss: 2.4103\n",
      "Epoch [1/1], Step [12830/48549], Loss: 2.4343\n",
      "Epoch [1/1], Step [12840/48549], Loss: 2.4788\n",
      "Epoch [1/1], Step [12850/48549], Loss: 2.4889\n",
      "Epoch [1/1], Step [12860/48549], Loss: 2.4908\n",
      "Epoch [1/1], Step [12870/48549], Loss: 2.4877\n",
      "Epoch [1/1], Step [12880/48549], Loss: 2.4572\n",
      "Epoch [1/1], Step [12890/48549], Loss: 2.4275\n",
      "Epoch [1/1], Step [12900/48549], Loss: 2.4180\n",
      "Epoch [1/1], Step [12910/48549], Loss: 2.4185\n",
      "Epoch [1/1], Step [12920/48549], Loss: 2.4289\n",
      "Epoch [1/1], Step [12930/48549], Loss: 2.5348\n",
      "Epoch [1/1], Step [12940/48549], Loss: 2.4440\n",
      "Epoch [1/1], Step [12950/48549], Loss: 2.4273\n",
      "Epoch [1/1], Step [12960/48549], Loss: 2.3925\n",
      "Epoch [1/1], Step [12970/48549], Loss: 2.4358\n",
      "Epoch [1/1], Step [12980/48549], Loss: 2.4589\n",
      "Epoch [1/1], Step [12990/48549], Loss: 2.4838\n",
      "Epoch [1/1], Step [13000/48549], Loss: 2.4498\n",
      "Epoch [1/1], Step [13010/48549], Loss: 2.4225\n",
      "Epoch [1/1], Step [13020/48549], Loss: 2.4485\n",
      "Epoch [1/1], Step [13030/48549], Loss: 2.4605\n",
      "Epoch [1/1], Step [13040/48549], Loss: 2.4704\n",
      "Epoch [1/1], Step [13050/48549], Loss: 2.4460\n",
      "Epoch [1/1], Step [13060/48549], Loss: 2.4164\n",
      "Epoch [1/1], Step [13070/48549], Loss: 2.4936\n",
      "Epoch [1/1], Step [13080/48549], Loss: 2.4173\n",
      "Epoch [1/1], Step [13090/48549], Loss: 2.4400\n",
      "Epoch [1/1], Step [13100/48549], Loss: 2.4382\n",
      "Epoch [1/1], Step [13110/48549], Loss: 2.4714\n",
      "Epoch [1/1], Step [13120/48549], Loss: 2.4051\n",
      "Epoch [1/1], Step [13130/48549], Loss: 2.4535\n",
      "Epoch [1/1], Step [13140/48549], Loss: 2.4404\n",
      "Epoch [1/1], Step [13150/48549], Loss: 2.4566\n",
      "Epoch [1/1], Step [13160/48549], Loss: 2.4268\n",
      "Epoch [1/1], Step [13170/48549], Loss: 2.4382\n",
      "Epoch [1/1], Step [13180/48549], Loss: 2.3870\n",
      "Epoch [1/1], Step [13190/48549], Loss: 2.4093\n",
      "Epoch [1/1], Step [13200/48549], Loss: 2.4570\n",
      "Epoch [1/1], Step [13210/48549], Loss: 2.4826\n",
      "Epoch [1/1], Step [13220/48549], Loss: 2.4541\n",
      "Epoch [1/1], Step [13230/48549], Loss: 2.3954\n",
      "Epoch [1/1], Step [13240/48549], Loss: 2.4413\n",
      "Epoch [1/1], Step [13250/48549], Loss: 2.4636\n",
      "Epoch [1/1], Step [13260/48549], Loss: 2.4544\n",
      "Epoch [1/1], Step [13270/48549], Loss: 2.4921\n",
      "Epoch [1/1], Step [13280/48549], Loss: 2.4569\n",
      "Epoch [1/1], Step [13290/48549], Loss: 2.4465\n",
      "Epoch [1/1], Step [13300/48549], Loss: 2.3991\n",
      "Epoch [1/1], Step [13310/48549], Loss: 2.4399\n",
      "Epoch [1/1], Step [13320/48549], Loss: 2.3991\n",
      "Epoch [1/1], Step [13330/48549], Loss: 2.4052\n",
      "Epoch [1/1], Step [13340/48549], Loss: 2.4169\n",
      "Epoch [1/1], Step [13350/48549], Loss: 2.4539\n",
      "Epoch [1/1], Step [13360/48549], Loss: 2.4612\n",
      "Epoch [1/1], Step [13370/48549], Loss: 2.4452\n",
      "Epoch [1/1], Step [13380/48549], Loss: 2.4236\n",
      "Epoch [1/1], Step [13390/48549], Loss: 2.4630\n",
      "Epoch [1/1], Step [13400/48549], Loss: 2.4755\n",
      "Epoch [1/1], Step [13410/48549], Loss: 2.4181\n",
      "Epoch [1/1], Step [13420/48549], Loss: 2.4348\n",
      "Epoch [1/1], Step [13430/48549], Loss: 2.6344\n",
      "Epoch [1/1], Step [13440/48549], Loss: 2.4419\n",
      "Epoch [1/1], Step [13450/48549], Loss: 2.4288\n",
      "Epoch [1/1], Step [13460/48549], Loss: 2.4878\n",
      "Epoch [1/1], Step [13470/48549], Loss: 2.4487\n",
      "Epoch [1/1], Step [13480/48549], Loss: 2.4539\n",
      "Epoch [1/1], Step [13490/48549], Loss: 2.5051\n",
      "Epoch [1/1], Step [13500/48549], Loss: 2.4484\n",
      "Epoch [1/1], Step [13510/48549], Loss: 2.4273\n",
      "Epoch [1/1], Step [13520/48549], Loss: 2.4078\n",
      "Epoch [1/1], Step [13530/48549], Loss: 2.4854\n",
      "Epoch [1/1], Step [13540/48549], Loss: 2.4666\n",
      "Epoch [1/1], Step [13550/48549], Loss: 2.4578\n",
      "Epoch [1/1], Step [13560/48549], Loss: 2.4445\n",
      "Epoch [1/1], Step [13570/48549], Loss: 2.4623\n",
      "Epoch [1/1], Step [13580/48549], Loss: 2.4525\n",
      "Epoch [1/1], Step [13590/48549], Loss: 2.4621\n",
      "Epoch [1/1], Step [13600/48549], Loss: 2.4131\n",
      "Epoch [1/1], Step [13610/48549], Loss: 2.4733\n",
      "Epoch [1/1], Step [13620/48549], Loss: 2.4721\n",
      "Epoch [1/1], Step [13630/48549], Loss: 2.4266\n",
      "Epoch [1/1], Step [13640/48549], Loss: 2.4463\n",
      "Epoch [1/1], Step [13650/48549], Loss: 2.5099\n",
      "Epoch [1/1], Step [13660/48549], Loss: 2.4330\n",
      "Epoch [1/1], Step [13670/48549], Loss: 2.4489\n",
      "Epoch [1/1], Step [13680/48549], Loss: 2.4354\n",
      "Epoch [1/1], Step [13690/48549], Loss: 2.4427\n",
      "Epoch [1/1], Step [13700/48549], Loss: 2.4551\n",
      "Epoch [1/1], Step [13710/48549], Loss: 2.4501\n",
      "Epoch [1/1], Step [13720/48549], Loss: 2.4160\n",
      "Epoch [1/1], Step [13730/48549], Loss: 2.4671\n",
      "Epoch [1/1], Step [13740/48549], Loss: 2.5013\n",
      "Epoch [1/1], Step [13750/48549], Loss: 2.4787\n",
      "Epoch [1/1], Step [13760/48549], Loss: 2.3824\n",
      "Epoch [1/1], Step [13770/48549], Loss: 2.4445\n",
      "Epoch [1/1], Step [13780/48549], Loss: 2.4669\n",
      "Epoch [1/1], Step [13790/48549], Loss: 2.4647\n",
      "Epoch [1/1], Step [13800/48549], Loss: 2.4434\n",
      "Epoch [1/1], Step [13810/48549], Loss: 2.5970\n",
      "Epoch [1/1], Step [13820/48549], Loss: 2.4654\n",
      "Epoch [1/1], Step [13830/48549], Loss: 2.4608\n",
      "Epoch [1/1], Step [13840/48549], Loss: 2.4526\n",
      "Epoch [1/1], Step [13850/48549], Loss: 2.4747\n",
      "Epoch [1/1], Step [13860/48549], Loss: 2.5062\n",
      "Epoch [1/1], Step [13870/48549], Loss: 2.4136\n",
      "Epoch [1/1], Step [13880/48549], Loss: 2.4046\n",
      "Epoch [1/1], Step [13890/48549], Loss: 2.4749\n",
      "Epoch [1/1], Step [13900/48549], Loss: 2.4323\n",
      "Epoch [1/1], Step [13910/48549], Loss: 2.4468\n",
      "Epoch [1/1], Step [13920/48549], Loss: 2.4373\n",
      "Epoch [1/1], Step [13930/48549], Loss: 2.4328\n",
      "Epoch [1/1], Step [13940/48549], Loss: 2.4074\n",
      "Epoch [1/1], Step [13950/48549], Loss: 2.4523\n",
      "Epoch [1/1], Step [13960/48549], Loss: 2.4108\n",
      "Epoch [1/1], Step [13970/48549], Loss: 2.4356\n",
      "Epoch [1/1], Step [13980/48549], Loss: 2.4625\n",
      "Epoch [1/1], Step [13990/48549], Loss: 2.4755\n",
      "Epoch [1/1], Step [14000/48549], Loss: 2.4938\n",
      "Epoch [1/1], Step [14010/48549], Loss: 2.4653\n",
      "Epoch [1/1], Step [14020/48549], Loss: 2.4456\n",
      "Epoch [1/1], Step [14030/48549], Loss: 2.4276\n",
      "Epoch [1/1], Step [14040/48549], Loss: 2.4749\n",
      "Epoch [1/1], Step [14050/48549], Loss: 2.4925\n",
      "Epoch [1/1], Step [14060/48549], Loss: 2.4322\n",
      "Epoch [1/1], Step [14070/48549], Loss: 2.4791\n",
      "Epoch [1/1], Step [14080/48549], Loss: 2.4827\n",
      "Epoch [1/1], Step [14090/48549], Loss: 2.4034\n",
      "Epoch [1/1], Step [14100/48549], Loss: 2.4250\n",
      "Epoch [1/1], Step [14110/48549], Loss: 2.4763\n",
      "Epoch [1/1], Step [14120/48549], Loss: 2.4595\n",
      "Epoch [1/1], Step [14130/48549], Loss: 2.4830\n",
      "Epoch [1/1], Step [14140/48549], Loss: 2.4631\n",
      "Epoch [1/1], Step [14150/48549], Loss: 2.4143\n",
      "Epoch [1/1], Step [14160/48549], Loss: 2.4594\n",
      "Epoch [1/1], Step [14170/48549], Loss: 2.4901\n",
      "Epoch [1/1], Step [14180/48549], Loss: 2.4580\n",
      "Epoch [1/1], Step [14190/48549], Loss: 2.3896\n",
      "Epoch [1/1], Step [14200/48549], Loss: 2.4604\n",
      "Epoch [1/1], Step [14210/48549], Loss: 2.4350\n",
      "Epoch [1/1], Step [14220/48549], Loss: 2.4513\n",
      "Epoch [1/1], Step [14230/48549], Loss: 2.5408\n",
      "Epoch [1/1], Step [14240/48549], Loss: 2.4650\n",
      "Epoch [1/1], Step [14250/48549], Loss: 2.4616\n",
      "Epoch [1/1], Step [14260/48549], Loss: 2.4656\n",
      "Epoch [1/1], Step [14270/48549], Loss: 2.4528\n",
      "Epoch [1/1], Step [14280/48549], Loss: 2.4560\n",
      "Epoch [1/1], Step [14290/48549], Loss: 2.3625\n",
      "Epoch [1/1], Step [14300/48549], Loss: 2.4919\n",
      "Epoch [1/1], Step [14310/48549], Loss: 2.4305\n",
      "Epoch [1/1], Step [14320/48549], Loss: 2.4343\n",
      "Epoch [1/1], Step [14330/48549], Loss: 2.4312\n",
      "Epoch [1/1], Step [14340/48549], Loss: 2.4277\n",
      "Epoch [1/1], Step [14350/48549], Loss: 2.4317\n",
      "Epoch [1/1], Step [14360/48549], Loss: 2.4318\n",
      "Epoch [1/1], Step [14370/48549], Loss: 2.4510\n",
      "Epoch [1/1], Step [14380/48549], Loss: 2.4014\n",
      "Epoch [1/1], Step [14390/48549], Loss: 2.3820\n",
      "Epoch [1/1], Step [14400/48549], Loss: 2.4069\n",
      "Epoch [1/1], Step [14410/48549], Loss: 2.4216\n",
      "Epoch [1/1], Step [14420/48549], Loss: 2.3828\n",
      "Epoch [1/1], Step [14430/48549], Loss: 2.3990\n",
      "Epoch [1/1], Step [14440/48549], Loss: 2.4718\n",
      "Epoch [1/1], Step [14450/48549], Loss: 2.4347\n",
      "Epoch [1/1], Step [14460/48549], Loss: 2.3967\n",
      "Epoch [1/1], Step [14470/48549], Loss: 2.4418\n",
      "Epoch [1/1], Step [14480/48549], Loss: 2.7123\n",
      "Epoch [1/1], Step [14490/48549], Loss: 2.4680\n",
      "Epoch [1/1], Step [14500/48549], Loss: 2.4895\n",
      "Epoch [1/1], Step [14510/48549], Loss: 2.4547\n",
      "Epoch [1/1], Step [14520/48549], Loss: 2.4646\n",
      "Epoch [1/1], Step [14530/48549], Loss: 2.4623\n",
      "Epoch [1/1], Step [14540/48549], Loss: 2.4598\n",
      "Epoch [1/1], Step [14550/48549], Loss: 2.4863\n",
      "Epoch [1/1], Step [14560/48549], Loss: 2.3753\n",
      "Epoch [1/1], Step [14570/48549], Loss: 2.3500\n",
      "Epoch [1/1], Step [14580/48549], Loss: 2.4563\n",
      "Epoch [1/1], Step [14590/48549], Loss: 2.4521\n",
      "Epoch [1/1], Step [14600/48549], Loss: 2.4838\n",
      "Epoch [1/1], Step [14610/48549], Loss: 2.4240\n",
      "Epoch [1/1], Step [14620/48549], Loss: 2.4389\n",
      "Epoch [1/1], Step [14630/48549], Loss: 2.4315\n",
      "Epoch [1/1], Step [14640/48549], Loss: 2.4449\n",
      "Epoch [1/1], Step [14650/48549], Loss: 2.4490\n",
      "Epoch [1/1], Step [14660/48549], Loss: 2.7137\n",
      "Epoch [1/1], Step [14670/48549], Loss: 2.4567\n",
      "Epoch [1/1], Step [14680/48549], Loss: 2.5306\n",
      "Epoch [1/1], Step [14690/48549], Loss: 2.5362\n",
      "Epoch [1/1], Step [14700/48549], Loss: 2.4899\n",
      "Epoch [1/1], Step [14710/48549], Loss: 2.4137\n",
      "Epoch [1/1], Step [14720/48549], Loss: 2.3941\n",
      "Epoch [1/1], Step [14730/48549], Loss: 2.4674\n",
      "Epoch [1/1], Step [14740/48549], Loss: 2.4426\n",
      "Epoch [1/1], Step [14750/48549], Loss: 2.4362\n",
      "Epoch [1/1], Step [14760/48549], Loss: 2.3978\n",
      "Epoch [1/1], Step [14770/48549], Loss: 2.3921\n",
      "Epoch [1/1], Step [14780/48549], Loss: 2.4336\n",
      "Epoch [1/1], Step [14790/48549], Loss: 2.4631\n",
      "Epoch [1/1], Step [14800/48549], Loss: 2.4618\n",
      "Epoch [1/1], Step [14810/48549], Loss: 2.4392\n",
      "Epoch [1/1], Step [14820/48549], Loss: 2.4474\n",
      "Epoch [1/1], Step [14830/48549], Loss: 2.4594\n",
      "Epoch [1/1], Step [14840/48549], Loss: 2.4634\n",
      "Epoch [1/1], Step [14850/48549], Loss: 2.4153\n",
      "Epoch [1/1], Step [14860/48549], Loss: 2.4319\n",
      "Epoch [1/1], Step [14870/48549], Loss: 2.4726\n",
      "Epoch [1/1], Step [14880/48549], Loss: 2.4758\n",
      "Epoch [1/1], Step [14890/48549], Loss: 2.4238\n",
      "Epoch [1/1], Step [14900/48549], Loss: 2.4378\n",
      "Epoch [1/1], Step [14910/48549], Loss: 2.4535\n",
      "Epoch [1/1], Step [14920/48549], Loss: 2.4458\n",
      "Epoch [1/1], Step [14930/48549], Loss: 2.4570\n",
      "Epoch [1/1], Step [14940/48549], Loss: 2.4085\n",
      "Epoch [1/1], Step [14950/48549], Loss: 2.6111\n",
      "Epoch [1/1], Step [14960/48549], Loss: 2.4503\n",
      "Epoch [1/1], Step [14970/48549], Loss: 2.4659\n",
      "Epoch [1/1], Step [14980/48549], Loss: 2.4884\n",
      "Epoch [1/1], Step [14990/48549], Loss: 2.4325\n",
      "Epoch [1/1], Step [15000/48549], Loss: 2.4482\n",
      "Epoch [1/1], Step [15010/48549], Loss: 2.5301\n",
      "Epoch [1/1], Step [15020/48549], Loss: 2.4190\n",
      "Epoch [1/1], Step [15030/48549], Loss: 2.4290\n",
      "Epoch [1/1], Step [15040/48549], Loss: 2.3833\n",
      "Epoch [1/1], Step [15050/48549], Loss: 2.3682\n",
      "Epoch [1/1], Step [15060/48549], Loss: 2.4243\n",
      "Epoch [1/1], Step [15070/48549], Loss: 2.4303\n",
      "Epoch [1/1], Step [15080/48549], Loss: 2.4489\n",
      "Epoch [1/1], Step [15090/48549], Loss: 2.4763\n",
      "Epoch [1/1], Step [15100/48549], Loss: 2.4097\n",
      "Epoch [1/1], Step [15110/48549], Loss: 2.4188\n",
      "Epoch [1/1], Step [15120/48549], Loss: 2.4584\n",
      "Epoch [1/1], Step [15130/48549], Loss: 2.4695\n",
      "Epoch [1/1], Step [15140/48549], Loss: 2.4600\n",
      "Epoch [1/1], Step [15150/48549], Loss: 2.4417\n",
      "Epoch [1/1], Step [15160/48549], Loss: 2.4121\n",
      "Epoch [1/1], Step [15170/48549], Loss: 2.4718\n",
      "Epoch [1/1], Step [15180/48549], Loss: 2.4407\n",
      "Epoch [1/1], Step [15190/48549], Loss: 2.4676\n",
      "Epoch [1/1], Step [15200/48549], Loss: 2.4027\n",
      "Epoch [1/1], Step [15210/48549], Loss: 2.5397\n",
      "Epoch [1/1], Step [15220/48549], Loss: 2.4586\n",
      "Epoch [1/1], Step [15230/48549], Loss: 2.4355\n",
      "Epoch [1/1], Step [15240/48549], Loss: 2.4105\n",
      "Epoch [1/1], Step [15250/48549], Loss: 2.4659\n",
      "Epoch [1/1], Step [15260/48549], Loss: 2.4207\n",
      "Epoch [1/1], Step [15270/48549], Loss: 2.4723\n",
      "Epoch [1/1], Step [15280/48549], Loss: 2.4698\n",
      "Epoch [1/1], Step [15290/48549], Loss: 2.5131\n",
      "Epoch [1/1], Step [15300/48549], Loss: 2.4447\n",
      "Epoch [1/1], Step [15310/48549], Loss: 2.4571\n",
      "Epoch [1/1], Step [15320/48549], Loss: 2.4041\n",
      "Epoch [1/1], Step [15330/48549], Loss: 2.4438\n",
      "Epoch [1/1], Step [15340/48549], Loss: 2.4337\n",
      "Epoch [1/1], Step [15350/48549], Loss: 2.5160\n",
      "Epoch [1/1], Step [15360/48549], Loss: 2.4524\n",
      "Epoch [1/1], Step [15370/48549], Loss: 2.4723\n",
      "Epoch [1/1], Step [15380/48549], Loss: 2.4309\n",
      "Epoch [1/1], Step [15390/48549], Loss: 2.4643\n",
      "Epoch [1/1], Step [15400/48549], Loss: 2.3777\n",
      "Epoch [1/1], Step [15410/48549], Loss: 2.4446\n",
      "Epoch [1/1], Step [15420/48549], Loss: 2.4418\n",
      "Epoch [1/1], Step [15430/48549], Loss: 2.3836\n",
      "Epoch [1/1], Step [15440/48549], Loss: 2.5340\n",
      "Epoch [1/1], Step [15450/48549], Loss: 2.5040\n",
      "Epoch [1/1], Step [15460/48549], Loss: 2.5064\n",
      "Epoch [1/1], Step [15470/48549], Loss: 2.4267\n",
      "Epoch [1/1], Step [15480/48549], Loss: 2.4179\n",
      "Epoch [1/1], Step [15490/48549], Loss: 2.4227\n",
      "Epoch [1/1], Step [15500/48549], Loss: 2.4579\n",
      "Epoch [1/1], Step [15510/48549], Loss: 2.4899\n",
      "Epoch [1/1], Step [15520/48549], Loss: 2.4696\n",
      "Epoch [1/1], Step [15530/48549], Loss: 2.4757\n",
      "Epoch [1/1], Step [15540/48549], Loss: 2.4553\n",
      "Epoch [1/1], Step [15550/48549], Loss: 2.4862\n",
      "Epoch [1/1], Step [15560/48549], Loss: 2.4452\n",
      "Epoch [1/1], Step [15570/48549], Loss: 2.5160\n",
      "Epoch [1/1], Step [15580/48549], Loss: 2.4594\n",
      "Epoch [1/1], Step [15590/48549], Loss: 2.4505\n",
      "Epoch [1/1], Step [15600/48549], Loss: 2.5938\n",
      "Epoch [1/1], Step [15610/48549], Loss: 2.4559\n",
      "Epoch [1/1], Step [15620/48549], Loss: 2.4679\n",
      "Epoch [1/1], Step [15630/48549], Loss: 2.4784\n",
      "Epoch [1/1], Step [15640/48549], Loss: 2.4905\n",
      "Epoch [1/1], Step [15650/48549], Loss: 2.4363\n",
      "Epoch [1/1], Step [15660/48549], Loss: 2.4287\n",
      "Epoch [1/1], Step [15670/48549], Loss: 2.4625\n",
      "Epoch [1/1], Step [15680/48549], Loss: 2.5231\n",
      "Epoch [1/1], Step [15690/48549], Loss: 2.4183\n",
      "Epoch [1/1], Step [15700/48549], Loss: 2.4410\n",
      "Epoch [1/1], Step [15710/48549], Loss: 2.4344\n",
      "Epoch [1/1], Step [15720/48549], Loss: 2.4450\n",
      "Epoch [1/1], Step [15730/48549], Loss: 2.4293\n",
      "Epoch [1/1], Step [15740/48549], Loss: 2.4312\n",
      "Epoch [1/1], Step [15750/48549], Loss: 2.4009\n",
      "Epoch [1/1], Step [15760/48549], Loss: 2.4039\n",
      "Epoch [1/1], Step [15770/48549], Loss: 2.4115\n",
      "Epoch [1/1], Step [15780/48549], Loss: 2.4563\n",
      "Epoch [1/1], Step [15790/48549], Loss: 2.5040\n",
      "Epoch [1/1], Step [15800/48549], Loss: 2.3930\n",
      "Epoch [1/1], Step [15810/48549], Loss: 2.4682\n",
      "Epoch [1/1], Step [15820/48549], Loss: 2.4905\n",
      "Epoch [1/1], Step [15830/48549], Loss: 2.4334\n",
      "Epoch [1/1], Step [15840/48549], Loss: 2.5042\n",
      "Epoch [1/1], Step [15850/48549], Loss: 2.5349\n",
      "Epoch [1/1], Step [15860/48549], Loss: 2.4581\n",
      "Epoch [1/1], Step [15870/48549], Loss: 2.3818\n",
      "Epoch [1/1], Step [15880/48549], Loss: 2.5028\n",
      "Epoch [1/1], Step [15890/48549], Loss: 2.4504\n",
      "Epoch [1/1], Step [15900/48549], Loss: 2.4092\n",
      "Epoch [1/1], Step [15910/48549], Loss: 2.4678\n",
      "Epoch [1/1], Step [15920/48549], Loss: 2.4901\n",
      "Epoch [1/1], Step [15930/48549], Loss: 2.5157\n",
      "Epoch [1/1], Step [15940/48549], Loss: 2.4627\n",
      "Epoch [1/1], Step [15950/48549], Loss: 2.4653\n",
      "Epoch [1/1], Step [15960/48549], Loss: 2.5983\n",
      "Epoch [1/1], Step [15970/48549], Loss: 2.5014\n",
      "Epoch [1/1], Step [15980/48549], Loss: 2.4962\n",
      "Epoch [1/1], Step [15990/48549], Loss: 2.4525\n",
      "Epoch [1/1], Step [16000/48549], Loss: 2.5216\n",
      "Epoch [1/1], Step [16010/48549], Loss: 2.4715\n",
      "Epoch [1/1], Step [16020/48549], Loss: 2.4866\n",
      "Epoch [1/1], Step [16030/48549], Loss: 2.3992\n",
      "Epoch [1/1], Step [16040/48549], Loss: 2.5558\n",
      "Epoch [1/1], Step [16050/48549], Loss: 2.4176\n",
      "Epoch [1/1], Step [16060/48549], Loss: 2.4073\n",
      "Epoch [1/1], Step [16070/48549], Loss: 2.4229\n",
      "Epoch [1/1], Step [16080/48549], Loss: 2.4245\n",
      "Epoch [1/1], Step [16090/48549], Loss: 2.4661\n",
      "Epoch [1/1], Step [16100/48549], Loss: 2.4241\n",
      "Epoch [1/1], Step [16110/48549], Loss: 2.4319\n",
      "Epoch [1/1], Step [16120/48549], Loss: 2.4664\n",
      "Epoch [1/1], Step [16130/48549], Loss: 2.4947\n",
      "Epoch [1/1], Step [16140/48549], Loss: 2.4703\n",
      "Epoch [1/1], Step [16150/48549], Loss: 2.4076\n",
      "Epoch [1/1], Step [16160/48549], Loss: 2.4626\n",
      "Epoch [1/1], Step [16170/48549], Loss: 2.4204\n",
      "Epoch [1/1], Step [16180/48549], Loss: 2.4601\n",
      "Epoch [1/1], Step [16190/48549], Loss: 2.4712\n",
      "Epoch [1/1], Step [16200/48549], Loss: 2.4280\n",
      "Epoch [1/1], Step [16210/48549], Loss: 2.4172\n",
      "Epoch [1/1], Step [16220/48549], Loss: 2.4165\n",
      "Epoch [1/1], Step [16230/48549], Loss: 2.4586\n",
      "Epoch [1/1], Step [16240/48549], Loss: 2.4622\n",
      "Epoch [1/1], Step [16250/48549], Loss: 2.4766\n",
      "Epoch [1/1], Step [16260/48549], Loss: 2.4178\n",
      "Epoch [1/1], Step [16270/48549], Loss: 2.4224\n",
      "Epoch [1/1], Step [16280/48549], Loss: 2.5451\n",
      "Epoch [1/1], Step [16290/48549], Loss: 2.4341\n",
      "Epoch [1/1], Step [16300/48549], Loss: 2.4615\n",
      "Epoch [1/1], Step [16310/48549], Loss: 2.4271\n",
      "Epoch [1/1], Step [16320/48549], Loss: 2.3971\n",
      "Epoch [1/1], Step [16330/48549], Loss: 2.3824\n",
      "Epoch [1/1], Step [16340/48549], Loss: 2.4688\n",
      "Epoch [1/1], Step [16350/48549], Loss: 2.4988\n",
      "Epoch [1/1], Step [16360/48549], Loss: 2.4757\n",
      "Epoch [1/1], Step [16370/48549], Loss: 2.3992\n",
      "Epoch [1/1], Step [16380/48549], Loss: 2.4304\n",
      "Epoch [1/1], Step [16390/48549], Loss: 2.3952\n",
      "Epoch [1/1], Step [16400/48549], Loss: 2.3868\n",
      "Epoch [1/1], Step [16410/48549], Loss: 2.4735\n",
      "Epoch [1/1], Step [16420/48549], Loss: 2.4502\n",
      "Epoch [1/1], Step [16430/48549], Loss: 2.4278\n",
      "Epoch [1/1], Step [16440/48549], Loss: 2.4175\n",
      "Epoch [1/1], Step [16450/48549], Loss: 2.4316\n",
      "Epoch [1/1], Step [16460/48549], Loss: 2.4136\n",
      "Epoch [1/1], Step [16470/48549], Loss: 2.4163\n",
      "Epoch [1/1], Step [16480/48549], Loss: 2.4280\n",
      "Epoch [1/1], Step [16490/48549], Loss: 2.4126\n",
      "Epoch [1/1], Step [16500/48549], Loss: 2.4938\n",
      "Epoch [1/1], Step [16510/48549], Loss: 2.4346\n",
      "Epoch [1/1], Step [16520/48549], Loss: 2.4084\n",
      "Epoch [1/1], Step [16530/48549], Loss: 2.5051\n",
      "Epoch [1/1], Step [16540/48549], Loss: 2.4402\n",
      "Epoch [1/1], Step [16550/48549], Loss: 2.4552\n",
      "Epoch [1/1], Step [16560/48549], Loss: 2.5798\n",
      "Epoch [1/1], Step [16570/48549], Loss: 2.4149\n",
      "Epoch [1/1], Step [16580/48549], Loss: 2.4257\n",
      "Epoch [1/1], Step [16590/48549], Loss: 2.4320\n",
      "Epoch [1/1], Step [16600/48549], Loss: 2.4418\n",
      "Epoch [1/1], Step [16610/48549], Loss: 2.4532\n",
      "Epoch [1/1], Step [16620/48549], Loss: 2.4037\n",
      "Epoch [1/1], Step [16630/48549], Loss: 2.4527\n",
      "Epoch [1/1], Step [16640/48549], Loss: 2.4367\n",
      "Epoch [1/1], Step [16650/48549], Loss: 2.4617\n",
      "Epoch [1/1], Step [16660/48549], Loss: 2.4191\n",
      "Epoch [1/1], Step [16670/48549], Loss: 2.4360\n",
      "Epoch [1/1], Step [16680/48549], Loss: 2.4164\n",
      "Epoch [1/1], Step [16690/48549], Loss: 2.4114\n",
      "Epoch [1/1], Step [16700/48549], Loss: 2.5044\n",
      "Epoch [1/1], Step [16710/48549], Loss: 2.4109\n",
      "Epoch [1/1], Step [16720/48549], Loss: 2.4736\n",
      "Epoch [1/1], Step [16730/48549], Loss: 2.4260\n",
      "Epoch [1/1], Step [16740/48549], Loss: 2.5222\n",
      "Epoch [1/1], Step [16750/48549], Loss: 2.4085\n",
      "Epoch [1/1], Step [16760/48549], Loss: 2.4300\n",
      "Epoch [1/1], Step [16770/48549], Loss: 2.4268\n",
      "Epoch [1/1], Step [16780/48549], Loss: 2.5536\n",
      "Epoch [1/1], Step [16790/48549], Loss: 2.4262\n",
      "Epoch [1/1], Step [16800/48549], Loss: 2.4681\n",
      "Epoch [1/1], Step [16810/48549], Loss: 2.5020\n",
      "Epoch [1/1], Step [16820/48549], Loss: 2.3898\n",
      "Epoch [1/1], Step [16830/48549], Loss: 2.4543\n",
      "Epoch [1/1], Step [16840/48549], Loss: 2.4249\n",
      "Epoch [1/1], Step [16850/48549], Loss: 2.4703\n",
      "Epoch [1/1], Step [16860/48549], Loss: 2.4705\n",
      "Epoch [1/1], Step [16870/48549], Loss: 2.4459\n",
      "Epoch [1/1], Step [16880/48549], Loss: 2.4756\n",
      "Epoch [1/1], Step [16890/48549], Loss: 2.4316\n",
      "Epoch [1/1], Step [16900/48549], Loss: 2.4321\n",
      "Epoch [1/1], Step [16910/48549], Loss: 2.4496\n",
      "Epoch [1/1], Step [16920/48549], Loss: 2.4388\n",
      "Epoch [1/1], Step [16930/48549], Loss: 2.6512\n",
      "Epoch [1/1], Step [16940/48549], Loss: 2.4224\n",
      "Epoch [1/1], Step [16950/48549], Loss: 2.4285\n",
      "Epoch [1/1], Step [16960/48549], Loss: 2.4066\n",
      "Epoch [1/1], Step [16970/48549], Loss: 2.4965\n",
      "Epoch [1/1], Step [16980/48549], Loss: 2.3855\n",
      "Epoch [1/1], Step [16990/48549], Loss: 2.4836\n",
      "Epoch [1/1], Step [17000/48549], Loss: 2.4123\n",
      "Epoch [1/1], Step [17010/48549], Loss: 2.4469\n",
      "Epoch [1/1], Step [17020/48549], Loss: 2.5015\n",
      "Epoch [1/1], Step [17030/48549], Loss: 2.4487\n",
      "Epoch [1/1], Step [17040/48549], Loss: 2.4504\n",
      "Epoch [1/1], Step [17050/48549], Loss: 2.4604\n",
      "Epoch [1/1], Step [17060/48549], Loss: 2.4212\n",
      "Epoch [1/1], Step [17070/48549], Loss: 2.3913\n",
      "Epoch [1/1], Step [17080/48549], Loss: 2.3834\n",
      "Epoch [1/1], Step [17090/48549], Loss: 2.4744\n",
      "Epoch [1/1], Step [17100/48549], Loss: 2.4074\n",
      "Epoch [1/1], Step [17110/48549], Loss: 2.4674\n",
      "Epoch [1/1], Step [17120/48549], Loss: 2.4709\n",
      "Epoch [1/1], Step [17130/48549], Loss: 2.4427\n",
      "Epoch [1/1], Step [17140/48549], Loss: 2.4340\n",
      "Epoch [1/1], Step [17150/48549], Loss: 2.4582\n",
      "Epoch [1/1], Step [17160/48549], Loss: 2.4527\n",
      "Epoch [1/1], Step [17170/48549], Loss: 2.4343\n",
      "Epoch [1/1], Step [17180/48549], Loss: 2.4471\n",
      "Epoch [1/1], Step [17190/48549], Loss: 2.4369\n",
      "Epoch [1/1], Step [17200/48549], Loss: 2.4184\n",
      "Epoch [1/1], Step [17210/48549], Loss: 2.4178\n",
      "Epoch [1/1], Step [17220/48549], Loss: 2.4118\n",
      "Epoch [1/1], Step [17230/48549], Loss: 2.4643\n",
      "Epoch [1/1], Step [17240/48549], Loss: 2.4697\n",
      "Epoch [1/1], Step [17250/48549], Loss: 2.4879\n",
      "Epoch [1/1], Step [17260/48549], Loss: 2.4708\n",
      "Epoch [1/1], Step [17270/48549], Loss: 2.4570\n",
      "Epoch [1/1], Step [17280/48549], Loss: 2.4746\n",
      "Epoch [1/1], Step [17290/48549], Loss: 2.4050\n",
      "Epoch [1/1], Step [17300/48549], Loss: 2.5096\n",
      "Epoch [1/1], Step [17310/48549], Loss: 2.4375\n",
      "Epoch [1/1], Step [17320/48549], Loss: 2.4655\n",
      "Epoch [1/1], Step [17330/48549], Loss: 2.4919\n",
      "Epoch [1/1], Step [17340/48549], Loss: 2.4784\n",
      "Epoch [1/1], Step [17350/48549], Loss: 2.4466\n",
      "Epoch [1/1], Step [17360/48549], Loss: 2.4065\n",
      "Epoch [1/1], Step [17370/48549], Loss: 2.4917\n",
      "Epoch [1/1], Step [17380/48549], Loss: 2.4673\n",
      "Epoch [1/1], Step [17390/48549], Loss: 2.4602\n",
      "Epoch [1/1], Step [17400/48549], Loss: 2.4758\n",
      "Epoch [1/1], Step [17410/48549], Loss: 2.5144\n",
      "Epoch [1/1], Step [17420/48549], Loss: 2.5049\n",
      "Epoch [1/1], Step [17430/48549], Loss: 2.4403\n",
      "Epoch [1/1], Step [17440/48549], Loss: 2.4685\n",
      "Epoch [1/1], Step [17450/48549], Loss: 2.4527\n",
      "Epoch [1/1], Step [17460/48549], Loss: 2.4589\n",
      "Epoch [1/1], Step [17470/48549], Loss: 2.4438\n",
      "Epoch [1/1], Step [17480/48549], Loss: 2.4416\n",
      "Epoch [1/1], Step [17490/48549], Loss: 2.4199\n",
      "Epoch [1/1], Step [17500/48549], Loss: 2.5163\n",
      "Epoch [1/1], Step [17510/48549], Loss: 2.3787\n",
      "Epoch [1/1], Step [17520/48549], Loss: 2.4622\n",
      "Epoch [1/1], Step [17530/48549], Loss: 2.6048\n",
      "Epoch [1/1], Step [17540/48549], Loss: 2.5586\n",
      "Epoch [1/1], Step [17550/48549], Loss: 2.4514\n",
      "Epoch [1/1], Step [17560/48549], Loss: 2.4398\n",
      "Epoch [1/1], Step [17570/48549], Loss: 2.5752\n",
      "Epoch [1/1], Step [17580/48549], Loss: 2.4665\n",
      "Epoch [1/1], Step [17590/48549], Loss: 2.4396\n",
      "Epoch [1/1], Step [17600/48549], Loss: 2.4324\n",
      "Epoch [1/1], Step [17610/48549], Loss: 2.4378\n",
      "Epoch [1/1], Step [17620/48549], Loss: 2.4064\n",
      "Epoch [1/1], Step [17630/48549], Loss: 2.5105\n",
      "Epoch [1/1], Step [17640/48549], Loss: 2.4387\n",
      "Epoch [1/1], Step [17650/48549], Loss: 2.4671\n",
      "Epoch [1/1], Step [17660/48549], Loss: 2.4371\n",
      "Epoch [1/1], Step [17670/48549], Loss: 2.4713\n",
      "Epoch [1/1], Step [17680/48549], Loss: 2.4251\n",
      "Epoch [1/1], Step [17690/48549], Loss: 2.4776\n",
      "Epoch [1/1], Step [17700/48549], Loss: 2.4500\n",
      "Epoch [1/1], Step [17710/48549], Loss: 2.4237\n",
      "Epoch [1/1], Step [17720/48549], Loss: 2.4519\n",
      "Epoch [1/1], Step [17730/48549], Loss: 2.4876\n",
      "Epoch [1/1], Step [17740/48549], Loss: 2.4722\n",
      "Epoch [1/1], Step [17750/48549], Loss: 2.4343\n",
      "Epoch [1/1], Step [17760/48549], Loss: 2.5304\n",
      "Epoch [1/1], Step [17770/48549], Loss: 2.4579\n",
      "Epoch [1/1], Step [17780/48549], Loss: 2.4124\n",
      "Epoch [1/1], Step [17790/48549], Loss: 2.4699\n",
      "Epoch [1/1], Step [17800/48549], Loss: 2.3692\n",
      "Epoch [1/1], Step [17810/48549], Loss: 2.4942\n",
      "Epoch [1/1], Step [17820/48549], Loss: 2.4341\n",
      "Epoch [1/1], Step [17830/48549], Loss: 2.3993\n",
      "Epoch [1/1], Step [17840/48549], Loss: 2.4153\n",
      "Epoch [1/1], Step [17850/48549], Loss: 2.4731\n",
      "Epoch [1/1], Step [17860/48549], Loss: 2.5187\n",
      "Epoch [1/1], Step [17870/48549], Loss: 2.5034\n",
      "Epoch [1/1], Step [17880/48549], Loss: 2.4537\n",
      "Epoch [1/1], Step [17890/48549], Loss: 2.4818\n",
      "Epoch [1/1], Step [17900/48549], Loss: 2.4055\n",
      "Epoch [1/1], Step [17910/48549], Loss: 2.4971\n",
      "Epoch [1/1], Step [17920/48549], Loss: 2.4646\n",
      "Epoch [1/1], Step [17930/48549], Loss: 2.4988\n",
      "Epoch [1/1], Step [17940/48549], Loss: 2.3964\n",
      "Epoch [1/1], Step [17950/48549], Loss: 2.4263\n",
      "Epoch [1/1], Step [17960/48549], Loss: 2.3829\n",
      "Epoch [1/1], Step [17970/48549], Loss: 2.4042\n",
      "Epoch [1/1], Step [17980/48549], Loss: 2.4321\n",
      "Epoch [1/1], Step [17990/48549], Loss: 2.4532\n",
      "Epoch [1/1], Step [18000/48549], Loss: 2.4304\n",
      "Epoch [1/1], Step [18010/48549], Loss: 2.4234\n",
      "Epoch [1/1], Step [18020/48549], Loss: 2.4070\n",
      "Epoch [1/1], Step [18030/48549], Loss: 2.4645\n",
      "Epoch [1/1], Step [18040/48549], Loss: 2.4700\n",
      "Epoch [1/1], Step [18050/48549], Loss: 2.4504\n",
      "Epoch [1/1], Step [18060/48549], Loss: 2.4914\n",
      "Epoch [1/1], Step [18070/48549], Loss: 2.4286\n",
      "Epoch [1/1], Step [18080/48549], Loss: 2.3976\n",
      "Epoch [1/1], Step [18090/48549], Loss: 2.4641\n",
      "Epoch [1/1], Step [18100/48549], Loss: 2.4441\n",
      "Epoch [1/1], Step [18110/48549], Loss: 2.5075\n",
      "Epoch [1/1], Step [18120/48549], Loss: 2.4123\n",
      "Epoch [1/1], Step [18130/48549], Loss: 2.3990\n",
      "Epoch [1/1], Step [18140/48549], Loss: 2.4992\n",
      "Epoch [1/1], Step [18150/48549], Loss: 2.4665\n",
      "Epoch [1/1], Step [18160/48549], Loss: 2.4128\n",
      "Epoch [1/1], Step [18170/48549], Loss: 2.4563\n",
      "Epoch [1/1], Step [18180/48549], Loss: 2.4450\n",
      "Epoch [1/1], Step [18190/48549], Loss: 2.4296\n",
      "Epoch [1/1], Step [18200/48549], Loss: 2.4271\n",
      "Epoch [1/1], Step [18210/48549], Loss: 2.4935\n",
      "Epoch [1/1], Step [18220/48549], Loss: 2.4525\n",
      "Epoch [1/1], Step [18230/48549], Loss: 2.4513\n",
      "Epoch [1/1], Step [18240/48549], Loss: 2.4486\n",
      "Epoch [1/1], Step [18250/48549], Loss: 2.4136\n",
      "Epoch [1/1], Step [18260/48549], Loss: 2.5328\n",
      "Epoch [1/1], Step [18270/48549], Loss: 2.5273\n",
      "Epoch [1/1], Step [18280/48549], Loss: 2.4837\n",
      "Epoch [1/1], Step [18290/48549], Loss: 2.4438\n",
      "Epoch [1/1], Step [18300/48549], Loss: 2.4584\n",
      "Epoch [1/1], Step [18310/48549], Loss: 2.5063\n",
      "Epoch [1/1], Step [18320/48549], Loss: 2.3946\n",
      "Epoch [1/1], Step [18330/48549], Loss: 2.4221\n",
      "Epoch [1/1], Step [18340/48549], Loss: 2.3755\n",
      "Epoch [1/1], Step [18350/48549], Loss: 2.4687\n",
      "Epoch [1/1], Step [18360/48549], Loss: 2.4159\n",
      "Epoch [1/1], Step [18370/48549], Loss: 2.5158\n",
      "Epoch [1/1], Step [18380/48549], Loss: 2.4326\n",
      "Epoch [1/1], Step [18390/48549], Loss: 2.4480\n",
      "Epoch [1/1], Step [18400/48549], Loss: 2.4461\n",
      "Epoch [1/1], Step [18410/48549], Loss: 2.4421\n",
      "Epoch [1/1], Step [18420/48549], Loss: 2.4890\n",
      "Epoch [1/1], Step [18430/48549], Loss: 2.3886\n",
      "Epoch [1/1], Step [18440/48549], Loss: 2.4051\n",
      "Epoch [1/1], Step [18450/48549], Loss: 2.4682\n",
      "Epoch [1/1], Step [18460/48549], Loss: 2.4066\n",
      "Epoch [1/1], Step [18470/48549], Loss: 2.4321\n",
      "Epoch [1/1], Step [18480/48549], Loss: 2.4384\n",
      "Epoch [1/1], Step [18490/48549], Loss: 2.4462\n",
      "Epoch [1/1], Step [18500/48549], Loss: 2.4332\n",
      "Epoch [1/1], Step [18510/48549], Loss: 2.4895\n",
      "Epoch [1/1], Step [18520/48549], Loss: 2.4001\n",
      "Epoch [1/1], Step [18530/48549], Loss: 2.3714\n",
      "Epoch [1/1], Step [18540/48549], Loss: 2.4778\n",
      "Epoch [1/1], Step [18550/48549], Loss: 2.4340\n",
      "Epoch [1/1], Step [18560/48549], Loss: 2.4437\n",
      "Epoch [1/1], Step [18570/48549], Loss: 2.5002\n",
      "Epoch [1/1], Step [18580/48549], Loss: 2.3934\n",
      "Epoch [1/1], Step [18590/48549], Loss: 2.4164\n",
      "Epoch [1/1], Step [18600/48549], Loss: 2.4038\n",
      "Epoch [1/1], Step [18610/48549], Loss: 2.4357\n",
      "Epoch [1/1], Step [18620/48549], Loss: 2.4052\n",
      "Epoch [1/1], Step [18630/48549], Loss: 2.4336\n",
      "Epoch [1/1], Step [18640/48549], Loss: 2.4205\n",
      "Epoch [1/1], Step [18650/48549], Loss: 2.3962\n",
      "Epoch [1/1], Step [18660/48549], Loss: 2.4452\n",
      "Epoch [1/1], Step [18670/48549], Loss: 2.4477\n",
      "Epoch [1/1], Step [18680/48549], Loss: 2.4395\n",
      "Epoch [1/1], Step [18690/48549], Loss: 2.4981\n",
      "Epoch [1/1], Step [18700/48549], Loss: 2.4369\n",
      "Epoch [1/1], Step [18710/48549], Loss: 2.4220\n",
      "Epoch [1/1], Step [18720/48549], Loss: 2.4330\n",
      "Epoch [1/1], Step [18730/48549], Loss: 2.4195\n",
      "Epoch [1/1], Step [18740/48549], Loss: 2.4560\n",
      "Epoch [1/1], Step [18750/48549], Loss: 2.5258\n",
      "Epoch [1/1], Step [18760/48549], Loss: 2.3807\n",
      "Epoch [1/1], Step [18770/48549], Loss: 2.3877\n",
      "Epoch [1/1], Step [18780/48549], Loss: 2.4663\n",
      "Epoch [1/1], Step [18790/48549], Loss: 2.4147\n",
      "Epoch [1/1], Step [18800/48549], Loss: 2.4666\n",
      "Epoch [1/1], Step [18810/48549], Loss: 2.4216\n",
      "Epoch [1/1], Step [18820/48549], Loss: 2.4335\n",
      "Epoch [1/1], Step [18830/48549], Loss: 2.4321\n",
      "Epoch [1/1], Step [18840/48549], Loss: 2.4612\n",
      "Epoch [1/1], Step [18850/48549], Loss: 2.4551\n",
      "Epoch [1/1], Step [18860/48549], Loss: 2.4182\n",
      "Epoch [1/1], Step [18870/48549], Loss: 2.4344\n",
      "Epoch [1/1], Step [18880/48549], Loss: 2.4430\n",
      "Epoch [1/1], Step [18890/48549], Loss: 2.4392\n",
      "Epoch [1/1], Step [18900/48549], Loss: 2.4606\n",
      "Epoch [1/1], Step [18910/48549], Loss: 2.4162\n",
      "Epoch [1/1], Step [18920/48549], Loss: 2.4016\n",
      "Epoch [1/1], Step [18930/48549], Loss: 2.4044\n",
      "Epoch [1/1], Step [18940/48549], Loss: 2.5347\n",
      "Epoch [1/1], Step [18950/48549], Loss: 2.4835\n",
      "Epoch [1/1], Step [18960/48549], Loss: 2.4312\n",
      "Epoch [1/1], Step [18970/48549], Loss: 2.5006\n",
      "Epoch [1/1], Step [18980/48549], Loss: 2.4381\n",
      "Epoch [1/1], Step [18990/48549], Loss: 2.4460\n",
      "Epoch [1/1], Step [19000/48549], Loss: 2.4444\n",
      "Epoch [1/1], Step [19010/48549], Loss: 2.4416\n",
      "Epoch [1/1], Step [19020/48549], Loss: 2.4285\n",
      "Epoch [1/1], Step [19030/48549], Loss: 2.4084\n",
      "Epoch [1/1], Step [19040/48549], Loss: 2.4216\n",
      "Epoch [1/1], Step [19050/48549], Loss: 2.4321\n",
      "Epoch [1/1], Step [19060/48549], Loss: 2.4203\n",
      "Epoch [1/1], Step [19070/48549], Loss: 2.4420\n",
      "Epoch [1/1], Step [19080/48549], Loss: 2.5058\n",
      "Epoch [1/1], Step [19090/48549], Loss: 2.4493\n",
      "Epoch [1/1], Step [19100/48549], Loss: 2.5284\n",
      "Epoch [1/1], Step [19110/48549], Loss: 2.4412\n",
      "Epoch [1/1], Step [19120/48549], Loss: 2.4425\n",
      "Epoch [1/1], Step [19130/48549], Loss: 2.4621\n",
      "Epoch [1/1], Step [19140/48549], Loss: 2.3889\n",
      "Epoch [1/1], Step [19150/48549], Loss: 2.4464\n",
      "Epoch [1/1], Step [19160/48549], Loss: 2.4666\n",
      "Epoch [1/1], Step [19170/48549], Loss: 2.3893\n",
      "Epoch [1/1], Step [19180/48549], Loss: 2.4709\n",
      "Epoch [1/1], Step [19190/48549], Loss: 2.4616\n",
      "Epoch [1/1], Step [19200/48549], Loss: 2.4578\n",
      "Epoch [1/1], Step [19210/48549], Loss: 2.4402\n",
      "Epoch [1/1], Step [19220/48549], Loss: 2.4728\n",
      "Epoch [1/1], Step [19230/48549], Loss: 2.4187\n",
      "Epoch [1/1], Step [19240/48549], Loss: 2.4008\n",
      "Epoch [1/1], Step [19250/48549], Loss: 2.4303\n",
      "Epoch [1/1], Step [19260/48549], Loss: 2.4108\n",
      "Epoch [1/1], Step [19270/48549], Loss: 2.4409\n",
      "Epoch [1/1], Step [19280/48549], Loss: 2.4669\n",
      "Epoch [1/1], Step [19290/48549], Loss: 2.5073\n",
      "Epoch [1/1], Step [19300/48549], Loss: 2.4711\n",
      "Epoch [1/1], Step [19310/48549], Loss: 2.4975\n",
      "Epoch [1/1], Step [19320/48549], Loss: 2.4591\n",
      "Epoch [1/1], Step [19330/48549], Loss: 2.3903\n",
      "Epoch [1/1], Step [19340/48549], Loss: 2.4754\n",
      "Epoch [1/1], Step [19350/48549], Loss: 2.4691\n",
      "Epoch [1/1], Step [19360/48549], Loss: 2.4156\n",
      "Epoch [1/1], Step [19370/48549], Loss: 2.4253\n",
      "Epoch [1/1], Step [19380/48549], Loss: 2.4812\n",
      "Epoch [1/1], Step [19390/48549], Loss: 2.4541\n",
      "Epoch [1/1], Step [19400/48549], Loss: 2.3724\n",
      "Epoch [1/1], Step [19410/48549], Loss: 2.4318\n",
      "Epoch [1/1], Step [19420/48549], Loss: 2.4350\n",
      "Epoch [1/1], Step [19430/48549], Loss: 2.4882\n",
      "Epoch [1/1], Step [19440/48549], Loss: 2.4208\n",
      "Epoch [1/1], Step [19450/48549], Loss: 2.5015\n",
      "Epoch [1/1], Step [19460/48549], Loss: 2.4939\n",
      "Epoch [1/1], Step [19470/48549], Loss: 2.4647\n",
      "Epoch [1/1], Step [19480/48549], Loss: 2.4215\n",
      "Epoch [1/1], Step [19490/48549], Loss: 2.4510\n",
      "Epoch [1/1], Step [19500/48549], Loss: 2.5035\n",
      "Epoch [1/1], Step [19510/48549], Loss: 2.4464\n",
      "Epoch [1/1], Step [19520/48549], Loss: 2.4228\n",
      "Epoch [1/1], Step [19530/48549], Loss: 2.4791\n",
      "Epoch [1/1], Step [19540/48549], Loss: 2.4295\n",
      "Epoch [1/1], Step [19550/48549], Loss: 2.4554\n",
      "Epoch [1/1], Step [19560/48549], Loss: 2.4523\n",
      "Epoch [1/1], Step [19570/48549], Loss: 2.4581\n",
      "Epoch [1/1], Step [19580/48549], Loss: 2.3870\n",
      "Epoch [1/1], Step [19590/48549], Loss: 2.4242\n",
      "Epoch [1/1], Step [19600/48549], Loss: 2.4357\n",
      "Epoch [1/1], Step [19610/48549], Loss: 2.4084\n",
      "Epoch [1/1], Step [19620/48549], Loss: 2.4596\n",
      "Epoch [1/1], Step [19630/48549], Loss: 2.4204\n",
      "Epoch [1/1], Step [19640/48549], Loss: 2.5020\n",
      "Epoch [1/1], Step [19650/48549], Loss: 2.4201\n",
      "Epoch [1/1], Step [19660/48549], Loss: 2.4249\n",
      "Epoch [1/1], Step [19670/48549], Loss: 2.4595\n",
      "Epoch [1/1], Step [19680/48549], Loss: 2.4428\n",
      "Epoch [1/1], Step [19690/48549], Loss: 2.4611\n",
      "Epoch [1/1], Step [19700/48549], Loss: 2.4401\n",
      "Epoch [1/1], Step [19710/48549], Loss: 2.4539\n",
      "Epoch [1/1], Step [19720/48549], Loss: 2.4350\n",
      "Epoch [1/1], Step [19730/48549], Loss: 2.3272\n",
      "Epoch [1/1], Step [19740/48549], Loss: 2.4211\n",
      "Epoch [1/1], Step [19750/48549], Loss: 2.4480\n",
      "Epoch [1/1], Step [19760/48549], Loss: 2.4237\n",
      "Epoch [1/1], Step [19770/48549], Loss: 2.4052\n",
      "Epoch [1/1], Step [19780/48549], Loss: 2.4619\n",
      "Epoch [1/1], Step [19790/48549], Loss: 2.4341\n",
      "Epoch [1/1], Step [19800/48549], Loss: 2.4292\n",
      "Epoch [1/1], Step [19810/48549], Loss: 2.4936\n",
      "Epoch [1/1], Step [19820/48549], Loss: 2.4718\n",
      "Epoch [1/1], Step [19830/48549], Loss: 2.4371\n",
      "Epoch [1/1], Step [19840/48549], Loss: 2.4434\n",
      "Epoch [1/1], Step [19850/48549], Loss: 2.4411\n",
      "Epoch [1/1], Step [19860/48549], Loss: 2.4472\n",
      "Epoch [1/1], Step [19870/48549], Loss: 2.4547\n",
      "Epoch [1/1], Step [19880/48549], Loss: 2.4757\n",
      "Epoch [1/1], Step [19890/48549], Loss: 2.4322\n",
      "Epoch [1/1], Step [19900/48549], Loss: 2.4643\n",
      "Epoch [1/1], Step [19910/48549], Loss: 2.4347\n",
      "Epoch [1/1], Step [19920/48549], Loss: 2.4871\n",
      "Epoch [1/1], Step [19930/48549], Loss: 2.4936\n",
      "Epoch [1/1], Step [19940/48549], Loss: 2.4562\n",
      "Epoch [1/1], Step [19950/48549], Loss: 2.4511\n",
      "Epoch [1/1], Step [19960/48549], Loss: 2.4405\n",
      "Epoch [1/1], Step [19970/48549], Loss: 2.3776\n",
      "Epoch [1/1], Step [19980/48549], Loss: 2.4354\n",
      "Epoch [1/1], Step [19990/48549], Loss: 2.4047\n",
      "Epoch [1/1], Step [20000/48549], Loss: 2.4863\n",
      "Epoch [1/1], Step [20010/48549], Loss: 2.4003\n",
      "Epoch [1/1], Step [20020/48549], Loss: 2.4225\n",
      "Epoch [1/1], Step [20030/48549], Loss: 2.4532\n",
      "Epoch [1/1], Step [20040/48549], Loss: 2.4410\n",
      "Epoch [1/1], Step [20050/48549], Loss: 2.4334\n",
      "Epoch [1/1], Step [20060/48549], Loss: 2.4556\n",
      "Epoch [1/1], Step [20070/48549], Loss: 2.5165\n",
      "Epoch [1/1], Step [20080/48549], Loss: 2.4113\n",
      "Epoch [1/1], Step [20090/48549], Loss: 2.4089\n",
      "Epoch [1/1], Step [20100/48549], Loss: 2.5020\n",
      "Epoch [1/1], Step [20110/48549], Loss: 2.5285\n",
      "Epoch [1/1], Step [20120/48549], Loss: 2.4768\n",
      "Epoch [1/1], Step [20130/48549], Loss: 2.4142\n",
      "Epoch [1/1], Step [20140/48549], Loss: 2.4469\n",
      "Epoch [1/1], Step [20150/48549], Loss: 2.4737\n",
      "Epoch [1/1], Step [20160/48549], Loss: 2.5296\n",
      "Epoch [1/1], Step [20170/48549], Loss: 2.4876\n",
      "Epoch [1/1], Step [20180/48549], Loss: 2.4216\n",
      "Epoch [1/1], Step [20190/48549], Loss: 2.4471\n",
      "Epoch [1/1], Step [20200/48549], Loss: 2.4141\n",
      "Epoch [1/1], Step [20210/48549], Loss: 2.4540\n",
      "Epoch [1/1], Step [20220/48549], Loss: 2.5736\n",
      "Epoch [1/1], Step [20230/48549], Loss: 2.4248\n",
      "Epoch [1/1], Step [20240/48549], Loss: 2.4908\n",
      "Epoch [1/1], Step [20250/48549], Loss: 2.4698\n",
      "Epoch [1/1], Step [20260/48549], Loss: 2.3699\n",
      "Epoch [1/1], Step [20270/48549], Loss: 2.4740\n",
      "Epoch [1/1], Step [20280/48549], Loss: 2.4738\n",
      "Epoch [1/1], Step [20290/48549], Loss: 2.4526\n",
      "Epoch [1/1], Step [20300/48549], Loss: 2.5204\n",
      "Epoch [1/1], Step [20310/48549], Loss: 2.4597\n",
      "Epoch [1/1], Step [20320/48549], Loss: 2.4708\n",
      "Epoch [1/1], Step [20330/48549], Loss: 2.4921\n",
      "Epoch [1/1], Step [20340/48549], Loss: 2.4348\n",
      "Epoch [1/1], Step [20350/48549], Loss: 2.4829\n",
      "Epoch [1/1], Step [20360/48549], Loss: 2.4682\n",
      "Epoch [1/1], Step [20370/48549], Loss: 2.3953\n",
      "Epoch [1/1], Step [20380/48549], Loss: 2.4324\n",
      "Epoch [1/1], Step [20390/48549], Loss: 2.4144\n",
      "Epoch [1/1], Step [20400/48549], Loss: 2.4810\n",
      "Epoch [1/1], Step [20410/48549], Loss: 2.4619\n",
      "Epoch [1/1], Step [20420/48549], Loss: 2.4102\n",
      "Epoch [1/1], Step [20430/48549], Loss: 2.4955\n",
      "Epoch [1/1], Step [20440/48549], Loss: 2.4624\n",
      "Epoch [1/1], Step [20450/48549], Loss: 2.4692\n",
      "Epoch [1/1], Step [20460/48549], Loss: 2.3978\n",
      "Epoch [1/1], Step [20470/48549], Loss: 2.4813\n",
      "Epoch [1/1], Step [20480/48549], Loss: 2.4342\n",
      "Epoch [1/1], Step [20490/48549], Loss: 2.5000\n",
      "Epoch [1/1], Step [20500/48549], Loss: 2.4894\n",
      "Epoch [1/1], Step [20510/48549], Loss: 2.4544\n",
      "Epoch [1/1], Step [20520/48549], Loss: 2.4731\n",
      "Epoch [1/1], Step [20530/48549], Loss: 2.4246\n",
      "Epoch [1/1], Step [20540/48549], Loss: 2.4965\n",
      "Epoch [1/1], Step [20550/48549], Loss: 2.4029\n",
      "Epoch [1/1], Step [20560/48549], Loss: 2.4853\n",
      "Epoch [1/1], Step [20570/48549], Loss: 2.4166\n",
      "Epoch [1/1], Step [20580/48549], Loss: 2.4346\n",
      "Epoch [1/1], Step [20590/48549], Loss: 2.3910\n",
      "Epoch [1/1], Step [20600/48549], Loss: 2.5160\n",
      "Epoch [1/1], Step [20610/48549], Loss: 2.4354\n",
      "Epoch [1/1], Step [20620/48549], Loss: 2.4448\n",
      "Epoch [1/1], Step [20630/48549], Loss: 2.4898\n",
      "Epoch [1/1], Step [20640/48549], Loss: 2.4356\n",
      "Epoch [1/1], Step [20650/48549], Loss: 2.4471\n",
      "Epoch [1/1], Step [20660/48549], Loss: 2.4175\n",
      "Epoch [1/1], Step [20670/48549], Loss: 2.5019\n",
      "Epoch [1/1], Step [20680/48549], Loss: 2.4459\n",
      "Epoch [1/1], Step [20690/48549], Loss: 2.4262\n",
      "Epoch [1/1], Step [20700/48549], Loss: 2.4037\n",
      "Epoch [1/1], Step [20710/48549], Loss: 2.4091\n",
      "Epoch [1/1], Step [20720/48549], Loss: 2.4696\n",
      "Epoch [1/1], Step [20730/48549], Loss: 2.4245\n",
      "Epoch [1/1], Step [20740/48549], Loss: 2.4120\n",
      "Epoch [1/1], Step [20750/48549], Loss: 2.4133\n",
      "Epoch [1/1], Step [20760/48549], Loss: 2.4225\n",
      "Epoch [1/1], Step [20770/48549], Loss: 2.4650\n",
      "Epoch [1/1], Step [20780/48549], Loss: 2.4494\n",
      "Epoch [1/1], Step [20790/48549], Loss: 2.4602\n",
      "Epoch [1/1], Step [20800/48549], Loss: 2.4437\n",
      "Epoch [1/1], Step [20810/48549], Loss: 2.4869\n",
      "Epoch [1/1], Step [20820/48549], Loss: 2.4318\n",
      "Epoch [1/1], Step [20830/48549], Loss: 2.4061\n",
      "Epoch [1/1], Step [20840/48549], Loss: 2.4666\n",
      "Epoch [1/1], Step [20850/48549], Loss: 2.4340\n",
      "Epoch [1/1], Step [20860/48549], Loss: 2.4664\n",
      "Epoch [1/1], Step [20870/48549], Loss: 2.4226\n",
      "Epoch [1/1], Step [20880/48549], Loss: 2.4471\n",
      "Epoch [1/1], Step [20890/48549], Loss: 2.4742\n",
      "Epoch [1/1], Step [20900/48549], Loss: 2.4762\n",
      "Epoch [1/1], Step [20910/48549], Loss: 2.4169\n",
      "Epoch [1/1], Step [20920/48549], Loss: 2.4194\n",
      "Epoch [1/1], Step [20930/48549], Loss: 2.4359\n",
      "Epoch [1/1], Step [20940/48549], Loss: 2.4850\n",
      "Epoch [1/1], Step [20950/48549], Loss: 2.4330\n",
      "Epoch [1/1], Step [20960/48549], Loss: 2.4912\n",
      "Epoch [1/1], Step [20970/48549], Loss: 2.4670\n",
      "Epoch [1/1], Step [20980/48549], Loss: 2.4535\n",
      "Epoch [1/1], Step [20990/48549], Loss: 2.4414\n",
      "Epoch [1/1], Step [21000/48549], Loss: 2.4656\n",
      "Epoch [1/1], Step [21010/48549], Loss: 2.4407\n",
      "Epoch [1/1], Step [21020/48549], Loss: 2.4503\n",
      "Epoch [1/1], Step [21030/48549], Loss: 2.3986\n",
      "Epoch [1/1], Step [21040/48549], Loss: 2.4745\n",
      "Epoch [1/1], Step [21050/48549], Loss: 2.4444\n",
      "Epoch [1/1], Step [21060/48549], Loss: 2.4347\n",
      "Epoch [1/1], Step [21070/48549], Loss: 2.4270\n",
      "Epoch [1/1], Step [21080/48549], Loss: 2.4299\n",
      "Epoch [1/1], Step [21090/48549], Loss: 2.4072\n",
      "Epoch [1/1], Step [21100/48549], Loss: 2.5124\n",
      "Epoch [1/1], Step [21110/48549], Loss: 2.4362\n",
      "Epoch [1/1], Step [21120/48549], Loss: 2.4523\n",
      "Epoch [1/1], Step [21130/48549], Loss: 2.4425\n",
      "Epoch [1/1], Step [21140/48549], Loss: 2.5186\n",
      "Epoch [1/1], Step [21150/48549], Loss: 2.4171\n",
      "Epoch [1/1], Step [21160/48549], Loss: 2.5065\n",
      "Epoch [1/1], Step [21170/48549], Loss: 2.5292\n",
      "Epoch [1/1], Step [21180/48549], Loss: 2.4646\n",
      "Epoch [1/1], Step [21190/48549], Loss: 2.4919\n",
      "Epoch [1/1], Step [21200/48549], Loss: 2.4576\n",
      "Epoch [1/1], Step [21210/48549], Loss: 2.4706\n",
      "Epoch [1/1], Step [21220/48549], Loss: 2.4474\n",
      "Epoch [1/1], Step [21230/48549], Loss: 2.4262\n",
      "Epoch [1/1], Step [21240/48549], Loss: 2.4298\n",
      "Epoch [1/1], Step [21250/48549], Loss: 2.4846\n",
      "Epoch [1/1], Step [21260/48549], Loss: 2.4353\n",
      "Epoch [1/1], Step [21270/48549], Loss: 2.4677\n",
      "Epoch [1/1], Step [21280/48549], Loss: 2.4116\n",
      "Epoch [1/1], Step [21290/48549], Loss: 2.4594\n",
      "Epoch [1/1], Step [21300/48549], Loss: 2.4675\n",
      "Epoch [1/1], Step [21310/48549], Loss: 2.4289\n",
      "Epoch [1/1], Step [21320/48549], Loss: 2.4249\n",
      "Epoch [1/1], Step [21330/48549], Loss: 2.5505\n",
      "Epoch [1/1], Step [21340/48549], Loss: 2.4645\n",
      "Epoch [1/1], Step [21350/48549], Loss: 2.4411\n",
      "Epoch [1/1], Step [21360/48549], Loss: 2.4597\n",
      "Epoch [1/1], Step [21370/48549], Loss: 2.4418\n",
      "Epoch [1/1], Step [21380/48549], Loss: 2.4310\n",
      "Epoch [1/1], Step [21390/48549], Loss: 2.4408\n",
      "Epoch [1/1], Step [21400/48549], Loss: 2.4956\n",
      "Epoch [1/1], Step [21410/48549], Loss: 2.4329\n",
      "Epoch [1/1], Step [21420/48549], Loss: 2.4537\n",
      "Epoch [1/1], Step [21430/48549], Loss: 2.4427\n",
      "Epoch [1/1], Step [21440/48549], Loss: 2.4336\n",
      "Epoch [1/1], Step [21450/48549], Loss: 2.4746\n",
      "Epoch [1/1], Step [21460/48549], Loss: 2.5894\n",
      "Epoch [1/1], Step [21470/48549], Loss: 2.4751\n",
      "Epoch [1/1], Step [21480/48549], Loss: 2.4243\n",
      "Epoch [1/1], Step [21490/48549], Loss: 2.4350\n",
      "Epoch [1/1], Step [21500/48549], Loss: 2.4100\n",
      "Epoch [1/1], Step [21510/48549], Loss: 2.4574\n",
      "Epoch [1/1], Step [21520/48549], Loss: 2.4570\n",
      "Epoch [1/1], Step [21530/48549], Loss: 2.3885\n",
      "Epoch [1/1], Step [21540/48549], Loss: 2.4877\n",
      "Epoch [1/1], Step [21550/48549], Loss: 2.4448\n",
      "Epoch [1/1], Step [21560/48549], Loss: 2.5099\n",
      "Epoch [1/1], Step [21570/48549], Loss: 2.4313\n",
      "Epoch [1/1], Step [21580/48549], Loss: 2.4633\n",
      "Epoch [1/1], Step [21590/48549], Loss: 2.4507\n",
      "Epoch [1/1], Step [21600/48549], Loss: 2.4504\n",
      "Epoch [1/1], Step [21610/48549], Loss: 2.3938\n",
      "Epoch [1/1], Step [21620/48549], Loss: 2.4765\n",
      "Epoch [1/1], Step [21630/48549], Loss: 2.4312\n",
      "Epoch [1/1], Step [21640/48549], Loss: 2.4041\n",
      "Epoch [1/1], Step [21650/48549], Loss: 2.4861\n",
      "Epoch [1/1], Step [21660/48549], Loss: 2.4595\n",
      "Epoch [1/1], Step [21670/48549], Loss: 2.5526\n",
      "Epoch [1/1], Step [21680/48549], Loss: 2.4185\n",
      "Epoch [1/1], Step [21690/48549], Loss: 2.3980\n",
      "Epoch [1/1], Step [21700/48549], Loss: 2.4395\n",
      "Epoch [1/1], Step [21710/48549], Loss: 2.4344\n",
      "Epoch [1/1], Step [21720/48549], Loss: 2.4505\n",
      "Epoch [1/1], Step [21730/48549], Loss: 2.4134\n",
      "Epoch [1/1], Step [21740/48549], Loss: 2.4740\n",
      "Epoch [1/1], Step [21750/48549], Loss: 2.4688\n",
      "Epoch [1/1], Step [21760/48549], Loss: 2.4273\n",
      "Epoch [1/1], Step [21770/48549], Loss: 2.4662\n",
      "Epoch [1/1], Step [21780/48549], Loss: 2.4534\n",
      "Epoch [1/1], Step [21790/48549], Loss: 2.4631\n",
      "Epoch [1/1], Step [21800/48549], Loss: 2.4547\n",
      "Epoch [1/1], Step [21810/48549], Loss: 2.5259\n",
      "Epoch [1/1], Step [21820/48549], Loss: 2.4178\n",
      "Epoch [1/1], Step [21830/48549], Loss: 2.4714\n",
      "Epoch [1/1], Step [21840/48549], Loss: 2.4216\n",
      "Epoch [1/1], Step [21850/48549], Loss: 2.4378\n",
      "Epoch [1/1], Step [21860/48549], Loss: 2.4001\n",
      "Epoch [1/1], Step [21870/48549], Loss: 2.4845\n",
      "Epoch [1/1], Step [21880/48549], Loss: 2.4987\n",
      "Epoch [1/1], Step [21890/48549], Loss: 2.4637\n",
      "Epoch [1/1], Step [21900/48549], Loss: 2.4476\n",
      "Epoch [1/1], Step [21910/48549], Loss: 2.4594\n",
      "Epoch [1/1], Step [21920/48549], Loss: 2.4071\n",
      "Epoch [1/1], Step [21930/48549], Loss: 2.4574\n",
      "Epoch [1/1], Step [21940/48549], Loss: 2.4397\n",
      "Epoch [1/1], Step [21950/48549], Loss: 2.4157\n",
      "Epoch [1/1], Step [21960/48549], Loss: 2.4398\n",
      "Epoch [1/1], Step [21970/48549], Loss: 2.4154\n",
      "Epoch [1/1], Step [21980/48549], Loss: 2.5049\n",
      "Epoch [1/1], Step [21990/48549], Loss: 2.4271\n",
      "Epoch [1/1], Step [22000/48549], Loss: 2.4175\n",
      "Epoch [1/1], Step [22010/48549], Loss: 2.4467\n",
      "Epoch [1/1], Step [22020/48549], Loss: 2.4242\n",
      "Epoch [1/1], Step [22030/48549], Loss: 2.4285\n",
      "Epoch [1/1], Step [22040/48549], Loss: 2.4781\n",
      "Epoch [1/1], Step [22050/48549], Loss: 2.4222\n",
      "Epoch [1/1], Step [22060/48549], Loss: 2.4503\n",
      "Epoch [1/1], Step [22070/48549], Loss: 2.5402\n",
      "Epoch [1/1], Step [22080/48549], Loss: 2.4335\n",
      "Epoch [1/1], Step [22090/48549], Loss: 2.3828\n",
      "Epoch [1/1], Step [22100/48549], Loss: 2.4136\n",
      "Epoch [1/1], Step [22110/48549], Loss: 2.5308\n",
      "Epoch [1/1], Step [22120/48549], Loss: 2.4602\n",
      "Epoch [1/1], Step [22130/48549], Loss: 2.5008\n",
      "Epoch [1/1], Step [22140/48549], Loss: 2.5789\n",
      "Epoch [1/1], Step [22150/48549], Loss: 2.4672\n",
      "Epoch [1/1], Step [22160/48549], Loss: 2.4475\n",
      "Epoch [1/1], Step [22170/48549], Loss: 2.3945\n",
      "Epoch [1/1], Step [22180/48549], Loss: 2.4471\n",
      "Epoch [1/1], Step [22190/48549], Loss: 2.4083\n",
      "Epoch [1/1], Step [22200/48549], Loss: 2.4370\n",
      "Epoch [1/1], Step [22210/48549], Loss: 2.4212\n",
      "Epoch [1/1], Step [22220/48549], Loss: 2.4647\n",
      "Epoch [1/1], Step [22230/48549], Loss: 2.4984\n",
      "Epoch [1/1], Step [22240/48549], Loss: 2.4528\n",
      "Epoch [1/1], Step [22250/48549], Loss: 2.4556\n",
      "Epoch [1/1], Step [22260/48549], Loss: 2.4916\n",
      "Epoch [1/1], Step [22270/48549], Loss: 2.5300\n",
      "Epoch [1/1], Step [22280/48549], Loss: 2.4451\n",
      "Epoch [1/1], Step [22290/48549], Loss: 2.5814\n",
      "Epoch [1/1], Step [22300/48549], Loss: 2.3940\n",
      "Epoch [1/1], Step [22310/48549], Loss: 2.4578\n",
      "Epoch [1/1], Step [22320/48549], Loss: 2.4052\n",
      "Epoch [1/1], Step [22330/48549], Loss: 2.4813\n",
      "Epoch [1/1], Step [22340/48549], Loss: 2.4213\n",
      "Epoch [1/1], Step [22350/48549], Loss: 2.4158\n",
      "Epoch [1/1], Step [22360/48549], Loss: 2.4538\n",
      "Epoch [1/1], Step [22370/48549], Loss: 2.4325\n",
      "Epoch [1/1], Step [22380/48549], Loss: 2.4321\n",
      "Epoch [1/1], Step [22390/48549], Loss: 2.4226\n",
      "Epoch [1/1], Step [22400/48549], Loss: 2.4465\n",
      "Epoch [1/1], Step [22410/48549], Loss: 2.4175\n",
      "Epoch [1/1], Step [22420/48549], Loss: 2.4768\n",
      "Epoch [1/1], Step [22430/48549], Loss: 2.4402\n",
      "Epoch [1/1], Step [22440/48549], Loss: 2.4186\n",
      "Epoch [1/1], Step [22450/48549], Loss: 2.3903\n",
      "Epoch [1/1], Step [22460/48549], Loss: 2.4456\n",
      "Epoch [1/1], Step [22470/48549], Loss: 2.4413\n",
      "Epoch [1/1], Step [22480/48549], Loss: 2.4149\n",
      "Epoch [1/1], Step [22490/48549], Loss: 2.4512\n",
      "Epoch [1/1], Step [22500/48549], Loss: 2.4372\n",
      "Epoch [1/1], Step [22510/48549], Loss: 2.4604\n",
      "Epoch [1/1], Step [22520/48549], Loss: 2.4490\n",
      "Epoch [1/1], Step [22530/48549], Loss: 2.4523\n",
      "Epoch [1/1], Step [22540/48549], Loss: 2.4459\n",
      "Epoch [1/1], Step [22550/48549], Loss: 2.4764\n",
      "Epoch [1/1], Step [22560/48549], Loss: 2.4270\n",
      "Epoch [1/1], Step [22570/48549], Loss: 2.4648\n",
      "Epoch [1/1], Step [22580/48549], Loss: 2.4157\n",
      "Epoch [1/1], Step [22590/48549], Loss: 2.4374\n",
      "Epoch [1/1], Step [22600/48549], Loss: 2.4182\n",
      "Epoch [1/1], Step [22610/48549], Loss: 2.4861\n",
      "Epoch [1/1], Step [22620/48549], Loss: 2.4449\n",
      "Epoch [1/1], Step [22630/48549], Loss: 2.4051\n",
      "Epoch [1/1], Step [22640/48549], Loss: 2.4192\n",
      "Epoch [1/1], Step [22650/48549], Loss: 2.4358\n",
      "Epoch [1/1], Step [22660/48549], Loss: 2.4455\n",
      "Epoch [1/1], Step [22670/48549], Loss: 2.4210\n",
      "Epoch [1/1], Step [22680/48549], Loss: 2.4339\n",
      "Epoch [1/1], Step [22690/48549], Loss: 2.4224\n",
      "Epoch [1/1], Step [22700/48549], Loss: 2.4753\n",
      "Epoch [1/1], Step [22710/48549], Loss: 2.4577\n",
      "Epoch [1/1], Step [22720/48549], Loss: 2.4348\n",
      "Epoch [1/1], Step [22730/48549], Loss: 2.4522\n",
      "Epoch [1/1], Step [22740/48549], Loss: 2.4243\n",
      "Epoch [1/1], Step [22750/48549], Loss: 2.4669\n",
      "Epoch [1/1], Step [22760/48549], Loss: 2.4000\n",
      "Epoch [1/1], Step [22770/48549], Loss: 2.4072\n",
      "Epoch [1/1], Step [22780/48549], Loss: 2.4380\n",
      "Epoch [1/1], Step [22790/48549], Loss: 2.4810\n",
      "Epoch [1/1], Step [22800/48549], Loss: 2.4181\n",
      "Epoch [1/1], Step [22810/48549], Loss: 2.4051\n",
      "Epoch [1/1], Step [22820/48549], Loss: 2.4540\n",
      "Epoch [1/1], Step [22830/48549], Loss: 2.6000\n",
      "Epoch [1/1], Step [22840/48549], Loss: 2.4050\n",
      "Epoch [1/1], Step [22850/48549], Loss: 2.3976\n",
      "Epoch [1/1], Step [22860/48549], Loss: 2.4451\n",
      "Epoch [1/1], Step [22870/48549], Loss: 2.4521\n",
      "Epoch [1/1], Step [22880/48549], Loss: 2.4106\n",
      "Epoch [1/1], Step [22890/48549], Loss: 2.4482\n",
      "Epoch [1/1], Step [22900/48549], Loss: 2.4547\n",
      "Epoch [1/1], Step [22910/48549], Loss: 2.4484\n",
      "Epoch [1/1], Step [22920/48549], Loss: 2.4909\n",
      "Epoch [1/1], Step [22930/48549], Loss: 2.4181\n",
      "Epoch [1/1], Step [22940/48549], Loss: 2.4371\n",
      "Epoch [1/1], Step [22950/48549], Loss: 2.4875\n",
      "Epoch [1/1], Step [22960/48549], Loss: 2.4437\n",
      "Epoch [1/1], Step [22970/48549], Loss: 2.4154\n",
      "Epoch [1/1], Step [22980/48549], Loss: 2.4153\n",
      "Epoch [1/1], Step [22990/48549], Loss: 2.4302\n",
      "Epoch [1/1], Step [23000/48549], Loss: 2.4360\n",
      "Epoch [1/1], Step [23010/48549], Loss: 2.4260\n",
      "Epoch [1/1], Step [23020/48549], Loss: 2.4045\n",
      "Epoch [1/1], Step [23030/48549], Loss: 2.4117\n",
      "Epoch [1/1], Step [23040/48549], Loss: 2.4030\n",
      "Epoch [1/1], Step [23050/48549], Loss: 2.4775\n",
      "Epoch [1/1], Step [23060/48549], Loss: 2.4941\n",
      "Epoch [1/1], Step [23070/48549], Loss: 2.4641\n",
      "Epoch [1/1], Step [23080/48549], Loss: 2.3815\n",
      "Epoch [1/1], Step [23090/48549], Loss: 2.4751\n",
      "Epoch [1/1], Step [23100/48549], Loss: 2.4322\n",
      "Epoch [1/1], Step [23110/48549], Loss: 2.4528\n",
      "Epoch [1/1], Step [23120/48549], Loss: 2.4855\n",
      "Epoch [1/1], Step [23130/48549], Loss: 2.4384\n",
      "Epoch [1/1], Step [23140/48549], Loss: 2.4743\n",
      "Epoch [1/1], Step [23150/48549], Loss: 2.4317\n",
      "Epoch [1/1], Step [23160/48549], Loss: 2.4350\n",
      "Epoch [1/1], Step [23170/48549], Loss: 2.4172\n",
      "Epoch [1/1], Step [23180/48549], Loss: 2.4091\n",
      "Epoch [1/1], Step [23190/48549], Loss: 2.4497\n",
      "Epoch [1/1], Step [23200/48549], Loss: 2.4232\n",
      "Epoch [1/1], Step [23210/48549], Loss: 2.4218\n",
      "Epoch [1/1], Step [23220/48549], Loss: 2.4379\n",
      "Epoch [1/1], Step [23230/48549], Loss: 2.4427\n",
      "Epoch [1/1], Step [23240/48549], Loss: 2.4585\n",
      "Epoch [1/1], Step [23250/48549], Loss: 2.4867\n",
      "Epoch [1/1], Step [23260/48549], Loss: 2.4301\n",
      "Epoch [1/1], Step [23270/48549], Loss: 2.4909\n",
      "Epoch [1/1], Step [23280/48549], Loss: 2.4009\n",
      "Epoch [1/1], Step [23290/48549], Loss: 2.5140\n",
      "Epoch [1/1], Step [23300/48549], Loss: 2.4193\n",
      "Epoch [1/1], Step [23310/48549], Loss: 2.4178\n",
      "Epoch [1/1], Step [23320/48549], Loss: 2.4686\n",
      "Epoch [1/1], Step [23330/48549], Loss: 2.4638\n",
      "Epoch [1/1], Step [23340/48549], Loss: 2.4265\n",
      "Epoch [1/1], Step [23350/48549], Loss: 2.4185\n",
      "Epoch [1/1], Step [23360/48549], Loss: 2.4342\n",
      "Epoch [1/1], Step [23370/48549], Loss: 2.4669\n",
      "Epoch [1/1], Step [23380/48549], Loss: 2.4006\n",
      "Epoch [1/1], Step [23390/48549], Loss: 2.4791\n",
      "Epoch [1/1], Step [23400/48549], Loss: 2.4781\n",
      "Epoch [1/1], Step [23410/48549], Loss: 2.4303\n",
      "Epoch [1/1], Step [23420/48549], Loss: 2.6321\n",
      "Epoch [1/1], Step [23430/48549], Loss: 2.4321\n",
      "Epoch [1/1], Step [23440/48549], Loss: 2.4890\n",
      "Epoch [1/1], Step [23450/48549], Loss: 2.4308\n",
      "Epoch [1/1], Step [23460/48549], Loss: 2.5155\n",
      "Epoch [1/1], Step [23470/48549], Loss: 2.4730\n",
      "Epoch [1/1], Step [23480/48549], Loss: 2.4577\n",
      "Epoch [1/1], Step [23490/48549], Loss: 2.4344\n",
      "Epoch [1/1], Step [23500/48549], Loss: 2.4165\n",
      "Epoch [1/1], Step [23510/48549], Loss: 2.4140\n",
      "Epoch [1/1], Step [23520/48549], Loss: 2.4071\n",
      "Epoch [1/1], Step [23530/48549], Loss: 2.4352\n",
      "Epoch [1/1], Step [23540/48549], Loss: 2.4060\n",
      "Epoch [1/1], Step [23550/48549], Loss: 2.3803\n",
      "Epoch [1/1], Step [23560/48549], Loss: 2.4803\n",
      "Epoch [1/1], Step [23570/48549], Loss: 2.4479\n",
      "Epoch [1/1], Step [23580/48549], Loss: 2.4523\n",
      "Epoch [1/1], Step [23590/48549], Loss: 2.3963\n",
      "Epoch [1/1], Step [23600/48549], Loss: 2.4669\n",
      "Epoch [1/1], Step [23610/48549], Loss: 2.4742\n",
      "Epoch [1/1], Step [23620/48549], Loss: 2.3781\n",
      "Epoch [1/1], Step [23630/48549], Loss: 2.4554\n",
      "Epoch [1/1], Step [23640/48549], Loss: 2.4596\n",
      "Epoch [1/1], Step [23650/48549], Loss: 2.4402\n",
      "Epoch [1/1], Step [23660/48549], Loss: 2.4096\n",
      "Epoch [1/1], Step [23670/48549], Loss: 2.4218\n",
      "Epoch [1/1], Step [23680/48549], Loss: 2.4128\n",
      "Epoch [1/1], Step [23690/48549], Loss: 2.3941\n",
      "Epoch [1/1], Step [23700/48549], Loss: 2.4348\n",
      "Epoch [1/1], Step [23710/48549], Loss: 2.5312\n",
      "Epoch [1/1], Step [23720/48549], Loss: 2.4356\n",
      "Epoch [1/1], Step [23730/48549], Loss: 2.3895\n",
      "Epoch [1/1], Step [23740/48549], Loss: 2.4144\n",
      "Epoch [1/1], Step [23750/48549], Loss: 2.4996\n",
      "Epoch [1/1], Step [23760/48549], Loss: 2.4428\n",
      "Epoch [1/1], Step [23770/48549], Loss: 2.4654\n",
      "Epoch [1/1], Step [23780/48549], Loss: 2.4850\n",
      "Epoch [1/1], Step [23790/48549], Loss: 2.5150\n",
      "Epoch [1/1], Step [23800/48549], Loss: 2.4365\n",
      "Epoch [1/1], Step [23810/48549], Loss: 2.5063\n",
      "Epoch [1/1], Step [23820/48549], Loss: 2.4324\n",
      "Epoch [1/1], Step [23830/48549], Loss: 2.4210\n",
      "Epoch [1/1], Step [23840/48549], Loss: 2.4420\n",
      "Epoch [1/1], Step [23850/48549], Loss: 2.4431\n",
      "Epoch [1/1], Step [23860/48549], Loss: 2.4525\n",
      "Epoch [1/1], Step [23870/48549], Loss: 2.4446\n",
      "Epoch [1/1], Step [23880/48549], Loss: 2.4725\n",
      "Epoch [1/1], Step [23890/48549], Loss: 2.4213\n",
      "Epoch [1/1], Step [23900/48549], Loss: 2.4967\n",
      "Epoch [1/1], Step [23910/48549], Loss: 2.5613\n",
      "Epoch [1/1], Step [23920/48549], Loss: 2.4527\n",
      "Epoch [1/1], Step [23930/48549], Loss: 2.4617\n",
      "Epoch [1/1], Step [23940/48549], Loss: 2.5402\n",
      "Epoch [1/1], Step [23950/48549], Loss: 2.4347\n",
      "Epoch [1/1], Step [23960/48549], Loss: 2.4885\n",
      "Epoch [1/1], Step [23970/48549], Loss: 2.5570\n",
      "Epoch [1/1], Step [23980/48549], Loss: 2.4137\n",
      "Epoch [1/1], Step [23990/48549], Loss: 2.4520\n",
      "Epoch [1/1], Step [24000/48549], Loss: 2.4818\n",
      "Epoch [1/1], Step [24010/48549], Loss: 2.4925\n",
      "Epoch [1/1], Step [24020/48549], Loss: 2.4769\n",
      "Epoch [1/1], Step [24030/48549], Loss: 2.4351\n",
      "Epoch [1/1], Step [24040/48549], Loss: 2.4427\n",
      "Epoch [1/1], Step [24050/48549], Loss: 2.4775\n",
      "Epoch [1/1], Step [24060/48549], Loss: 2.4062\n",
      "Epoch [1/1], Step [24070/48549], Loss: 2.4460\n",
      "Epoch [1/1], Step [24080/48549], Loss: 2.4635\n",
      "Epoch [1/1], Step [24090/48549], Loss: 2.4432\n",
      "Epoch [1/1], Step [24100/48549], Loss: 2.4560\n",
      "Epoch [1/1], Step [24110/48549], Loss: 2.4477\n",
      "Epoch [1/1], Step [24120/48549], Loss: 2.4752\n",
      "Epoch [1/1], Step [24130/48549], Loss: 2.5122\n",
      "Epoch [1/1], Step [24140/48549], Loss: 2.3943\n",
      "Epoch [1/1], Step [24150/48549], Loss: 2.4126\n",
      "Epoch [1/1], Step [24160/48549], Loss: 2.4414\n",
      "Epoch [1/1], Step [24170/48549], Loss: 2.4509\n",
      "Epoch [1/1], Step [24180/48549], Loss: 2.4308\n",
      "Epoch [1/1], Step [24190/48549], Loss: 2.4729\n",
      "Epoch [1/1], Step [24200/48549], Loss: 2.4428\n",
      "Epoch [1/1], Step [24210/48549], Loss: 2.3796\n",
      "Epoch [1/1], Step [24220/48549], Loss: 2.4960\n",
      "Epoch [1/1], Step [24230/48549], Loss: 2.4428\n",
      "Epoch [1/1], Step [24240/48549], Loss: 2.3916\n",
      "Epoch [1/1], Step [24250/48549], Loss: 2.4806\n",
      "Epoch [1/1], Step [24260/48549], Loss: 2.5465\n",
      "Epoch [1/1], Step [24270/48549], Loss: 2.4425\n",
      "Epoch [1/1], Step [24280/48549], Loss: 2.4643\n",
      "Epoch [1/1], Step [24290/48549], Loss: 2.3835\n",
      "Epoch [1/1], Step [24300/48549], Loss: 2.4293\n",
      "Epoch [1/1], Step [24310/48549], Loss: 2.4746\n",
      "Epoch [1/1], Step [24320/48549], Loss: 2.4835\n",
      "Epoch [1/1], Step [24330/48549], Loss: 2.4452\n",
      "Epoch [1/1], Step [24340/48549], Loss: 2.5321\n",
      "Epoch [1/1], Step [24350/48549], Loss: 2.4943\n",
      "Epoch [1/1], Step [24360/48549], Loss: 2.4570\n",
      "Epoch [1/1], Step [24370/48549], Loss: 2.4646\n",
      "Epoch [1/1], Step [24380/48549], Loss: 2.4785\n",
      "Epoch [1/1], Step [24390/48549], Loss: 2.4453\n",
      "Epoch [1/1], Step [24400/48549], Loss: 2.5695\n",
      "Epoch [1/1], Step [24410/48549], Loss: 2.3842\n",
      "Epoch [1/1], Step [24420/48549], Loss: 2.4126\n",
      "Epoch [1/1], Step [24430/48549], Loss: 2.4516\n",
      "Epoch [1/1], Step [24440/48549], Loss: 2.4616\n",
      "Epoch [1/1], Step [24450/48549], Loss: 2.4470\n",
      "Epoch [1/1], Step [24460/48549], Loss: 2.4133\n",
      "Epoch [1/1], Step [24470/48549], Loss: 2.4415\n",
      "Epoch [1/1], Step [24480/48549], Loss: 2.4642\n",
      "Epoch [1/1], Step [24490/48549], Loss: 2.4461\n",
      "Epoch [1/1], Step [24500/48549], Loss: 2.4330\n",
      "Epoch [1/1], Step [24510/48549], Loss: 2.4545\n",
      "Epoch [1/1], Step [24520/48549], Loss: 2.4368\n",
      "Epoch [1/1], Step [24530/48549], Loss: 2.4430\n",
      "Epoch [1/1], Step [24540/48549], Loss: 2.4829\n",
      "Epoch [1/1], Step [24550/48549], Loss: 2.3886\n",
      "Epoch [1/1], Step [24560/48549], Loss: 2.4772\n",
      "Epoch [1/1], Step [24570/48549], Loss: 2.3969\n",
      "Epoch [1/1], Step [24580/48549], Loss: 2.4605\n",
      "Epoch [1/1], Step [24590/48549], Loss: 2.4114\n",
      "Epoch [1/1], Step [24600/48549], Loss: 2.4464\n",
      "Epoch [1/1], Step [24610/48549], Loss: 2.4233\n",
      "Epoch [1/1], Step [24620/48549], Loss: 2.3881\n",
      "Epoch [1/1], Step [24630/48549], Loss: 2.4673\n",
      "Epoch [1/1], Step [24640/48549], Loss: 2.4424\n",
      "Epoch [1/1], Step [24650/48549], Loss: 2.4428\n",
      "Epoch [1/1], Step [24660/48549], Loss: 2.4241\n",
      "Epoch [1/1], Step [24670/48549], Loss: 2.4172\n",
      "Epoch [1/1], Step [24680/48549], Loss: 2.5004\n",
      "Epoch [1/1], Step [24690/48549], Loss: 2.4299\n",
      "Epoch [1/1], Step [24700/48549], Loss: 2.4646\n",
      "Epoch [1/1], Step [24710/48549], Loss: 2.4117\n",
      "Epoch [1/1], Step [24720/48549], Loss: 2.4050\n",
      "Epoch [1/1], Step [24730/48549], Loss: 2.4326\n",
      "Epoch [1/1], Step [24740/48549], Loss: 2.4181\n",
      "Epoch [1/1], Step [24750/48549], Loss: 2.4844\n",
      "Epoch [1/1], Step [24760/48549], Loss: 2.4310\n",
      "Epoch [1/1], Step [24770/48549], Loss: 2.4509\n",
      "Epoch [1/1], Step [24780/48549], Loss: 2.4410\n",
      "Epoch [1/1], Step [24790/48549], Loss: 2.4475\n",
      "Epoch [1/1], Step [24800/48549], Loss: 2.4586\n",
      "Epoch [1/1], Step [24810/48549], Loss: 2.4837\n",
      "Epoch [1/1], Step [24820/48549], Loss: 2.4537\n",
      "Epoch [1/1], Step [24830/48549], Loss: 2.3949\n",
      "Epoch [1/1], Step [24840/48549], Loss: 2.4650\n",
      "Epoch [1/1], Step [24850/48549], Loss: 2.4234\n",
      "Epoch [1/1], Step [24860/48549], Loss: 2.4205\n",
      "Epoch [1/1], Step [24870/48549], Loss: 2.4229\n",
      "Epoch [1/1], Step [24880/48549], Loss: 2.4374\n",
      "Epoch [1/1], Step [24890/48549], Loss: 2.4744\n",
      "Epoch [1/1], Step [24900/48549], Loss: 2.4298\n",
      "Epoch [1/1], Step [24910/48549], Loss: 2.4170\n",
      "Epoch [1/1], Step [24920/48549], Loss: 2.4564\n",
      "Epoch [1/1], Step [24930/48549], Loss: 2.4231\n",
      "Epoch [1/1], Step [24940/48549], Loss: 2.4833\n",
      "Epoch [1/1], Step [24950/48549], Loss: 2.4520\n",
      "Epoch [1/1], Step [24960/48549], Loss: 2.4758\n",
      "Epoch [1/1], Step [24970/48549], Loss: 2.4471\n",
      "Epoch [1/1], Step [24980/48549], Loss: 2.3571\n",
      "Epoch [1/1], Step [24990/48549], Loss: 2.4483\n",
      "Epoch [1/1], Step [25000/48549], Loss: 2.4344\n",
      "Epoch [1/1], Step [25010/48549], Loss: 2.3996\n",
      "Epoch [1/1], Step [25020/48549], Loss: 2.4106\n",
      "Epoch [1/1], Step [25030/48549], Loss: 2.4615\n",
      "Epoch [1/1], Step [25040/48549], Loss: 2.4587\n",
      "Epoch [1/1], Step [25050/48549], Loss: 2.4239\n",
      "Epoch [1/1], Step [25060/48549], Loss: 2.4338\n",
      "Epoch [1/1], Step [25070/48549], Loss: 2.4089\n",
      "Epoch [1/1], Step [25080/48549], Loss: 2.4551\n",
      "Epoch [1/1], Step [25090/48549], Loss: 2.4427\n",
      "Epoch [1/1], Step [25100/48549], Loss: 2.4487\n",
      "Epoch [1/1], Step [25110/48549], Loss: 2.4664\n",
      "Epoch [1/1], Step [25120/48549], Loss: 2.4980\n",
      "Epoch [1/1], Step [25130/48549], Loss: 2.3854\n",
      "Epoch [1/1], Step [25140/48549], Loss: 2.5062\n",
      "Epoch [1/1], Step [25150/48549], Loss: 2.4719\n",
      "Epoch [1/1], Step [25160/48549], Loss: 2.4649\n",
      "Epoch [1/1], Step [25170/48549], Loss: 2.5108\n",
      "Epoch [1/1], Step [25180/48549], Loss: 2.4333\n",
      "Epoch [1/1], Step [25190/48549], Loss: 2.3881\n",
      "Epoch [1/1], Step [25200/48549], Loss: 2.4328\n",
      "Epoch [1/1], Step [25210/48549], Loss: 2.4459\n",
      "Epoch [1/1], Step [25220/48549], Loss: 2.4513\n",
      "Epoch [1/1], Step [25230/48549], Loss: 2.4105\n",
      "Epoch [1/1], Step [25240/48549], Loss: 2.4291\n",
      "Epoch [1/1], Step [25250/48549], Loss: 2.4786\n",
      "Epoch [1/1], Step [25260/48549], Loss: 2.3994\n",
      "Epoch [1/1], Step [25270/48549], Loss: 2.3638\n",
      "Epoch [1/1], Step [25280/48549], Loss: 2.4254\n",
      "Epoch [1/1], Step [25290/48549], Loss: 2.4006\n",
      "Epoch [1/1], Step [25300/48549], Loss: 2.4721\n",
      "Epoch [1/1], Step [25310/48549], Loss: 2.4343\n",
      "Epoch [1/1], Step [25320/48549], Loss: 2.4350\n",
      "Epoch [1/1], Step [25330/48549], Loss: 2.4904\n",
      "Epoch [1/1], Step [25340/48549], Loss: 2.4428\n",
      "Epoch [1/1], Step [25350/48549], Loss: 2.4245\n",
      "Epoch [1/1], Step [25360/48549], Loss: 2.4731\n",
      "Epoch [1/1], Step [25370/48549], Loss: 2.4383\n",
      "Epoch [1/1], Step [25380/48549], Loss: 2.4361\n",
      "Epoch [1/1], Step [25390/48549], Loss: 2.4664\n",
      "Epoch [1/1], Step [25400/48549], Loss: 2.4079\n",
      "Epoch [1/1], Step [25410/48549], Loss: 2.4830\n",
      "Epoch [1/1], Step [25420/48549], Loss: 2.4569\n",
      "Epoch [1/1], Step [25430/48549], Loss: 2.4165\n",
      "Epoch [1/1], Step [25440/48549], Loss: 2.4505\n",
      "Epoch [1/1], Step [25450/48549], Loss: 2.4379\n",
      "Epoch [1/1], Step [25460/48549], Loss: 2.4647\n",
      "Epoch [1/1], Step [25470/48549], Loss: 2.4706\n",
      "Epoch [1/1], Step [25480/48549], Loss: 2.4562\n",
      "Epoch [1/1], Step [25490/48549], Loss: 2.4219\n",
      "Epoch [1/1], Step [25500/48549], Loss: 2.4422\n",
      "Epoch [1/1], Step [25510/48549], Loss: 2.4501\n",
      "Epoch [1/1], Step [25520/48549], Loss: 2.4029\n",
      "Epoch [1/1], Step [25530/48549], Loss: 2.4647\n",
      "Epoch [1/1], Step [25540/48549], Loss: 2.4625\n",
      "Epoch [1/1], Step [25550/48549], Loss: 2.4991\n",
      "Epoch [1/1], Step [25560/48549], Loss: 2.4524\n",
      "Epoch [1/1], Step [25570/48549], Loss: 2.4851\n",
      "Epoch [1/1], Step [25580/48549], Loss: 2.4769\n",
      "Epoch [1/1], Step [25590/48549], Loss: 2.4210\n",
      "Epoch [1/1], Step [25600/48549], Loss: 2.5303\n",
      "Epoch [1/1], Step [25610/48549], Loss: 2.4232\n",
      "Epoch [1/1], Step [25620/48549], Loss: 2.4243\n",
      "Epoch [1/1], Step [25630/48549], Loss: 2.4775\n",
      "Epoch [1/1], Step [25640/48549], Loss: 2.4677\n",
      "Epoch [1/1], Step [25650/48549], Loss: 2.5202\n",
      "Epoch [1/1], Step [25660/48549], Loss: 2.3964\n",
      "Epoch [1/1], Step [25670/48549], Loss: 2.4323\n",
      "Epoch [1/1], Step [25680/48549], Loss: 2.5235\n",
      "Epoch [1/1], Step [25690/48549], Loss: 2.4108\n",
      "Epoch [1/1], Step [25700/48549], Loss: 2.4628\n",
      "Epoch [1/1], Step [25710/48549], Loss: 2.4445\n",
      "Epoch [1/1], Step [25720/48549], Loss: 2.4085\n",
      "Epoch [1/1], Step [25730/48549], Loss: 2.4468\n",
      "Epoch [1/1], Step [25740/48549], Loss: 2.6297\n",
      "Epoch [1/1], Step [25750/48549], Loss: 2.4224\n",
      "Epoch [1/1], Step [25760/48549], Loss: 2.4708\n",
      "Epoch [1/1], Step [25770/48549], Loss: 2.4490\n",
      "Epoch [1/1], Step [25780/48549], Loss: 2.4388\n",
      "Epoch [1/1], Step [25790/48549], Loss: 2.4419\n",
      "Epoch [1/1], Step [25800/48549], Loss: 2.4232\n",
      "Epoch [1/1], Step [25810/48549], Loss: 2.3969\n",
      "Epoch [1/1], Step [25820/48549], Loss: 2.4510\n",
      "Epoch [1/1], Step [25830/48549], Loss: 2.4529\n",
      "Epoch [1/1], Step [25840/48549], Loss: 2.4013\n",
      "Epoch [1/1], Step [25850/48549], Loss: 2.4504\n",
      "Epoch [1/1], Step [25860/48549], Loss: 2.4897\n",
      "Epoch [1/1], Step [25870/48549], Loss: 2.4056\n",
      "Epoch [1/1], Step [25880/48549], Loss: 2.3629\n",
      "Epoch [1/1], Step [25890/48549], Loss: 2.4182\n",
      "Epoch [1/1], Step [25900/48549], Loss: 2.4131\n",
      "Epoch [1/1], Step [25910/48549], Loss: 2.4951\n",
      "Epoch [1/1], Step [25920/48549], Loss: 2.4801\n",
      "Epoch [1/1], Step [25930/48549], Loss: 2.4409\n",
      "Epoch [1/1], Step [25940/48549], Loss: 2.4532\n",
      "Epoch [1/1], Step [25950/48549], Loss: 2.4214\n",
      "Epoch [1/1], Step [25960/48549], Loss: 2.4887\n",
      "Epoch [1/1], Step [25970/48549], Loss: 2.3749\n",
      "Epoch [1/1], Step [25980/48549], Loss: 2.4728\n",
      "Epoch [1/1], Step [25990/48549], Loss: 2.4475\n",
      "Epoch [1/1], Step [26000/48549], Loss: 2.4479\n",
      "Epoch [1/1], Step [26010/48549], Loss: 2.4567\n",
      "Epoch [1/1], Step [26020/48549], Loss: 2.4911\n",
      "Epoch [1/1], Step [26030/48549], Loss: 2.4956\n",
      "Epoch [1/1], Step [26040/48549], Loss: 2.4910\n",
      "Epoch [1/1], Step [26050/48549], Loss: 2.4193\n",
      "Epoch [1/1], Step [26060/48549], Loss: 2.4612\n",
      "Epoch [1/1], Step [26070/48549], Loss: 2.4357\n",
      "Epoch [1/1], Step [26080/48549], Loss: 2.4548\n",
      "Epoch [1/1], Step [26090/48549], Loss: 2.4108\n",
      "Epoch [1/1], Step [26100/48549], Loss: 2.4419\n",
      "Epoch [1/1], Step [26110/48549], Loss: 2.3938\n",
      "Epoch [1/1], Step [26120/48549], Loss: 2.4087\n",
      "Epoch [1/1], Step [26130/48549], Loss: 2.6136\n",
      "Epoch [1/1], Step [26140/48549], Loss: 2.4338\n",
      "Epoch [1/1], Step [26150/48549], Loss: 2.4749\n",
      "Epoch [1/1], Step [26160/48549], Loss: 2.3867\n",
      "Epoch [1/1], Step [26170/48549], Loss: 2.4753\n",
      "Epoch [1/1], Step [26180/48549], Loss: 2.4334\n",
      "Epoch [1/1], Step [26190/48549], Loss: 2.4328\n",
      "Epoch [1/1], Step [26200/48549], Loss: 2.4624\n",
      "Epoch [1/1], Step [26210/48549], Loss: 2.4630\n",
      "Epoch [1/1], Step [26220/48549], Loss: 2.3795\n",
      "Epoch [1/1], Step [26230/48549], Loss: 2.4907\n",
      "Epoch [1/1], Step [26240/48549], Loss: 2.4475\n",
      "Epoch [1/1], Step [26250/48549], Loss: 2.5101\n",
      "Epoch [1/1], Step [26260/48549], Loss: 2.4045\n",
      "Epoch [1/1], Step [26270/48549], Loss: 2.4280\n",
      "Epoch [1/1], Step [26280/48549], Loss: 2.4117\n",
      "Epoch [1/1], Step [26290/48549], Loss: 2.4344\n",
      "Epoch [1/1], Step [26300/48549], Loss: 2.4225\n",
      "Epoch [1/1], Step [26310/48549], Loss: 2.4046\n",
      "Epoch [1/1], Step [26320/48549], Loss: 2.4582\n",
      "Epoch [1/1], Step [26330/48549], Loss: 2.4672\n",
      "Epoch [1/1], Step [26340/48549], Loss: 2.4788\n",
      "Epoch [1/1], Step [26350/48549], Loss: 2.4541\n",
      "Epoch [1/1], Step [26360/48549], Loss: 2.4320\n",
      "Epoch [1/1], Step [26370/48549], Loss: 2.4980\n",
      "Epoch [1/1], Step [26380/48549], Loss: 2.5026\n",
      "Epoch [1/1], Step [26390/48549], Loss: 2.4794\n",
      "Epoch [1/1], Step [26400/48549], Loss: 2.4108\n",
      "Epoch [1/1], Step [26410/48549], Loss: 2.4820\n",
      "Epoch [1/1], Step [26420/48549], Loss: 2.4070\n",
      "Epoch [1/1], Step [26430/48549], Loss: 2.4401\n",
      "Epoch [1/1], Step [26440/48549], Loss: 2.4016\n",
      "Epoch [1/1], Step [26450/48549], Loss: 2.4772\n",
      "Epoch [1/1], Step [26460/48549], Loss: 2.4537\n",
      "Epoch [1/1], Step [26470/48549], Loss: 2.4521\n",
      "Epoch [1/1], Step [26480/48549], Loss: 2.4088\n",
      "Epoch [1/1], Step [26490/48549], Loss: 2.4445\n",
      "Epoch [1/1], Step [26500/48549], Loss: 2.4229\n",
      "Epoch [1/1], Step [26510/48549], Loss: 2.4628\n",
      "Epoch [1/1], Step [26520/48549], Loss: 2.4738\n",
      "Epoch [1/1], Step [26530/48549], Loss: 2.4887\n",
      "Epoch [1/1], Step [26540/48549], Loss: 2.4572\n",
      "Epoch [1/1], Step [26550/48549], Loss: 2.4003\n",
      "Epoch [1/1], Step [26560/48549], Loss: 2.5276\n",
      "Epoch [1/1], Step [26570/48549], Loss: 2.5707\n",
      "Epoch [1/1], Step [26580/48549], Loss: 2.4332\n",
      "Epoch [1/1], Step [26590/48549], Loss: 2.4123\n",
      "Epoch [1/1], Step [26600/48549], Loss: 2.4164\n",
      "Epoch [1/1], Step [26610/48549], Loss: 2.4372\n",
      "Epoch [1/1], Step [26620/48549], Loss: 2.3909\n",
      "Epoch [1/1], Step [26630/48549], Loss: 2.3657\n",
      "Epoch [1/1], Step [26640/48549], Loss: 2.3449\n",
      "Epoch [1/1], Step [26650/48549], Loss: 2.4541\n",
      "Epoch [1/1], Step [26660/48549], Loss: 2.3987\n",
      "Epoch [1/1], Step [26670/48549], Loss: 2.4412\n",
      "Epoch [1/1], Step [26680/48549], Loss: 2.4490\n",
      "Epoch [1/1], Step [26690/48549], Loss: 2.4558\n",
      "Epoch [1/1], Step [26700/48549], Loss: 2.3890\n",
      "Epoch [1/1], Step [26710/48549], Loss: 2.4231\n",
      "Epoch [1/1], Step [26720/48549], Loss: 2.3742\n",
      "Epoch [1/1], Step [26730/48549], Loss: 2.4424\n",
      "Epoch [1/1], Step [26740/48549], Loss: 2.4439\n",
      "Epoch [1/1], Step [26750/48549], Loss: 2.4927\n",
      "Epoch [1/1], Step [26760/48549], Loss: 2.4135\n",
      "Epoch [1/1], Step [26770/48549], Loss: 2.3979\n",
      "Epoch [1/1], Step [26780/48549], Loss: 2.4264\n",
      "Epoch [1/1], Step [26790/48549], Loss: 2.4826\n",
      "Epoch [1/1], Step [26800/48549], Loss: 2.4994\n",
      "Epoch [1/1], Step [26810/48549], Loss: 2.4688\n",
      "Epoch [1/1], Step [26820/48549], Loss: 2.4479\n",
      "Epoch [1/1], Step [26830/48549], Loss: 2.4711\n",
      "Epoch [1/1], Step [26840/48549], Loss: 2.4012\n",
      "Epoch [1/1], Step [26850/48549], Loss: 2.4783\n",
      "Epoch [1/1], Step [26860/48549], Loss: 2.4844\n",
      "Epoch [1/1], Step [26870/48549], Loss: 2.4928\n",
      "Epoch [1/1], Step [26880/48549], Loss: 2.3956\n",
      "Epoch [1/1], Step [26890/48549], Loss: 2.4466\n",
      "Epoch [1/1], Step [26900/48549], Loss: 2.4906\n",
      "Epoch [1/1], Step [26910/48549], Loss: 2.4432\n",
      "Epoch [1/1], Step [26920/48549], Loss: 2.4273\n",
      "Epoch [1/1], Step [26930/48549], Loss: 2.4201\n",
      "Epoch [1/1], Step [26940/48549], Loss: 2.4502\n",
      "Epoch [1/1], Step [26950/48549], Loss: 2.4380\n",
      "Epoch [1/1], Step [26960/48549], Loss: 2.4373\n",
      "Epoch [1/1], Step [26970/48549], Loss: 2.4371\n",
      "Epoch [1/1], Step [26980/48549], Loss: 2.4458\n",
      "Epoch [1/1], Step [26990/48549], Loss: 2.3954\n",
      "Epoch [1/1], Step [27000/48549], Loss: 2.4358\n",
      "Epoch [1/1], Step [27010/48549], Loss: 2.4212\n",
      "Epoch [1/1], Step [27020/48549], Loss: 2.4659\n",
      "Epoch [1/1], Step [27030/48549], Loss: 2.4069\n",
      "Epoch [1/1], Step [27040/48549], Loss: 2.4578\n",
      "Epoch [1/1], Step [27050/48549], Loss: 2.4099\n",
      "Epoch [1/1], Step [27060/48549], Loss: 2.4521\n",
      "Epoch [1/1], Step [27070/48549], Loss: 2.4346\n",
      "Epoch [1/1], Step [27080/48549], Loss: 2.3946\n",
      "Epoch [1/1], Step [27090/48549], Loss: 2.4793\n",
      "Epoch [1/1], Step [27100/48549], Loss: 2.4219\n",
      "Epoch [1/1], Step [27110/48549], Loss: 2.4047\n",
      "Epoch [1/1], Step [27120/48549], Loss: 2.5135\n",
      "Epoch [1/1], Step [27130/48549], Loss: 2.4636\n",
      "Epoch [1/1], Step [27140/48549], Loss: 2.5134\n",
      "Epoch [1/1], Step [27150/48549], Loss: 2.4769\n",
      "Epoch [1/1], Step [27160/48549], Loss: 2.5003\n",
      "Epoch [1/1], Step [27170/48549], Loss: 2.4057\n",
      "Epoch [1/1], Step [27180/48549], Loss: 2.4395\n",
      "Epoch [1/1], Step [27190/48549], Loss: 2.4454\n",
      "Epoch [1/1], Step [27200/48549], Loss: 2.4271\n",
      "Epoch [1/1], Step [27210/48549], Loss: 2.4335\n",
      "Epoch [1/1], Step [27220/48549], Loss: 2.4304\n",
      "Epoch [1/1], Step [27230/48549], Loss: 2.4219\n",
      "Epoch [1/1], Step [27240/48549], Loss: 2.4212\n",
      "Epoch [1/1], Step [27250/48549], Loss: 2.4599\n",
      "Epoch [1/1], Step [27260/48549], Loss: 2.4660\n",
      "Epoch [1/1], Step [27270/48549], Loss: 2.4432\n",
      "Epoch [1/1], Step [27280/48549], Loss: 2.4374\n",
      "Epoch [1/1], Step [27290/48549], Loss: 2.4354\n",
      "Epoch [1/1], Step [27300/48549], Loss: 2.4562\n",
      "Epoch [1/1], Step [27310/48549], Loss: 2.4816\n",
      "Epoch [1/1], Step [27320/48549], Loss: 2.4518\n",
      "Epoch [1/1], Step [27330/48549], Loss: 2.4186\n",
      "Epoch [1/1], Step [27340/48549], Loss: 2.4269\n",
      "Epoch [1/1], Step [27350/48549], Loss: 2.4419\n",
      "Epoch [1/1], Step [27360/48549], Loss: 2.4096\n",
      "Epoch [1/1], Step [27370/48549], Loss: 2.4777\n",
      "Epoch [1/1], Step [27380/48549], Loss: 2.4681\n",
      "Epoch [1/1], Step [27390/48549], Loss: 2.4387\n",
      "Epoch [1/1], Step [27400/48549], Loss: 2.4797\n",
      "Epoch [1/1], Step [27410/48549], Loss: 2.4425\n",
      "Epoch [1/1], Step [27420/48549], Loss: 2.4621\n",
      "Epoch [1/1], Step [27430/48549], Loss: 2.4691\n",
      "Epoch [1/1], Step [27440/48549], Loss: 2.4979\n",
      "Epoch [1/1], Step [27450/48549], Loss: 2.5024\n",
      "Epoch [1/1], Step [27460/48549], Loss: 2.4109\n",
      "Epoch [1/1], Step [27470/48549], Loss: 2.4303\n",
      "Epoch [1/1], Step [27480/48549], Loss: 2.4301\n",
      "Epoch [1/1], Step [27490/48549], Loss: 2.4442\n",
      "Epoch [1/1], Step [27500/48549], Loss: 2.4140\n",
      "Epoch [1/1], Step [27510/48549], Loss: 2.4650\n",
      "Epoch [1/1], Step [27520/48549], Loss: 2.5230\n",
      "Epoch [1/1], Step [27530/48549], Loss: 2.4742\n",
      "Epoch [1/1], Step [27540/48549], Loss: 2.4292\n",
      "Epoch [1/1], Step [27550/48549], Loss: 2.3978\n",
      "Epoch [1/1], Step [27560/48549], Loss: 2.4231\n",
      "Epoch [1/1], Step [27570/48549], Loss: 2.4462\n",
      "Epoch [1/1], Step [27580/48549], Loss: 2.5249\n",
      "Epoch [1/1], Step [27590/48549], Loss: 2.4536\n",
      "Epoch [1/1], Step [27600/48549], Loss: 2.4522\n",
      "Epoch [1/1], Step [27610/48549], Loss: 2.4371\n",
      "Epoch [1/1], Step [27620/48549], Loss: 2.4534\n",
      "Epoch [1/1], Step [27630/48549], Loss: 2.4170\n",
      "Epoch [1/1], Step [27640/48549], Loss: 2.4611\n",
      "Epoch [1/1], Step [27650/48549], Loss: 2.4209\n",
      "Epoch [1/1], Step [27660/48549], Loss: 2.4075\n",
      "Epoch [1/1], Step [27670/48549], Loss: 2.4815\n",
      "Epoch [1/1], Step [27680/48549], Loss: 2.4418\n",
      "Epoch [1/1], Step [27690/48549], Loss: 2.4271\n",
      "Epoch [1/1], Step [27700/48549], Loss: 2.4436\n",
      "Epoch [1/1], Step [27710/48549], Loss: 2.4250\n",
      "Epoch [1/1], Step [27720/48549], Loss: 2.4901\n",
      "Epoch [1/1], Step [27730/48549], Loss: 2.4486\n",
      "Epoch [1/1], Step [27740/48549], Loss: 2.4498\n",
      "Epoch [1/1], Step [27750/48549], Loss: 2.4559\n",
      "Epoch [1/1], Step [27760/48549], Loss: 2.4021\n",
      "Epoch [1/1], Step [27770/48549], Loss: 2.4585\n",
      "Epoch [1/1], Step [27780/48549], Loss: 2.4233\n",
      "Epoch [1/1], Step [27790/48549], Loss: 2.4355\n",
      "Epoch [1/1], Step [27800/48549], Loss: 2.4460\n",
      "Epoch [1/1], Step [27810/48549], Loss: 2.4527\n",
      "Epoch [1/1], Step [27820/48549], Loss: 2.4493\n",
      "Epoch [1/1], Step [27830/48549], Loss: 2.4464\n",
      "Epoch [1/1], Step [27840/48549], Loss: 2.4396\n",
      "Epoch [1/1], Step [27850/48549], Loss: 2.3967\n",
      "Epoch [1/1], Step [27860/48549], Loss: 2.4274\n",
      "Epoch [1/1], Step [27870/48549], Loss: 2.4412\n",
      "Epoch [1/1], Step [27880/48549], Loss: 2.4116\n",
      "Epoch [1/1], Step [27890/48549], Loss: 2.4388\n",
      "Epoch [1/1], Step [27900/48549], Loss: 2.4671\n",
      "Epoch [1/1], Step [27910/48549], Loss: 2.4209\n",
      "Epoch [1/1], Step [27920/48549], Loss: 2.4205\n",
      "Epoch [1/1], Step [27930/48549], Loss: 2.5037\n",
      "Epoch [1/1], Step [27940/48549], Loss: 2.4542\n",
      "Epoch [1/1], Step [27950/48549], Loss: 2.4938\n",
      "Epoch [1/1], Step [27960/48549], Loss: 2.4258\n",
      "Epoch [1/1], Step [27970/48549], Loss: 2.4202\n",
      "Epoch [1/1], Step [27980/48549], Loss: 2.4801\n",
      "Epoch [1/1], Step [27990/48549], Loss: 2.4750\n",
      "Epoch [1/1], Step [28000/48549], Loss: 2.4635\n",
      "Epoch [1/1], Step [28010/48549], Loss: 2.4161\n",
      "Epoch [1/1], Step [28020/48549], Loss: 2.4170\n",
      "Epoch [1/1], Step [28030/48549], Loss: 2.4349\n",
      "Epoch [1/1], Step [28040/48549], Loss: 2.4077\n",
      "Epoch [1/1], Step [28050/48549], Loss: 2.4701\n",
      "Epoch [1/1], Step [28060/48549], Loss: 2.4065\n",
      "Epoch [1/1], Step [28070/48549], Loss: 2.5046\n",
      "Epoch [1/1], Step [28080/48549], Loss: 2.3712\n",
      "Epoch [1/1], Step [28090/48549], Loss: 2.4393\n",
      "Epoch [1/1], Step [28100/48549], Loss: 2.4269\n",
      "Epoch [1/1], Step [28110/48549], Loss: 2.4261\n",
      "Epoch [1/1], Step [28120/48549], Loss: 2.4763\n",
      "Epoch [1/1], Step [28130/48549], Loss: 2.4360\n",
      "Epoch [1/1], Step [28140/48549], Loss: 2.4913\n",
      "Epoch [1/1], Step [28150/48549], Loss: 2.4808\n",
      "Epoch [1/1], Step [28160/48549], Loss: 2.4346\n",
      "Epoch [1/1], Step [28170/48549], Loss: 2.5182\n",
      "Epoch [1/1], Step [28180/48549], Loss: 2.4076\n",
      "Epoch [1/1], Step [28190/48549], Loss: 2.4106\n",
      "Epoch [1/1], Step [28200/48549], Loss: 2.4691\n",
      "Epoch [1/1], Step [28210/48549], Loss: 2.4572\n",
      "Epoch [1/1], Step [28220/48549], Loss: 2.5200\n",
      "Epoch [1/1], Step [28230/48549], Loss: 2.4403\n",
      "Epoch [1/1], Step [28240/48549], Loss: 2.4691\n",
      "Epoch [1/1], Step [28250/48549], Loss: 2.4080\n",
      "Epoch [1/1], Step [28260/48549], Loss: 2.4946\n",
      "Epoch [1/1], Step [28270/48549], Loss: 2.4242\n",
      "Epoch [1/1], Step [28280/48549], Loss: 2.4450\n",
      "Epoch [1/1], Step [28290/48549], Loss: 2.4855\n",
      "Epoch [1/1], Step [28300/48549], Loss: 2.4817\n",
      "Epoch [1/1], Step [28310/48549], Loss: 2.4608\n",
      "Epoch [1/1], Step [28320/48549], Loss: 2.4436\n",
      "Epoch [1/1], Step [28330/48549], Loss: 2.4951\n",
      "Epoch [1/1], Step [28340/48549], Loss: 2.4606\n",
      "Epoch [1/1], Step [28350/48549], Loss: 2.4527\n",
      "Epoch [1/1], Step [28360/48549], Loss: 2.4299\n",
      "Epoch [1/1], Step [28370/48549], Loss: 2.4512\n",
      "Epoch [1/1], Step [28380/48549], Loss: 2.4614\n",
      "Epoch [1/1], Step [28390/48549], Loss: 2.3932\n",
      "Epoch [1/1], Step [28400/48549], Loss: 2.4456\n",
      "Epoch [1/1], Step [28410/48549], Loss: 2.4412\n",
      "Epoch [1/1], Step [28420/48549], Loss: 2.4364\n",
      "Epoch [1/1], Step [28430/48549], Loss: 2.4344\n",
      "Epoch [1/1], Step [28440/48549], Loss: 2.4569\n",
      "Epoch [1/1], Step [28450/48549], Loss: 2.4280\n",
      "Epoch [1/1], Step [28460/48549], Loss: 2.4289\n",
      "Epoch [1/1], Step [28470/48549], Loss: 2.4185\n",
      "Epoch [1/1], Step [28480/48549], Loss: 2.5008\n",
      "Epoch [1/1], Step [28490/48549], Loss: 2.4395\n",
      "Epoch [1/1], Step [28500/48549], Loss: 2.4333\n",
      "Epoch [1/1], Step [28510/48549], Loss: 2.3862\n",
      "Epoch [1/1], Step [28520/48549], Loss: 2.4814\n",
      "Epoch [1/1], Step [28530/48549], Loss: 2.5664\n",
      "Epoch [1/1], Step [28540/48549], Loss: 2.4474\n",
      "Epoch [1/1], Step [28550/48549], Loss: 2.4330\n",
      "Epoch [1/1], Step [28560/48549], Loss: 2.4399\n",
      "Epoch [1/1], Step [28570/48549], Loss: 2.4455\n",
      "Epoch [1/1], Step [28580/48549], Loss: 2.4740\n",
      "Epoch [1/1], Step [28590/48549], Loss: 2.4210\n",
      "Epoch [1/1], Step [28600/48549], Loss: 2.4402\n",
      "Epoch [1/1], Step [28610/48549], Loss: 2.4850\n",
      "Epoch [1/1], Step [28620/48549], Loss: 2.4128\n",
      "Epoch [1/1], Step [28630/48549], Loss: 2.4723\n",
      "Epoch [1/1], Step [28640/48549], Loss: 2.6683\n",
      "Epoch [1/1], Step [28650/48549], Loss: 2.4301\n",
      "Epoch [1/1], Step [28660/48549], Loss: 2.5330\n",
      "Epoch [1/1], Step [28670/48549], Loss: 2.4680\n",
      "Epoch [1/1], Step [28680/48549], Loss: 2.4417\n",
      "Epoch [1/1], Step [28690/48549], Loss: 2.5264\n",
      "Epoch [1/1], Step [28700/48549], Loss: 2.4379\n",
      "Epoch [1/1], Step [28710/48549], Loss: 2.4731\n",
      "Epoch [1/1], Step [28720/48549], Loss: 2.4506\n",
      "Epoch [1/1], Step [28730/48549], Loss: 2.4531\n",
      "Epoch [1/1], Step [28740/48549], Loss: 2.4880\n",
      "Epoch [1/1], Step [28750/48549], Loss: 2.4411\n",
      "Epoch [1/1], Step [28760/48549], Loss: 2.4400\n",
      "Epoch [1/1], Step [28770/48549], Loss: 2.4274\n",
      "Epoch [1/1], Step [28780/48549], Loss: 2.4161\n",
      "Epoch [1/1], Step [28790/48549], Loss: 2.4333\n",
      "Epoch [1/1], Step [28800/48549], Loss: 2.4075\n",
      "Epoch [1/1], Step [28810/48549], Loss: 2.4458\n",
      "Epoch [1/1], Step [28820/48549], Loss: 2.4431\n",
      "Epoch [1/1], Step [28830/48549], Loss: 2.4314\n",
      "Epoch [1/1], Step [28840/48549], Loss: 2.4083\n",
      "Epoch [1/1], Step [28850/48549], Loss: 2.4600\n",
      "Epoch [1/1], Step [28860/48549], Loss: 2.4424\n",
      "Epoch [1/1], Step [28870/48549], Loss: 2.4502\n",
      "Epoch [1/1], Step [28880/48549], Loss: 2.4272\n",
      "Epoch [1/1], Step [28890/48549], Loss: 2.4452\n",
      "Epoch [1/1], Step [28900/48549], Loss: 2.5245\n",
      "Epoch [1/1], Step [28910/48549], Loss: 2.4968\n",
      "Epoch [1/1], Step [28920/48549], Loss: 2.4912\n",
      "Epoch [1/1], Step [28930/48549], Loss: 2.4749\n",
      "Epoch [1/1], Step [28940/48549], Loss: 2.5232\n",
      "Epoch [1/1], Step [28950/48549], Loss: 2.5828\n",
      "Epoch [1/1], Step [28960/48549], Loss: 2.4566\n",
      "Epoch [1/1], Step [28970/48549], Loss: 2.4446\n",
      "Epoch [1/1], Step [28980/48549], Loss: 2.4152\n",
      "Epoch [1/1], Step [28990/48549], Loss: 2.4797\n",
      "Epoch [1/1], Step [29000/48549], Loss: 2.5016\n",
      "Epoch [1/1], Step [29010/48549], Loss: 2.4581\n",
      "Epoch [1/1], Step [29020/48549], Loss: 2.4548\n",
      "Epoch [1/1], Step [29030/48549], Loss: 2.3397\n",
      "Epoch [1/1], Step [29040/48549], Loss: 2.4460\n",
      "Epoch [1/1], Step [29050/48549], Loss: 2.4395\n",
      "Epoch [1/1], Step [29060/48549], Loss: 2.4708\n",
      "Epoch [1/1], Step [29070/48549], Loss: 2.4166\n",
      "Epoch [1/1], Step [29080/48549], Loss: 2.3949\n",
      "Epoch [1/1], Step [29090/48549], Loss: 2.4540\n",
      "Epoch [1/1], Step [29100/48549], Loss: 2.4214\n",
      "Epoch [1/1], Step [29110/48549], Loss: 2.5041\n",
      "Epoch [1/1], Step [29120/48549], Loss: 2.4473\n",
      "Epoch [1/1], Step [29130/48549], Loss: 2.4369\n",
      "Epoch [1/1], Step [29140/48549], Loss: 2.4168\n",
      "Epoch [1/1], Step [29150/48549], Loss: 2.4386\n",
      "Epoch [1/1], Step [29160/48549], Loss: 2.4371\n",
      "Epoch [1/1], Step [29170/48549], Loss: 2.4566\n",
      "Epoch [1/1], Step [29180/48549], Loss: 2.5042\n",
      "Epoch [1/1], Step [29190/48549], Loss: 2.4403\n",
      "Epoch [1/1], Step [29200/48549], Loss: 2.4340\n",
      "Epoch [1/1], Step [29210/48549], Loss: 2.3640\n",
      "Epoch [1/1], Step [29220/48549], Loss: 2.4280\n",
      "Epoch [1/1], Step [29230/48549], Loss: 2.4031\n",
      "Epoch [1/1], Step [29240/48549], Loss: 2.4496\n",
      "Epoch [1/1], Step [29250/48549], Loss: 2.4575\n",
      "Epoch [1/1], Step [29260/48549], Loss: 2.4446\n",
      "Epoch [1/1], Step [29270/48549], Loss: 2.4772\n",
      "Epoch [1/1], Step [29280/48549], Loss: 2.4316\n",
      "Epoch [1/1], Step [29290/48549], Loss: 2.4505\n",
      "Epoch [1/1], Step [29300/48549], Loss: 2.4595\n",
      "Epoch [1/1], Step [29310/48549], Loss: 2.3720\n",
      "Epoch [1/1], Step [29320/48549], Loss: 2.4431\n",
      "Epoch [1/1], Step [29330/48549], Loss: 2.4049\n",
      "Epoch [1/1], Step [29340/48549], Loss: 2.4130\n",
      "Epoch [1/1], Step [29350/48549], Loss: 2.4279\n",
      "Epoch [1/1], Step [29360/48549], Loss: 2.4420\n",
      "Epoch [1/1], Step [29370/48549], Loss: 2.3969\n",
      "Epoch [1/1], Step [29380/48549], Loss: 2.4932\n",
      "Epoch [1/1], Step [29390/48549], Loss: 2.4092\n",
      "Epoch [1/1], Step [29400/48549], Loss: 2.4150\n",
      "Epoch [1/1], Step [29410/48549], Loss: 2.4342\n",
      "Epoch [1/1], Step [29420/48549], Loss: 2.6135\n",
      "Epoch [1/1], Step [29430/48549], Loss: 2.4100\n",
      "Epoch [1/1], Step [29440/48549], Loss: 2.4836\n",
      "Epoch [1/1], Step [29450/48549], Loss: 2.5246\n",
      "Epoch [1/1], Step [29460/48549], Loss: 2.4495\n",
      "Epoch [1/1], Step [29470/48549], Loss: 2.4157\n",
      "Epoch [1/1], Step [29480/48549], Loss: 2.4497\n",
      "Epoch [1/1], Step [29490/48549], Loss: 2.4027\n",
      "Epoch [1/1], Step [29500/48549], Loss: 2.4744\n",
      "Epoch [1/1], Step [29510/48549], Loss: 2.4612\n",
      "Epoch [1/1], Step [29520/48549], Loss: 2.4350\n",
      "Epoch [1/1], Step [29530/48549], Loss: 2.4391\n",
      "Epoch [1/1], Step [29540/48549], Loss: 2.4154\n",
      "Epoch [1/1], Step [29550/48549], Loss: 2.4745\n",
      "Epoch [1/1], Step [29560/48549], Loss: 2.4149\n",
      "Epoch [1/1], Step [29570/48549], Loss: 2.4231\n",
      "Epoch [1/1], Step [29580/48549], Loss: 2.4151\n",
      "Epoch [1/1], Step [29590/48549], Loss: 2.4460\n",
      "Epoch [1/1], Step [29600/48549], Loss: 2.4190\n",
      "Epoch [1/1], Step [29610/48549], Loss: 2.4470\n",
      "Epoch [1/1], Step [29620/48549], Loss: 2.4951\n",
      "Epoch [1/1], Step [29630/48549], Loss: 2.4718\n",
      "Epoch [1/1], Step [29640/48549], Loss: 2.4361\n",
      "Epoch [1/1], Step [29650/48549], Loss: 2.4770\n",
      "Epoch [1/1], Step [29660/48549], Loss: 2.4602\n",
      "Epoch [1/1], Step [29670/48549], Loss: 2.4119\n",
      "Epoch [1/1], Step [29680/48549], Loss: 2.4566\n",
      "Epoch [1/1], Step [29690/48549], Loss: 2.4643\n",
      "Epoch [1/1], Step [29700/48549], Loss: 2.4250\n",
      "Epoch [1/1], Step [29710/48549], Loss: 2.3954\n",
      "Epoch [1/1], Step [29720/48549], Loss: 2.4377\n",
      "Epoch [1/1], Step [29730/48549], Loss: 2.4176\n",
      "Epoch [1/1], Step [29740/48549], Loss: 2.5129\n",
      "Epoch [1/1], Step [29750/48549], Loss: 2.3777\n",
      "Epoch [1/1], Step [29760/48549], Loss: 2.3873\n",
      "Epoch [1/1], Step [29770/48549], Loss: 2.4811\n",
      "Epoch [1/1], Step [29780/48549], Loss: 2.6128\n",
      "Epoch [1/1], Step [29790/48549], Loss: 2.4192\n",
      "Epoch [1/1], Step [29800/48549], Loss: 2.4895\n",
      "Epoch [1/1], Step [29810/48549], Loss: 2.4320\n",
      "Epoch [1/1], Step [29820/48549], Loss: 2.5023\n",
      "Epoch [1/1], Step [29830/48549], Loss: 2.4185\n",
      "Epoch [1/1], Step [29840/48549], Loss: 2.4393\n",
      "Epoch [1/1], Step [29850/48549], Loss: 2.4260\n",
      "Epoch [1/1], Step [29860/48549], Loss: 2.3731\n",
      "Epoch [1/1], Step [29870/48549], Loss: 2.4661\n",
      "Epoch [1/1], Step [29880/48549], Loss: 2.4296\n",
      "Epoch [1/1], Step [29890/48549], Loss: 2.3957\n",
      "Epoch [1/1], Step [29900/48549], Loss: 2.4691\n",
      "Epoch [1/1], Step [29910/48549], Loss: 2.4321\n",
      "Epoch [1/1], Step [29920/48549], Loss: 2.4326\n",
      "Epoch [1/1], Step [29930/48549], Loss: 2.4095\n",
      "Epoch [1/1], Step [29940/48549], Loss: 2.4055\n",
      "Epoch [1/1], Step [29950/48549], Loss: 2.4528\n",
      "Epoch [1/1], Step [29960/48549], Loss: 2.4446\n",
      "Epoch [1/1], Step [29970/48549], Loss: 2.4538\n",
      "Epoch [1/1], Step [29980/48549], Loss: 2.4775\n",
      "Epoch [1/1], Step [29990/48549], Loss: 2.4571\n",
      "Epoch [1/1], Step [30000/48549], Loss: 2.4492\n",
      "Epoch [1/1], Step [30010/48549], Loss: 2.3834\n",
      "Epoch [1/1], Step [30020/48549], Loss: 2.4532\n",
      "Epoch [1/1], Step [30030/48549], Loss: 2.4080\n",
      "Epoch [1/1], Step [30040/48549], Loss: 2.4784\n",
      "Epoch [1/1], Step [30050/48549], Loss: 2.4281\n",
      "Epoch [1/1], Step [30060/48549], Loss: 2.4374\n",
      "Epoch [1/1], Step [30070/48549], Loss: 2.4207\n",
      "Epoch [1/1], Step [30080/48549], Loss: 2.4274\n",
      "Epoch [1/1], Step [30090/48549], Loss: 2.4143\n",
      "Epoch [1/1], Step [30100/48549], Loss: 2.5265\n",
      "Epoch [1/1], Step [30110/48549], Loss: 2.5424\n",
      "Epoch [1/1], Step [30120/48549], Loss: 2.4575\n",
      "Epoch [1/1], Step [30130/48549], Loss: 2.4247\n",
      "Epoch [1/1], Step [30140/48549], Loss: 2.4834\n",
      "Epoch [1/1], Step [30150/48549], Loss: 2.4469\n",
      "Epoch [1/1], Step [30160/48549], Loss: 2.4819\n",
      "Epoch [1/1], Step [30170/48549], Loss: 2.4795\n",
      "Epoch [1/1], Step [30180/48549], Loss: 2.4347\n",
      "Epoch [1/1], Step [30190/48549], Loss: 2.4425\n",
      "Epoch [1/1], Step [30200/48549], Loss: 2.4566\n",
      "Epoch [1/1], Step [30210/48549], Loss: 2.5052\n",
      "Epoch [1/1], Step [30220/48549], Loss: 2.4518\n",
      "Epoch [1/1], Step [30230/48549], Loss: 2.4360\n",
      "Epoch [1/1], Step [30240/48549], Loss: 2.3978\n",
      "Epoch [1/1], Step [30250/48549], Loss: 2.4653\n",
      "Epoch [1/1], Step [30260/48549], Loss: 2.3843\n",
      "Epoch [1/1], Step [30270/48549], Loss: 2.4029\n",
      "Epoch [1/1], Step [30280/48549], Loss: 2.4958\n",
      "Epoch [1/1], Step [30290/48549], Loss: 2.4493\n",
      "Epoch [1/1], Step [30300/48549], Loss: 2.4264\n",
      "Epoch [1/1], Step [30310/48549], Loss: 2.4158\n",
      "Epoch [1/1], Step [30320/48549], Loss: 2.4268\n",
      "Epoch [1/1], Step [30330/48549], Loss: 2.4171\n",
      "Epoch [1/1], Step [30340/48549], Loss: 2.4724\n",
      "Epoch [1/1], Step [30350/48549], Loss: 2.3968\n",
      "Epoch [1/1], Step [30360/48549], Loss: 2.4220\n",
      "Epoch [1/1], Step [30370/48549], Loss: 2.4505\n",
      "Epoch [1/1], Step [30380/48549], Loss: 2.3957\n",
      "Epoch [1/1], Step [30390/48549], Loss: 2.4373\n",
      "Epoch [1/1], Step [30400/48549], Loss: 2.4826\n",
      "Epoch [1/1], Step [30410/48549], Loss: 2.4560\n",
      "Epoch [1/1], Step [30420/48549], Loss: 2.4717\n",
      "Epoch [1/1], Step [30430/48549], Loss: 2.4290\n",
      "Epoch [1/1], Step [30440/48549], Loss: 2.4636\n",
      "Epoch [1/1], Step [30450/48549], Loss: 2.3947\n",
      "Epoch [1/1], Step [30460/48549], Loss: 2.5050\n",
      "Epoch [1/1], Step [30470/48549], Loss: 2.4483\n",
      "Epoch [1/1], Step [30480/48549], Loss: 2.4386\n",
      "Epoch [1/1], Step [30490/48549], Loss: 2.4216\n",
      "Epoch [1/1], Step [30500/48549], Loss: 2.4206\n",
      "Epoch [1/1], Step [30510/48549], Loss: 2.4261\n",
      "Epoch [1/1], Step [30520/48549], Loss: 2.4065\n",
      "Epoch [1/1], Step [30530/48549], Loss: 2.4915\n",
      "Epoch [1/1], Step [30540/48549], Loss: 2.4224\n",
      "Epoch [1/1], Step [30550/48549], Loss: 2.5299\n",
      "Epoch [1/1], Step [30560/48549], Loss: 2.3940\n",
      "Epoch [1/1], Step [30570/48549], Loss: 2.4183\n",
      "Epoch [1/1], Step [30580/48549], Loss: 2.4078\n",
      "Epoch [1/1], Step [30590/48549], Loss: 2.3993\n",
      "Epoch [1/1], Step [30600/48549], Loss: 2.3873\n",
      "Epoch [1/1], Step [30610/48549], Loss: 2.4342\n",
      "Epoch [1/1], Step [30620/48549], Loss: 2.4463\n",
      "Epoch [1/1], Step [30630/48549], Loss: 2.3999\n",
      "Epoch [1/1], Step [30640/48549], Loss: 2.4412\n",
      "Epoch [1/1], Step [30650/48549], Loss: 2.4929\n",
      "Epoch [1/1], Step [30660/48549], Loss: 2.4007\n",
      "Epoch [1/1], Step [30670/48549], Loss: 2.4188\n",
      "Epoch [1/1], Step [30680/48549], Loss: 2.4420\n",
      "Epoch [1/1], Step [30690/48549], Loss: 2.4157\n",
      "Epoch [1/1], Step [30700/48549], Loss: 2.4236\n",
      "Epoch [1/1], Step [30710/48549], Loss: 2.4433\n",
      "Epoch [1/1], Step [30720/48549], Loss: 2.5000\n",
      "Epoch [1/1], Step [30730/48549], Loss: 2.4471\n",
      "Epoch [1/1], Step [30740/48549], Loss: 2.4306\n",
      "Epoch [1/1], Step [30750/48549], Loss: 2.4770\n",
      "Epoch [1/1], Step [30760/48549], Loss: 2.4097\n",
      "Epoch [1/1], Step [30770/48549], Loss: 2.4237\n",
      "Epoch [1/1], Step [30780/48549], Loss: 2.4322\n",
      "Epoch [1/1], Step [30790/48549], Loss: 2.4817\n",
      "Epoch [1/1], Step [30800/48549], Loss: 2.4782\n",
      "Epoch [1/1], Step [30810/48549], Loss: 2.4442\n",
      "Epoch [1/1], Step [30820/48549], Loss: 2.4897\n",
      "Epoch [1/1], Step [30830/48549], Loss: 2.4807\n",
      "Epoch [1/1], Step [30840/48549], Loss: 2.4307\n",
      "Epoch [1/1], Step [30850/48549], Loss: 2.4343\n",
      "Epoch [1/1], Step [30860/48549], Loss: 2.4512\n",
      "Epoch [1/1], Step [30870/48549], Loss: 2.4612\n",
      "Epoch [1/1], Step [30880/48549], Loss: 2.4857\n",
      "Epoch [1/1], Step [30890/48549], Loss: 2.4344\n",
      "Epoch [1/1], Step [30900/48549], Loss: 2.4407\n",
      "Epoch [1/1], Step [30910/48549], Loss: 2.5547\n",
      "Epoch [1/1], Step [30920/48549], Loss: 2.4308\n",
      "Epoch [1/1], Step [30930/48549], Loss: 2.4757\n",
      "Epoch [1/1], Step [30940/48549], Loss: 2.4475\n",
      "Epoch [1/1], Step [30950/48549], Loss: 2.4352\n",
      "Epoch [1/1], Step [30960/48549], Loss: 2.4516\n",
      "Epoch [1/1], Step [30970/48549], Loss: 2.4438\n",
      "Epoch [1/1], Step [30980/48549], Loss: 2.5333\n",
      "Epoch [1/1], Step [30990/48549], Loss: 2.4648\n",
      "Epoch [1/1], Step [31000/48549], Loss: 2.4636\n",
      "Epoch [1/1], Step [31010/48549], Loss: 2.4469\n",
      "Epoch [1/1], Step [31020/48549], Loss: 2.5001\n",
      "Epoch [1/1], Step [31030/48549], Loss: 2.4773\n",
      "Epoch [1/1], Step [31040/48549], Loss: 2.4337\n",
      "Epoch [1/1], Step [31050/48549], Loss: 2.4203\n",
      "Epoch [1/1], Step [31060/48549], Loss: 2.4508\n",
      "Epoch [1/1], Step [31070/48549], Loss: 2.4389\n",
      "Epoch [1/1], Step [31080/48549], Loss: 2.3946\n",
      "Epoch [1/1], Step [31090/48549], Loss: 2.4478\n",
      "Epoch [1/1], Step [31100/48549], Loss: 2.4092\n",
      "Epoch [1/1], Step [31110/48549], Loss: 2.4591\n",
      "Epoch [1/1], Step [31120/48549], Loss: 2.3993\n",
      "Epoch [1/1], Step [31130/48549], Loss: 2.4231\n",
      "Epoch [1/1], Step [31140/48549], Loss: 2.4647\n",
      "Epoch [1/1], Step [31150/48549], Loss: 2.4125\n",
      "Epoch [1/1], Step [31160/48549], Loss: 2.4121\n",
      "Epoch [1/1], Step [31170/48549], Loss: 2.4714\n",
      "Epoch [1/1], Step [31180/48549], Loss: 2.6561\n",
      "Epoch [1/1], Step [31190/48549], Loss: 2.4703\n",
      "Epoch [1/1], Step [31200/48549], Loss: 2.4677\n",
      "Epoch [1/1], Step [31210/48549], Loss: 2.4398\n",
      "Epoch [1/1], Step [31220/48549], Loss: 2.4537\n",
      "Epoch [1/1], Step [31230/48549], Loss: 2.4585\n",
      "Epoch [1/1], Step [31240/48549], Loss: 2.4002\n",
      "Epoch [1/1], Step [31250/48549], Loss: 2.4345\n",
      "Epoch [1/1], Step [31260/48549], Loss: 2.4894\n",
      "Epoch [1/1], Step [31270/48549], Loss: 2.3954\n",
      "Epoch [1/1], Step [31280/48549], Loss: 2.4796\n",
      "Epoch [1/1], Step [31290/48549], Loss: 2.4375\n",
      "Epoch [1/1], Step [31300/48549], Loss: 2.3817\n",
      "Epoch [1/1], Step [31310/48549], Loss: 2.3989\n",
      "Epoch [1/1], Step [31320/48549], Loss: 2.4496\n",
      "Epoch [1/1], Step [31330/48549], Loss: 2.4743\n",
      "Epoch [1/1], Step [31340/48549], Loss: 2.4360\n",
      "Epoch [1/1], Step [31350/48549], Loss: 2.4682\n",
      "Epoch [1/1], Step [31360/48549], Loss: 2.4587\n",
      "Epoch [1/1], Step [31370/48549], Loss: 2.4774\n",
      "Epoch [1/1], Step [31380/48549], Loss: 2.3999\n",
      "Epoch [1/1], Step [31390/48549], Loss: 2.4689\n",
      "Epoch [1/1], Step [31400/48549], Loss: 2.5060\n",
      "Epoch [1/1], Step [31410/48549], Loss: 2.4771\n",
      "Epoch [1/1], Step [31420/48549], Loss: 2.4412\n",
      "Epoch [1/1], Step [31430/48549], Loss: 2.4839\n",
      "Epoch [1/1], Step [31440/48549], Loss: 2.4406\n",
      "Epoch [1/1], Step [31450/48549], Loss: 2.4236\n",
      "Epoch [1/1], Step [31460/48549], Loss: 2.4325\n",
      "Epoch [1/1], Step [31470/48549], Loss: 2.4811\n",
      "Epoch [1/1], Step [31480/48549], Loss: 2.4459\n",
      "Epoch [1/1], Step [31490/48549], Loss: 2.5011\n",
      "Epoch [1/1], Step [31500/48549], Loss: 2.4671\n",
      "Epoch [1/1], Step [31510/48549], Loss: 2.3910\n",
      "Epoch [1/1], Step [31520/48549], Loss: 2.4674\n",
      "Epoch [1/1], Step [31530/48549], Loss: 2.4168\n",
      "Epoch [1/1], Step [31540/48549], Loss: 2.4081\n",
      "Epoch [1/1], Step [31550/48549], Loss: 2.5317\n",
      "Epoch [1/1], Step [31560/48549], Loss: 2.4285\n",
      "Epoch [1/1], Step [31570/48549], Loss: 2.5434\n",
      "Epoch [1/1], Step [31580/48549], Loss: 2.4091\n",
      "Epoch [1/1], Step [31590/48549], Loss: 2.4595\n",
      "Epoch [1/1], Step [31600/48549], Loss: 2.4393\n",
      "Epoch [1/1], Step [31610/48549], Loss: 2.3858\n",
      "Epoch [1/1], Step [31620/48549], Loss: 2.4424\n",
      "Epoch [1/1], Step [31630/48549], Loss: 2.4922\n",
      "Epoch [1/1], Step [31640/48549], Loss: 2.4403\n",
      "Epoch [1/1], Step [31650/48549], Loss: 2.5564\n",
      "Epoch [1/1], Step [31660/48549], Loss: 2.4273\n",
      "Epoch [1/1], Step [31670/48549], Loss: 2.3820\n",
      "Epoch [1/1], Step [31680/48549], Loss: 2.4699\n",
      "Epoch [1/1], Step [31690/48549], Loss: 2.4709\n",
      "Epoch [1/1], Step [31700/48549], Loss: 2.4728\n",
      "Epoch [1/1], Step [31710/48549], Loss: 2.4625\n",
      "Epoch [1/1], Step [31720/48549], Loss: 2.4173\n",
      "Epoch [1/1], Step [31730/48549], Loss: 2.5091\n",
      "Epoch [1/1], Step [31740/48549], Loss: 2.5411\n",
      "Epoch [1/1], Step [31750/48549], Loss: 2.3857\n",
      "Epoch [1/1], Step [31760/48549], Loss: 2.3928\n",
      "Epoch [1/1], Step [31770/48549], Loss: 2.4991\n",
      "Epoch [1/1], Step [31780/48549], Loss: 2.4862\n",
      "Epoch [1/1], Step [31790/48549], Loss: 2.4853\n",
      "Epoch [1/1], Step [31800/48549], Loss: 2.4031\n",
      "Epoch [1/1], Step [31810/48549], Loss: 2.4282\n",
      "Epoch [1/1], Step [31820/48549], Loss: 2.4900\n",
      "Epoch [1/1], Step [31830/48549], Loss: 2.4523\n",
      "Epoch [1/1], Step [31840/48549], Loss: 2.4732\n",
      "Epoch [1/1], Step [31850/48549], Loss: 2.5223\n",
      "Epoch [1/1], Step [31860/48549], Loss: 2.4863\n",
      "Epoch [1/1], Step [31870/48549], Loss: 2.4590\n",
      "Epoch [1/1], Step [31880/48549], Loss: 2.4155\n",
      "Epoch [1/1], Step [31890/48549], Loss: 2.4304\n",
      "Epoch [1/1], Step [31900/48549], Loss: 2.4198\n",
      "Epoch [1/1], Step [31910/48549], Loss: 2.4031\n",
      "Epoch [1/1], Step [31920/48549], Loss: 2.4558\n",
      "Epoch [1/1], Step [31930/48549], Loss: 2.4199\n",
      "Epoch [1/1], Step [31940/48549], Loss: 2.4248\n",
      "Epoch [1/1], Step [31950/48549], Loss: 2.4150\n",
      "Epoch [1/1], Step [31960/48549], Loss: 2.4543\n",
      "Epoch [1/1], Step [31970/48549], Loss: 2.3909\n",
      "Epoch [1/1], Step [31980/48549], Loss: 2.4342\n",
      "Epoch [1/1], Step [31990/48549], Loss: 2.4472\n",
      "Epoch [1/1], Step [32000/48549], Loss: 2.4549\n",
      "Epoch [1/1], Step [32010/48549], Loss: 2.4663\n",
      "Epoch [1/1], Step [32020/48549], Loss: 2.4894\n",
      "Epoch [1/1], Step [32030/48549], Loss: 2.4553\n",
      "Epoch [1/1], Step [32040/48549], Loss: 2.4283\n",
      "Epoch [1/1], Step [32050/48549], Loss: 2.4457\n",
      "Epoch [1/1], Step [32060/48549], Loss: 2.4540\n",
      "Epoch [1/1], Step [32070/48549], Loss: 2.4018\n",
      "Epoch [1/1], Step [32080/48549], Loss: 2.4531\n",
      "Epoch [1/1], Step [32090/48549], Loss: 2.4127\n",
      "Epoch [1/1], Step [32100/48549], Loss: 2.3893\n",
      "Epoch [1/1], Step [32110/48549], Loss: 2.4341\n",
      "Epoch [1/1], Step [32120/48549], Loss: 2.4814\n",
      "Epoch [1/1], Step [32130/48549], Loss: 2.4474\n",
      "Epoch [1/1], Step [32140/48549], Loss: 2.4678\n",
      "Epoch [1/1], Step [32150/48549], Loss: 2.3710\n",
      "Epoch [1/1], Step [32160/48549], Loss: 2.5108\n",
      "Epoch [1/1], Step [32170/48549], Loss: 2.4691\n",
      "Epoch [1/1], Step [32180/48549], Loss: 2.4143\n",
      "Epoch [1/1], Step [32190/48549], Loss: 2.3908\n",
      "Epoch [1/1], Step [32200/48549], Loss: 2.4368\n",
      "Epoch [1/1], Step [32210/48549], Loss: 2.4070\n",
      "Epoch [1/1], Step [32220/48549], Loss: 2.3683\n",
      "Epoch [1/1], Step [32230/48549], Loss: 2.4472\n",
      "Epoch [1/1], Step [32240/48549], Loss: 2.4179\n",
      "Epoch [1/1], Step [32250/48549], Loss: 2.4904\n",
      "Epoch [1/1], Step [32260/48549], Loss: 2.6635\n",
      "Epoch [1/1], Step [32270/48549], Loss: 2.4910\n",
      "Epoch [1/1], Step [32280/48549], Loss: 2.4329\n",
      "Epoch [1/1], Step [32290/48549], Loss: 2.3959\n",
      "Epoch [1/1], Step [32300/48549], Loss: 2.4694\n",
      "Epoch [1/1], Step [32310/48549], Loss: 2.4287\n",
      "Epoch [1/1], Step [32320/48549], Loss: 2.4328\n",
      "Epoch [1/1], Step [32330/48549], Loss: 2.4497\n",
      "Epoch [1/1], Step [32340/48549], Loss: 2.4464\n",
      "Epoch [1/1], Step [32350/48549], Loss: 2.5173\n",
      "Epoch [1/1], Step [32360/48549], Loss: 2.4353\n",
      "Epoch [1/1], Step [32370/48549], Loss: 2.4523\n",
      "Epoch [1/1], Step [32380/48549], Loss: 2.4707\n",
      "Epoch [1/1], Step [32390/48549], Loss: 2.5594\n",
      "Epoch [1/1], Step [32400/48549], Loss: 2.4572\n",
      "Epoch [1/1], Step [32410/48549], Loss: 2.4620\n",
      "Epoch [1/1], Step [32420/48549], Loss: 2.4315\n",
      "Epoch [1/1], Step [32430/48549], Loss: 2.5120\n",
      "Epoch [1/1], Step [32440/48549], Loss: 2.4149\n",
      "Epoch [1/1], Step [32450/48549], Loss: 2.4300\n",
      "Epoch [1/1], Step [32460/48549], Loss: 2.3890\n",
      "Epoch [1/1], Step [32470/48549], Loss: 2.4910\n",
      "Epoch [1/1], Step [32480/48549], Loss: 2.4160\n",
      "Epoch [1/1], Step [32490/48549], Loss: 2.4888\n",
      "Epoch [1/1], Step [32500/48549], Loss: 2.4347\n",
      "Epoch [1/1], Step [32510/48549], Loss: 2.5274\n",
      "Epoch [1/1], Step [32520/48549], Loss: 2.4162\n",
      "Epoch [1/1], Step [32530/48549], Loss: 2.4358\n",
      "Epoch [1/1], Step [32540/48549], Loss: 2.4866\n",
      "Epoch [1/1], Step [32550/48549], Loss: 2.3810\n",
      "Epoch [1/1], Step [32560/48549], Loss: 2.4450\n",
      "Epoch [1/1], Step [32570/48549], Loss: 2.4666\n",
      "Epoch [1/1], Step [32580/48549], Loss: 2.4956\n",
      "Epoch [1/1], Step [32590/48549], Loss: 2.4626\n",
      "Epoch [1/1], Step [32600/48549], Loss: 2.4475\n",
      "Epoch [1/1], Step [32610/48549], Loss: 2.4495\n",
      "Epoch [1/1], Step [32620/48549], Loss: 2.5281\n",
      "Epoch [1/1], Step [32630/48549], Loss: 2.4270\n",
      "Epoch [1/1], Step [32640/48549], Loss: 2.4431\n",
      "Epoch [1/1], Step [32650/48549], Loss: 2.4410\n",
      "Epoch [1/1], Step [32660/48549], Loss: 2.4352\n",
      "Epoch [1/1], Step [32670/48549], Loss: 2.4700\n",
      "Epoch [1/1], Step [32680/48549], Loss: 2.3654\n",
      "Epoch [1/1], Step [32690/48549], Loss: 2.4159\n",
      "Epoch [1/1], Step [32700/48549], Loss: 2.4797\n",
      "Epoch [1/1], Step [32710/48549], Loss: 2.4007\n",
      "Epoch [1/1], Step [32720/48549], Loss: 2.4523\n",
      "Epoch [1/1], Step [32730/48549], Loss: 2.4422\n",
      "Epoch [1/1], Step [32740/48549], Loss: 2.4410\n",
      "Epoch [1/1], Step [32750/48549], Loss: 2.4373\n",
      "Epoch [1/1], Step [32760/48549], Loss: 2.4979\n",
      "Epoch [1/1], Step [32770/48549], Loss: 2.4117\n",
      "Epoch [1/1], Step [32780/48549], Loss: 2.5617\n",
      "Epoch [1/1], Step [32790/48549], Loss: 2.4491\n",
      "Epoch [1/1], Step [32800/48549], Loss: 2.4274\n",
      "Epoch [1/1], Step [32810/48549], Loss: 2.4381\n",
      "Epoch [1/1], Step [32820/48549], Loss: 2.4763\n",
      "Epoch [1/1], Step [32830/48549], Loss: 2.4254\n",
      "Epoch [1/1], Step [32840/48549], Loss: 2.4330\n",
      "Epoch [1/1], Step [32850/48549], Loss: 2.4308\n",
      "Epoch [1/1], Step [32860/48549], Loss: 2.4867\n",
      "Epoch [1/1], Step [32870/48549], Loss: 2.4156\n",
      "Epoch [1/1], Step [32880/48549], Loss: 2.4742\n",
      "Epoch [1/1], Step [32890/48549], Loss: 2.3965\n",
      "Epoch [1/1], Step [32900/48549], Loss: 2.4298\n",
      "Epoch [1/1], Step [32910/48549], Loss: 2.4925\n",
      "Epoch [1/1], Step [32920/48549], Loss: 2.4370\n",
      "Epoch [1/1], Step [32930/48549], Loss: 2.4385\n",
      "Epoch [1/1], Step [32940/48549], Loss: 2.4222\n",
      "Epoch [1/1], Step [32950/48549], Loss: 2.4215\n",
      "Epoch [1/1], Step [32960/48549], Loss: 2.3741\n",
      "Epoch [1/1], Step [32970/48549], Loss: 2.4184\n",
      "Epoch [1/1], Step [32980/48549], Loss: 2.4691\n",
      "Epoch [1/1], Step [32990/48549], Loss: 2.4411\n",
      "Epoch [1/1], Step [33000/48549], Loss: 2.4231\n",
      "Epoch [1/1], Step [33010/48549], Loss: 2.4423\n",
      "Epoch [1/1], Step [33020/48549], Loss: 2.4996\n",
      "Epoch [1/1], Step [33030/48549], Loss: 2.4225\n",
      "Epoch [1/1], Step [33040/48549], Loss: 2.4579\n",
      "Epoch [1/1], Step [33050/48549], Loss: 2.4470\n",
      "Epoch [1/1], Step [33060/48549], Loss: 2.4148\n",
      "Epoch [1/1], Step [33070/48549], Loss: 2.4100\n",
      "Epoch [1/1], Step [33080/48549], Loss: 2.4340\n",
      "Epoch [1/1], Step [33090/48549], Loss: 2.4141\n",
      "Epoch [1/1], Step [33100/48549], Loss: 2.4712\n",
      "Epoch [1/1], Step [33110/48549], Loss: 2.4280\n",
      "Epoch [1/1], Step [33120/48549], Loss: 2.3776\n",
      "Epoch [1/1], Step [33130/48549], Loss: 2.3990\n",
      "Epoch [1/1], Step [33140/48549], Loss: 2.4028\n",
      "Epoch [1/1], Step [33150/48549], Loss: 2.4558\n",
      "Epoch [1/1], Step [33160/48549], Loss: 2.4710\n",
      "Epoch [1/1], Step [33170/48549], Loss: 2.4475\n",
      "Epoch [1/1], Step [33180/48549], Loss: 2.4662\n",
      "Epoch [1/1], Step [33190/48549], Loss: 2.3981\n",
      "Epoch [1/1], Step [33200/48549], Loss: 2.4079\n",
      "Epoch [1/1], Step [33210/48549], Loss: 2.4431\n",
      "Epoch [1/1], Step [33220/48549], Loss: 2.4711\n",
      "Epoch [1/1], Step [33230/48549], Loss: 2.4726\n",
      "Epoch [1/1], Step [33240/48549], Loss: 2.4357\n",
      "Epoch [1/1], Step [33250/48549], Loss: 2.4284\n",
      "Epoch [1/1], Step [33260/48549], Loss: 2.4646\n",
      "Epoch [1/1], Step [33270/48549], Loss: 2.4281\n",
      "Epoch [1/1], Step [33280/48549], Loss: 2.4444\n",
      "Epoch [1/1], Step [33290/48549], Loss: 2.4104\n",
      "Epoch [1/1], Step [33300/48549], Loss: 2.4722\n",
      "Epoch [1/1], Step [33310/48549], Loss: 2.4582\n",
      "Epoch [1/1], Step [33320/48549], Loss: 2.4716\n",
      "Epoch [1/1], Step [33330/48549], Loss: 2.4026\n",
      "Epoch [1/1], Step [33340/48549], Loss: 2.4700\n",
      "Epoch [1/1], Step [33350/48549], Loss: 2.4555\n",
      "Epoch [1/1], Step [33360/48549], Loss: 2.4971\n",
      "Epoch [1/1], Step [33370/48549], Loss: 2.4203\n",
      "Epoch [1/1], Step [33380/48549], Loss: 2.4342\n",
      "Epoch [1/1], Step [33390/48549], Loss: 2.4709\n",
      "Epoch [1/1], Step [33400/48549], Loss: 2.4607\n",
      "Epoch [1/1], Step [33410/48549], Loss: 2.4689\n",
      "Epoch [1/1], Step [33420/48549], Loss: 2.4743\n",
      "Epoch [1/1], Step [33430/48549], Loss: 2.4125\n",
      "Epoch [1/1], Step [33440/48549], Loss: 2.4166\n",
      "Epoch [1/1], Step [33450/48549], Loss: 2.4272\n",
      "Epoch [1/1], Step [33460/48549], Loss: 2.4769\n",
      "Epoch [1/1], Step [33470/48549], Loss: 2.4286\n",
      "Epoch [1/1], Step [33480/48549], Loss: 2.5321\n",
      "Epoch [1/1], Step [33490/48549], Loss: 2.4506\n",
      "Epoch [1/1], Step [33500/48549], Loss: 2.4061\n",
      "Epoch [1/1], Step [33510/48549], Loss: 2.4393\n",
      "Epoch [1/1], Step [33520/48549], Loss: 2.4286\n",
      "Epoch [1/1], Step [33530/48549], Loss: 2.4710\n",
      "Epoch [1/1], Step [33540/48549], Loss: 2.5035\n",
      "Epoch [1/1], Step [33550/48549], Loss: 2.4649\n",
      "Epoch [1/1], Step [33560/48549], Loss: 2.4228\n",
      "Epoch [1/1], Step [33570/48549], Loss: 2.4439\n",
      "Epoch [1/1], Step [33580/48549], Loss: 2.4294\n",
      "Epoch [1/1], Step [33590/48549], Loss: 2.4768\n",
      "Epoch [1/1], Step [33600/48549], Loss: 2.4431\n",
      "Epoch [1/1], Step [33610/48549], Loss: 2.4598\n",
      "Epoch [1/1], Step [33620/48549], Loss: 2.5415\n",
      "Epoch [1/1], Step [33630/48549], Loss: 2.3916\n",
      "Epoch [1/1], Step [33640/48549], Loss: 2.4787\n",
      "Epoch [1/1], Step [33650/48549], Loss: 2.4414\n",
      "Epoch [1/1], Step [33660/48549], Loss: 2.4022\n",
      "Epoch [1/1], Step [33670/48549], Loss: 2.4425\n",
      "Epoch [1/1], Step [33680/48549], Loss: 2.4440\n",
      "Epoch [1/1], Step [33690/48549], Loss: 2.4719\n",
      "Epoch [1/1], Step [33700/48549], Loss: 2.3992\n",
      "Epoch [1/1], Step [33710/48549], Loss: 2.4493\n",
      "Epoch [1/1], Step [33720/48549], Loss: 2.4388\n",
      "Epoch [1/1], Step [33730/48549], Loss: 2.4774\n",
      "Epoch [1/1], Step [33740/48549], Loss: 2.4948\n",
      "Epoch [1/1], Step [33750/48549], Loss: 2.4401\n",
      "Epoch [1/1], Step [33760/48549], Loss: 2.4471\n",
      "Epoch [1/1], Step [33770/48549], Loss: 2.3811\n",
      "Epoch [1/1], Step [33780/48549], Loss: 2.4392\n",
      "Epoch [1/1], Step [33790/48549], Loss: 2.4587\n",
      "Epoch [1/1], Step [33800/48549], Loss: 2.4384\n",
      "Epoch [1/1], Step [33810/48549], Loss: 2.4505\n",
      "Epoch [1/1], Step [33820/48549], Loss: 2.4194\n",
      "Epoch [1/1], Step [33830/48549], Loss: 2.4253\n",
      "Epoch [1/1], Step [33840/48549], Loss: 2.4328\n",
      "Epoch [1/1], Step [33850/48549], Loss: 2.4241\n",
      "Epoch [1/1], Step [33860/48549], Loss: 2.4194\n",
      "Epoch [1/1], Step [33870/48549], Loss: 2.4217\n",
      "Epoch [1/1], Step [33880/48549], Loss: 2.4346\n",
      "Epoch [1/1], Step [33890/48549], Loss: 2.4604\n",
      "Epoch [1/1], Step [33900/48549], Loss: 2.4768\n",
      "Epoch [1/1], Step [33910/48549], Loss: 2.5709\n",
      "Epoch [1/1], Step [33920/48549], Loss: 2.4487\n",
      "Epoch [1/1], Step [33930/48549], Loss: 2.4279\n",
      "Epoch [1/1], Step [33940/48549], Loss: 2.4527\n",
      "Epoch [1/1], Step [33950/48549], Loss: 2.4412\n",
      "Epoch [1/1], Step [33960/48549], Loss: 2.4419\n",
      "Epoch [1/1], Step [33970/48549], Loss: 2.4407\n",
      "Epoch [1/1], Step [33980/48549], Loss: 2.4532\n",
      "Epoch [1/1], Step [33990/48549], Loss: 2.4127\n",
      "Epoch [1/1], Step [34000/48549], Loss: 2.3524\n",
      "Epoch [1/1], Step [34010/48549], Loss: 2.4285\n",
      "Epoch [1/1], Step [34020/48549], Loss: 2.4626\n",
      "Epoch [1/1], Step [34030/48549], Loss: 2.3628\n",
      "Epoch [1/1], Step [34040/48549], Loss: 2.4396\n",
      "Epoch [1/1], Step [34050/48549], Loss: 2.4651\n",
      "Epoch [1/1], Step [34060/48549], Loss: 2.4841\n",
      "Epoch [1/1], Step [34070/48549], Loss: 2.5983\n",
      "Epoch [1/1], Step [34080/48549], Loss: 2.4128\n",
      "Epoch [1/1], Step [34090/48549], Loss: 2.4388\n",
      "Epoch [1/1], Step [34100/48549], Loss: 2.4134\n",
      "Epoch [1/1], Step [34110/48549], Loss: 2.4230\n",
      "Epoch [1/1], Step [34120/48549], Loss: 2.4540\n",
      "Epoch [1/1], Step [34130/48549], Loss: 2.4069\n",
      "Epoch [1/1], Step [34140/48549], Loss: 2.4583\n",
      "Epoch [1/1], Step [34150/48549], Loss: 2.4753\n",
      "Epoch [1/1], Step [34160/48549], Loss: 2.4549\n",
      "Epoch [1/1], Step [34170/48549], Loss: 2.3946\n",
      "Epoch [1/1], Step [34180/48549], Loss: 2.4693\n",
      "Epoch [1/1], Step [34190/48549], Loss: 2.4606\n",
      "Epoch [1/1], Step [34200/48549], Loss: 2.4940\n",
      "Epoch [1/1], Step [34210/48549], Loss: 2.3900\n",
      "Epoch [1/1], Step [34220/48549], Loss: 2.3904\n",
      "Epoch [1/1], Step [34230/48549], Loss: 2.5199\n",
      "Epoch [1/1], Step [34240/48549], Loss: 2.4119\n",
      "Epoch [1/1], Step [34250/48549], Loss: 2.4739\n",
      "Epoch [1/1], Step [34260/48549], Loss: 2.3957\n",
      "Epoch [1/1], Step [34270/48549], Loss: 2.4028\n",
      "Epoch [1/1], Step [34280/48549], Loss: 2.4819\n",
      "Epoch [1/1], Step [34290/48549], Loss: 2.4467\n",
      "Epoch [1/1], Step [34300/48549], Loss: 2.4677\n",
      "Epoch [1/1], Step [34310/48549], Loss: 2.4929\n",
      "Epoch [1/1], Step [34320/48549], Loss: 2.4113\n",
      "Epoch [1/1], Step [34330/48549], Loss: 2.4432\n",
      "Epoch [1/1], Step [34340/48549], Loss: 2.4290\n",
      "Epoch [1/1], Step [34350/48549], Loss: 2.5061\n",
      "Epoch [1/1], Step [34360/48549], Loss: 2.4399\n",
      "Epoch [1/1], Step [34370/48549], Loss: 2.4505\n",
      "Epoch [1/1], Step [34380/48549], Loss: 2.3825\n",
      "Epoch [1/1], Step [34390/48549], Loss: 2.4474\n",
      "Epoch [1/1], Step [34400/48549], Loss: 2.4249\n",
      "Epoch [1/1], Step [34410/48549], Loss: 2.4604\n",
      "Epoch [1/1], Step [34420/48549], Loss: 2.4202\n",
      "Epoch [1/1], Step [34430/48549], Loss: 2.4897\n",
      "Epoch [1/1], Step [34440/48549], Loss: 2.5244\n",
      "Epoch [1/1], Step [34450/48549], Loss: 2.4537\n",
      "Epoch [1/1], Step [34460/48549], Loss: 2.4608\n",
      "Epoch [1/1], Step [34470/48549], Loss: 2.4819\n",
      "Epoch [1/1], Step [34480/48549], Loss: 2.4689\n",
      "Epoch [1/1], Step [34490/48549], Loss: 2.4397\n",
      "Epoch [1/1], Step [34500/48549], Loss: 2.4811\n",
      "Epoch [1/1], Step [34510/48549], Loss: 2.4426\n",
      "Epoch [1/1], Step [34520/48549], Loss: 2.5804\n",
      "Epoch [1/1], Step [34530/48549], Loss: 2.4733\n",
      "Epoch [1/1], Step [34540/48549], Loss: 2.3796\n",
      "Epoch [1/1], Step [34550/48549], Loss: 2.4523\n",
      "Epoch [1/1], Step [34560/48549], Loss: 2.4562\n",
      "Epoch [1/1], Step [34570/48549], Loss: 2.4228\n",
      "Epoch [1/1], Step [34580/48549], Loss: 2.4410\n",
      "Epoch [1/1], Step [34590/48549], Loss: 2.4143\n",
      "Epoch [1/1], Step [34600/48549], Loss: 2.5018\n",
      "Epoch [1/1], Step [34610/48549], Loss: 2.3755\n",
      "Epoch [1/1], Step [34620/48549], Loss: 2.4881\n",
      "Epoch [1/1], Step [34630/48549], Loss: 2.4305\n",
      "Epoch [1/1], Step [34640/48549], Loss: 2.3793\n",
      "Epoch [1/1], Step [34650/48549], Loss: 2.3940\n",
      "Epoch [1/1], Step [34660/48549], Loss: 2.4629\n",
      "Epoch [1/1], Step [34670/48549], Loss: 2.4492\n",
      "Epoch [1/1], Step [34680/48549], Loss: 2.3929\n",
      "Epoch [1/1], Step [34690/48549], Loss: 2.4713\n",
      "Epoch [1/1], Step [34700/48549], Loss: 2.5198\n",
      "Epoch [1/1], Step [34710/48549], Loss: 2.4002\n",
      "Epoch [1/1], Step [34720/48549], Loss: 2.4618\n",
      "Epoch [1/1], Step [34730/48549], Loss: 2.5064\n",
      "Epoch [1/1], Step [34740/48549], Loss: 2.4489\n",
      "Epoch [1/1], Step [34750/48549], Loss: 2.4069\n",
      "Epoch [1/1], Step [34760/48549], Loss: 2.4038\n",
      "Epoch [1/1], Step [34770/48549], Loss: 2.4392\n",
      "Epoch [1/1], Step [34780/48549], Loss: 2.4800\n",
      "Epoch [1/1], Step [34790/48549], Loss: 2.5509\n",
      "Epoch [1/1], Step [34800/48549], Loss: 2.4212\n",
      "Epoch [1/1], Step [34810/48549], Loss: 2.4463\n",
      "Epoch [1/1], Step [34820/48549], Loss: 2.4361\n",
      "Epoch [1/1], Step [34830/48549], Loss: 2.4166\n",
      "Epoch [1/1], Step [34840/48549], Loss: 2.4742\n",
      "Epoch [1/1], Step [34850/48549], Loss: 2.4475\n",
      "Epoch [1/1], Step [34860/48549], Loss: 2.4777\n",
      "Epoch [1/1], Step [34870/48549], Loss: 2.4367\n",
      "Epoch [1/1], Step [34880/48549], Loss: 2.4183\n",
      "Epoch [1/1], Step [34890/48549], Loss: 2.4696\n",
      "Epoch [1/1], Step [34900/48549], Loss: 2.3893\n",
      "Epoch [1/1], Step [34910/48549], Loss: 2.4812\n",
      "Epoch [1/1], Step [34920/48549], Loss: 2.4978\n",
      "Epoch [1/1], Step [34930/48549], Loss: 2.5401\n",
      "Epoch [1/1], Step [34940/48549], Loss: 2.5059\n",
      "Epoch [1/1], Step [34950/48549], Loss: 2.4389\n",
      "Epoch [1/1], Step [34960/48549], Loss: 2.3920\n",
      "Epoch [1/1], Step [34970/48549], Loss: 2.4413\n",
      "Epoch [1/1], Step [34980/48549], Loss: 2.4232\n",
      "Epoch [1/1], Step [34990/48549], Loss: 2.4795\n",
      "Epoch [1/1], Step [35000/48549], Loss: 2.4036\n",
      "Epoch [1/1], Step [35010/48549], Loss: 2.4982\n",
      "Epoch [1/1], Step [35020/48549], Loss: 2.4673\n",
      "Epoch [1/1], Step [35030/48549], Loss: 2.4912\n",
      "Epoch [1/1], Step [35040/48549], Loss: 2.4485\n",
      "Epoch [1/1], Step [35050/48549], Loss: 2.4063\n",
      "Epoch [1/1], Step [35060/48549], Loss: 2.4224\n",
      "Epoch [1/1], Step [35070/48549], Loss: 2.5452\n",
      "Epoch [1/1], Step [35080/48549], Loss: 2.4290\n",
      "Epoch [1/1], Step [35090/48549], Loss: 2.4146\n",
      "Epoch [1/1], Step [35100/48549], Loss: 2.5084\n",
      "Epoch [1/1], Step [35110/48549], Loss: 2.5085\n",
      "Epoch [1/1], Step [35120/48549], Loss: 2.4667\n",
      "Epoch [1/1], Step [35130/48549], Loss: 2.4239\n",
      "Epoch [1/1], Step [35140/48549], Loss: 2.4299\n",
      "Epoch [1/1], Step [35150/48549], Loss: 2.4425\n",
      "Epoch [1/1], Step [35160/48549], Loss: 2.4294\n",
      "Epoch [1/1], Step [35170/48549], Loss: 2.4536\n",
      "Epoch [1/1], Step [35180/48549], Loss: 2.4914\n",
      "Epoch [1/1], Step [35190/48549], Loss: 2.4453\n",
      "Epoch [1/1], Step [35200/48549], Loss: 2.4514\n",
      "Epoch [1/1], Step [35210/48549], Loss: 2.6014\n",
      "Epoch [1/1], Step [35220/48549], Loss: 2.4130\n",
      "Epoch [1/1], Step [35230/48549], Loss: 2.4864\n",
      "Epoch [1/1], Step [35240/48549], Loss: 2.4600\n",
      "Epoch [1/1], Step [35250/48549], Loss: 2.4188\n",
      "Epoch [1/1], Step [35260/48549], Loss: 2.4437\n",
      "Epoch [1/1], Step [35270/48549], Loss: 2.4465\n",
      "Epoch [1/1], Step [35280/48549], Loss: 2.4339\n",
      "Epoch [1/1], Step [35290/48549], Loss: 2.4280\n",
      "Epoch [1/1], Step [35300/48549], Loss: 2.5321\n",
      "Epoch [1/1], Step [35310/48549], Loss: 2.4668\n",
      "Epoch [1/1], Step [35320/48549], Loss: 2.4286\n",
      "Epoch [1/1], Step [35330/48549], Loss: 2.4188\n",
      "Epoch [1/1], Step [35340/48549], Loss: 2.4654\n",
      "Epoch [1/1], Step [35350/48549], Loss: 2.4255\n",
      "Epoch [1/1], Step [35360/48549], Loss: 2.4174\n",
      "Epoch [1/1], Step [35370/48549], Loss: 2.4478\n",
      "Epoch [1/1], Step [35380/48549], Loss: 2.4555\n",
      "Epoch [1/1], Step [35390/48549], Loss: 2.4274\n",
      "Epoch [1/1], Step [35400/48549], Loss: 2.4068\n",
      "Epoch [1/1], Step [35410/48549], Loss: 2.3811\n",
      "Epoch [1/1], Step [35420/48549], Loss: 2.3997\n",
      "Epoch [1/1], Step [35430/48549], Loss: 2.4498\n",
      "Epoch [1/1], Step [35440/48549], Loss: 2.4673\n",
      "Epoch [1/1], Step [35450/48549], Loss: 2.4400\n",
      "Epoch [1/1], Step [35460/48549], Loss: 2.3885\n",
      "Epoch [1/1], Step [35470/48549], Loss: 2.3612\n",
      "Epoch [1/1], Step [35480/48549], Loss: 2.4300\n",
      "Epoch [1/1], Step [35490/48549], Loss: 2.4134\n",
      "Epoch [1/1], Step [35500/48549], Loss: 2.4325\n",
      "Epoch [1/1], Step [35510/48549], Loss: 2.4746\n",
      "Epoch [1/1], Step [35520/48549], Loss: 2.4669\n",
      "Epoch [1/1], Step [35530/48549], Loss: 2.4282\n",
      "Epoch [1/1], Step [35540/48549], Loss: 2.4669\n",
      "Epoch [1/1], Step [35550/48549], Loss: 2.4329\n",
      "Epoch [1/1], Step [35560/48549], Loss: 2.4498\n",
      "Epoch [1/1], Step [35570/48549], Loss: 2.4905\n",
      "Epoch [1/1], Step [35580/48549], Loss: 2.4217\n",
      "Epoch [1/1], Step [35590/48549], Loss: 2.3791\n",
      "Epoch [1/1], Step [35600/48549], Loss: 2.5631\n",
      "Epoch [1/1], Step [35610/48549], Loss: 2.4022\n",
      "Epoch [1/1], Step [35620/48549], Loss: 2.4130\n",
      "Epoch [1/1], Step [35630/48549], Loss: 2.4352\n",
      "Epoch [1/1], Step [35640/48549], Loss: 2.4103\n",
      "Epoch [1/1], Step [35650/48549], Loss: 2.4509\n",
      "Epoch [1/1], Step [35660/48549], Loss: 2.4808\n",
      "Epoch [1/1], Step [35670/48549], Loss: 2.4737\n",
      "Epoch [1/1], Step [35680/48549], Loss: 2.5827\n",
      "Epoch [1/1], Step [35690/48549], Loss: 2.4716\n",
      "Epoch [1/1], Step [35700/48549], Loss: 2.4020\n",
      "Epoch [1/1], Step [35710/48549], Loss: 2.4211\n",
      "Epoch [1/1], Step [35720/48549], Loss: 2.4440\n",
      "Epoch [1/1], Step [35730/48549], Loss: 2.4363\n",
      "Epoch [1/1], Step [35740/48549], Loss: 2.3946\n",
      "Epoch [1/1], Step [35750/48549], Loss: 2.4342\n",
      "Epoch [1/1], Step [35760/48549], Loss: 2.4264\n",
      "Epoch [1/1], Step [35770/48549], Loss: 2.3940\n",
      "Epoch [1/1], Step [35780/48549], Loss: 2.4346\n",
      "Epoch [1/1], Step [35790/48549], Loss: 2.5101\n",
      "Epoch [1/1], Step [35800/48549], Loss: 2.4140\n",
      "Epoch [1/1], Step [35810/48549], Loss: 2.4901\n",
      "Epoch [1/1], Step [35820/48549], Loss: 2.4164\n",
      "Epoch [1/1], Step [35830/48549], Loss: 2.3988\n",
      "Epoch [1/1], Step [35840/48549], Loss: 2.4385\n",
      "Epoch [1/1], Step [35850/48549], Loss: 2.4088\n",
      "Epoch [1/1], Step [35860/48549], Loss: 2.4027\n",
      "Epoch [1/1], Step [35870/48549], Loss: 2.4297\n",
      "Epoch [1/1], Step [35880/48549], Loss: 2.4742\n",
      "Epoch [1/1], Step [35890/48549], Loss: 2.4378\n",
      "Epoch [1/1], Step [35900/48549], Loss: 2.4271\n",
      "Epoch [1/1], Step [35910/48549], Loss: 2.4412\n",
      "Epoch [1/1], Step [35920/48549], Loss: 2.4452\n",
      "Epoch [1/1], Step [35930/48549], Loss: 2.5477\n",
      "Epoch [1/1], Step [35940/48549], Loss: 2.4145\n",
      "Epoch [1/1], Step [35950/48549], Loss: 2.4637\n",
      "Epoch [1/1], Step [35960/48549], Loss: 2.4692\n",
      "Epoch [1/1], Step [35970/48549], Loss: 2.4346\n",
      "Epoch [1/1], Step [35980/48549], Loss: 2.4001\n",
      "Epoch [1/1], Step [35990/48549], Loss: 2.4323\n",
      "Epoch [1/1], Step [36000/48549], Loss: 2.4861\n",
      "Epoch [1/1], Step [36010/48549], Loss: 2.4954\n",
      "Epoch [1/1], Step [36020/48549], Loss: 2.4635\n",
      "Epoch [1/1], Step [36030/48549], Loss: 2.5010\n",
      "Epoch [1/1], Step [36040/48549], Loss: 2.4444\n",
      "Epoch [1/1], Step [36050/48549], Loss: 2.4859\n",
      "Epoch [1/1], Step [36060/48549], Loss: 2.4343\n",
      "Epoch [1/1], Step [36070/48549], Loss: 2.4253\n",
      "Epoch [1/1], Step [36080/48549], Loss: 2.3952\n",
      "Epoch [1/1], Step [36090/48549], Loss: 2.4515\n",
      "Epoch [1/1], Step [36100/48549], Loss: 2.4720\n",
      "Epoch [1/1], Step [36110/48549], Loss: 2.5008\n",
      "Epoch [1/1], Step [36120/48549], Loss: 2.4856\n",
      "Epoch [1/1], Step [36130/48549], Loss: 2.4317\n",
      "Epoch [1/1], Step [36140/48549], Loss: 2.4875\n",
      "Epoch [1/1], Step [36150/48549], Loss: 2.3968\n",
      "Epoch [1/1], Step [36160/48549], Loss: 2.4667\n",
      "Epoch [1/1], Step [36170/48549], Loss: 2.4415\n",
      "Epoch [1/1], Step [36180/48549], Loss: 2.4417\n",
      "Epoch [1/1], Step [36190/48549], Loss: 2.4003\n",
      "Epoch [1/1], Step [36200/48549], Loss: 2.4645\n",
      "Epoch [1/1], Step [36210/48549], Loss: 2.4426\n",
      "Epoch [1/1], Step [36220/48549], Loss: 2.3868\n",
      "Epoch [1/1], Step [36230/48549], Loss: 2.4061\n",
      "Epoch [1/1], Step [36240/48549], Loss: 2.4626\n",
      "Epoch [1/1], Step [36250/48549], Loss: 2.4454\n",
      "Epoch [1/1], Step [36260/48549], Loss: 2.4565\n",
      "Epoch [1/1], Step [36270/48549], Loss: 2.4369\n",
      "Epoch [1/1], Step [36280/48549], Loss: 2.4248\n",
      "Epoch [1/1], Step [36290/48549], Loss: 2.4801\n",
      "Epoch [1/1], Step [36300/48549], Loss: 2.4791\n",
      "Epoch [1/1], Step [36310/48549], Loss: 2.4581\n",
      "Epoch [1/1], Step [36320/48549], Loss: 2.4236\n",
      "Epoch [1/1], Step [36330/48549], Loss: 2.4606\n",
      "Epoch [1/1], Step [36340/48549], Loss: 2.4719\n",
      "Epoch [1/1], Step [36350/48549], Loss: 2.4453\n",
      "Epoch [1/1], Step [36360/48549], Loss: 2.4878\n",
      "Epoch [1/1], Step [36370/48549], Loss: 2.4316\n",
      "Epoch [1/1], Step [36380/48549], Loss: 2.4596\n",
      "Epoch [1/1], Step [36390/48549], Loss: 2.4451\n",
      "Epoch [1/1], Step [36400/48549], Loss: 2.4601\n",
      "Epoch [1/1], Step [36410/48549], Loss: 2.4855\n",
      "Epoch [1/1], Step [36420/48549], Loss: 2.4208\n",
      "Epoch [1/1], Step [36430/48549], Loss: 2.4795\n",
      "Epoch [1/1], Step [36440/48549], Loss: 2.4136\n",
      "Epoch [1/1], Step [36450/48549], Loss: 2.5713\n",
      "Epoch [1/1], Step [36460/48549], Loss: 2.5252\n",
      "Epoch [1/1], Step [36470/48549], Loss: 2.4490\n",
      "Epoch [1/1], Step [36480/48549], Loss: 2.3857\n",
      "Epoch [1/1], Step [36490/48549], Loss: 2.4488\n",
      "Epoch [1/1], Step [36500/48549], Loss: 2.5115\n",
      "Epoch [1/1], Step [36510/48549], Loss: 2.4390\n",
      "Epoch [1/1], Step [36520/48549], Loss: 2.4666\n",
      "Epoch [1/1], Step [36530/48549], Loss: 2.4200\n",
      "Epoch [1/1], Step [36540/48549], Loss: 2.4359\n",
      "Epoch [1/1], Step [36550/48549], Loss: 2.4485\n",
      "Epoch [1/1], Step [36560/48549], Loss: 2.4400\n",
      "Epoch [1/1], Step [36570/48549], Loss: 2.4139\n",
      "Epoch [1/1], Step [36580/48549], Loss: 2.3781\n",
      "Epoch [1/1], Step [36590/48549], Loss: 2.3690\n",
      "Epoch [1/1], Step [36600/48549], Loss: 2.4859\n",
      "Epoch [1/1], Step [36610/48549], Loss: 2.4644\n",
      "Epoch [1/1], Step [36620/48549], Loss: 2.4405\n",
      "Epoch [1/1], Step [36630/48549], Loss: 2.4288\n",
      "Epoch [1/1], Step [36640/48549], Loss: 2.4856\n",
      "Epoch [1/1], Step [36650/48549], Loss: 2.4698\n",
      "Epoch [1/1], Step [36660/48549], Loss: 2.4349\n",
      "Epoch [1/1], Step [36670/48549], Loss: 2.4451\n",
      "Epoch [1/1], Step [36680/48549], Loss: 2.4398\n",
      "Epoch [1/1], Step [36690/48549], Loss: 2.4333\n",
      "Epoch [1/1], Step [36700/48549], Loss: 2.4090\n",
      "Epoch [1/1], Step [36710/48549], Loss: 2.3968\n",
      "Epoch [1/1], Step [36720/48549], Loss: 2.4508\n",
      "Epoch [1/1], Step [36730/48549], Loss: 2.4284\n",
      "Epoch [1/1], Step [36740/48549], Loss: 2.4495\n",
      "Epoch [1/1], Step [36750/48549], Loss: 2.4697\n",
      "Epoch [1/1], Step [36760/48549], Loss: 2.4155\n",
      "Epoch [1/1], Step [36770/48549], Loss: 2.3798\n",
      "Epoch [1/1], Step [36780/48549], Loss: 2.4700\n",
      "Epoch [1/1], Step [36790/48549], Loss: 2.4396\n",
      "Epoch [1/1], Step [36800/48549], Loss: 2.6332\n",
      "Epoch [1/1], Step [36810/48549], Loss: 2.4309\n",
      "Epoch [1/1], Step [36820/48549], Loss: 2.4161\n",
      "Epoch [1/1], Step [36830/48549], Loss: 2.4667\n",
      "Epoch [1/1], Step [36840/48549], Loss: 2.4628\n",
      "Epoch [1/1], Step [36850/48549], Loss: 2.5375\n",
      "Epoch [1/1], Step [36860/48549], Loss: 2.4787\n",
      "Epoch [1/1], Step [36870/48549], Loss: 2.4347\n",
      "Epoch [1/1], Step [36880/48549], Loss: 2.4633\n",
      "Epoch [1/1], Step [36890/48549], Loss: 2.4499\n",
      "Epoch [1/1], Step [36900/48549], Loss: 2.4319\n",
      "Epoch [1/1], Step [36910/48549], Loss: 2.4437\n",
      "Epoch [1/1], Step [36920/48549], Loss: 2.4597\n",
      "Epoch [1/1], Step [36930/48549], Loss: 2.3783\n",
      "Epoch [1/1], Step [36940/48549], Loss: 2.4577\n",
      "Epoch [1/1], Step [36950/48549], Loss: 2.4382\n",
      "Epoch [1/1], Step [36960/48549], Loss: 2.5211\n",
      "Epoch [1/1], Step [36970/48549], Loss: 2.4432\n",
      "Epoch [1/1], Step [36980/48549], Loss: 2.5108\n",
      "Epoch [1/1], Step [36990/48549], Loss: 2.4401\n",
      "Epoch [1/1], Step [37000/48549], Loss: 2.4883\n",
      "Epoch [1/1], Step [37010/48549], Loss: 2.3918\n",
      "Epoch [1/1], Step [37020/48549], Loss: 2.3897\n",
      "Epoch [1/1], Step [37030/48549], Loss: 2.4379\n",
      "Epoch [1/1], Step [37040/48549], Loss: 2.4957\n",
      "Epoch [1/1], Step [37050/48549], Loss: 2.4223\n",
      "Epoch [1/1], Step [37060/48549], Loss: 2.4347\n",
      "Epoch [1/1], Step [37070/48549], Loss: 2.4659\n",
      "Epoch [1/1], Step [37080/48549], Loss: 2.4747\n",
      "Epoch [1/1], Step [37090/48549], Loss: 2.4524\n",
      "Epoch [1/1], Step [37100/48549], Loss: 2.5287\n",
      "Epoch [1/1], Step [37110/48549], Loss: 2.4403\n",
      "Epoch [1/1], Step [37120/48549], Loss: 2.5730\n",
      "Epoch [1/1], Step [37130/48549], Loss: 2.4183\n",
      "Epoch [1/1], Step [37140/48549], Loss: 2.4449\n",
      "Epoch [1/1], Step [37150/48549], Loss: 2.4773\n",
      "Epoch [1/1], Step [37160/48549], Loss: 2.4611\n",
      "Epoch [1/1], Step [37170/48549], Loss: 2.4379\n",
      "Epoch [1/1], Step [37180/48549], Loss: 2.4600\n",
      "Epoch [1/1], Step [37190/48549], Loss: 2.4400\n",
      "Epoch [1/1], Step [37200/48549], Loss: 2.4078\n",
      "Epoch [1/1], Step [37210/48549], Loss: 2.4637\n",
      "Epoch [1/1], Step [37220/48549], Loss: 2.4111\n",
      "Epoch [1/1], Step [37230/48549], Loss: 2.6133\n",
      "Epoch [1/1], Step [37240/48549], Loss: 2.4582\n",
      "Epoch [1/1], Step [37250/48549], Loss: 2.4387\n",
      "Epoch [1/1], Step [37260/48549], Loss: 2.4116\n",
      "Epoch [1/1], Step [37270/48549], Loss: 2.4511\n",
      "Epoch [1/1], Step [37280/48549], Loss: 2.4280\n",
      "Epoch [1/1], Step [37290/48549], Loss: 2.4720\n",
      "Epoch [1/1], Step [37300/48549], Loss: 2.4313\n",
      "Epoch [1/1], Step [37310/48549], Loss: 2.4452\n",
      "Epoch [1/1], Step [37320/48549], Loss: 2.5156\n",
      "Epoch [1/1], Step [37330/48549], Loss: 2.4313\n",
      "Epoch [1/1], Step [37340/48549], Loss: 2.4393\n",
      "Epoch [1/1], Step [37350/48549], Loss: 2.4609\n",
      "Epoch [1/1], Step [37360/48549], Loss: 2.4627\n",
      "Epoch [1/1], Step [37370/48549], Loss: 2.4275\n",
      "Epoch [1/1], Step [37380/48549], Loss: 2.4096\n",
      "Epoch [1/1], Step [37390/48549], Loss: 2.4345\n",
      "Epoch [1/1], Step [37400/48549], Loss: 2.4652\n",
      "Epoch [1/1], Step [37410/48549], Loss: 2.4188\n",
      "Epoch [1/1], Step [37420/48549], Loss: 2.4283\n",
      "Epoch [1/1], Step [37430/48549], Loss: 2.4398\n",
      "Epoch [1/1], Step [37440/48549], Loss: 2.4019\n",
      "Epoch [1/1], Step [37450/48549], Loss: 2.4561\n",
      "Epoch [1/1], Step [37460/48549], Loss: 2.4979\n",
      "Epoch [1/1], Step [37470/48549], Loss: 2.5065\n",
      "Epoch [1/1], Step [37480/48549], Loss: 2.4297\n",
      "Epoch [1/1], Step [37490/48549], Loss: 2.4635\n",
      "Epoch [1/1], Step [37500/48549], Loss: 2.4561\n",
      "Epoch [1/1], Step [37510/48549], Loss: 2.4448\n",
      "Epoch [1/1], Step [37520/48549], Loss: 2.4260\n",
      "Epoch [1/1], Step [37530/48549], Loss: 2.4403\n",
      "Epoch [1/1], Step [37540/48549], Loss: 2.3995\n",
      "Epoch [1/1], Step [37550/48549], Loss: 2.4232\n",
      "Epoch [1/1], Step [37560/48549], Loss: 2.4452\n",
      "Epoch [1/1], Step [37570/48549], Loss: 2.4165\n",
      "Epoch [1/1], Step [37580/48549], Loss: 2.4224\n",
      "Epoch [1/1], Step [37590/48549], Loss: 2.4684\n",
      "Epoch [1/1], Step [37600/48549], Loss: 2.4960\n",
      "Epoch [1/1], Step [37610/48549], Loss: 2.4637\n",
      "Epoch [1/1], Step [37620/48549], Loss: 2.4831\n",
      "Epoch [1/1], Step [37630/48549], Loss: 2.4255\n",
      "Epoch [1/1], Step [37640/48549], Loss: 2.3953\n",
      "Epoch [1/1], Step [37650/48549], Loss: 2.4165\n",
      "Epoch [1/1], Step [37660/48549], Loss: 2.4398\n",
      "Epoch [1/1], Step [37670/48549], Loss: 2.4380\n",
      "Epoch [1/1], Step [37680/48549], Loss: 2.4042\n",
      "Epoch [1/1], Step [37690/48549], Loss: 2.4117\n",
      "Epoch [1/1], Step [37700/48549], Loss: 2.4541\n",
      "Epoch [1/1], Step [37710/48549], Loss: 2.4723\n",
      "Epoch [1/1], Step [37720/48549], Loss: 2.4202\n",
      "Epoch [1/1], Step [37730/48549], Loss: 2.4810\n",
      "Epoch [1/1], Step [37740/48549], Loss: 2.4108\n",
      "Epoch [1/1], Step [37750/48549], Loss: 2.4309\n",
      "Epoch [1/1], Step [37760/48549], Loss: 2.4922\n",
      "Epoch [1/1], Step [37770/48549], Loss: 2.4504\n",
      "Epoch [1/1], Step [37780/48549], Loss: 2.4905\n",
      "Epoch [1/1], Step [37790/48549], Loss: 2.4503\n",
      "Epoch [1/1], Step [37800/48549], Loss: 2.4506\n",
      "Epoch [1/1], Step [37810/48549], Loss: 2.3829\n",
      "Epoch [1/1], Step [37820/48549], Loss: 2.4018\n",
      "Epoch [1/1], Step [37830/48549], Loss: 2.4785\n",
      "Epoch [1/1], Step [37840/48549], Loss: 2.4770\n",
      "Epoch [1/1], Step [37850/48549], Loss: 2.4391\n",
      "Epoch [1/1], Step [37860/48549], Loss: 2.4974\n",
      "Epoch [1/1], Step [37870/48549], Loss: 2.4470\n",
      "Epoch [1/1], Step [37880/48549], Loss: 2.4744\n",
      "Epoch [1/1], Step [37890/48549], Loss: 2.4701\n",
      "Epoch [1/1], Step [37900/48549], Loss: 2.4389\n",
      "Epoch [1/1], Step [37910/48549], Loss: 2.5043\n",
      "Epoch [1/1], Step [37920/48549], Loss: 2.4762\n",
      "Epoch [1/1], Step [37930/48549], Loss: 2.4240\n",
      "Epoch [1/1], Step [37940/48549], Loss: 2.4394\n",
      "Epoch [1/1], Step [37950/48549], Loss: 2.4107\n",
      "Epoch [1/1], Step [37960/48549], Loss: 2.3897\n",
      "Epoch [1/1], Step [37970/48549], Loss: 2.4568\n",
      "Epoch [1/1], Step [37980/48549], Loss: 2.4088\n",
      "Epoch [1/1], Step [37990/48549], Loss: 2.3948\n",
      "Epoch [1/1], Step [38000/48549], Loss: 2.5399\n",
      "Epoch [1/1], Step [38010/48549], Loss: 2.4230\n",
      "Epoch [1/1], Step [38020/48549], Loss: 2.4526\n",
      "Epoch [1/1], Step [38030/48549], Loss: 2.4335\n",
      "Epoch [1/1], Step [38040/48549], Loss: 2.4551\n",
      "Epoch [1/1], Step [38050/48549], Loss: 2.4503\n",
      "Epoch [1/1], Step [38060/48549], Loss: 2.4307\n",
      "Epoch [1/1], Step [38070/48549], Loss: 2.4121\n",
      "Epoch [1/1], Step [38080/48549], Loss: 2.4638\n",
      "Epoch [1/1], Step [38090/48549], Loss: 2.3966\n",
      "Epoch [1/1], Step [38100/48549], Loss: 2.3879\n",
      "Epoch [1/1], Step [38110/48549], Loss: 2.3934\n",
      "Epoch [1/1], Step [38120/48549], Loss: 2.4164\n",
      "Epoch [1/1], Step [38130/48549], Loss: 2.4580\n",
      "Epoch [1/1], Step [38140/48549], Loss: 2.4517\n",
      "Epoch [1/1], Step [38150/48549], Loss: 2.4756\n",
      "Epoch [1/1], Step [38160/48549], Loss: 2.4659\n",
      "Epoch [1/1], Step [38170/48549], Loss: 2.4179\n",
      "Epoch [1/1], Step [38180/48549], Loss: 2.4130\n",
      "Epoch [1/1], Step [38190/48549], Loss: 2.4142\n",
      "Epoch [1/1], Step [38200/48549], Loss: 2.4597\n",
      "Epoch [1/1], Step [38210/48549], Loss: 2.4027\n",
      "Epoch [1/1], Step [38220/48549], Loss: 2.4274\n",
      "Epoch [1/1], Step [38230/48549], Loss: 2.4506\n",
      "Epoch [1/1], Step [38240/48549], Loss: 2.4193\n",
      "Epoch [1/1], Step [38250/48549], Loss: 2.4004\n",
      "Epoch [1/1], Step [38260/48549], Loss: 2.4447\n",
      "Epoch [1/1], Step [38270/48549], Loss: 2.4145\n",
      "Epoch [1/1], Step [38280/48549], Loss: 2.4575\n",
      "Epoch [1/1], Step [38290/48549], Loss: 2.4299\n",
      "Epoch [1/1], Step [38300/48549], Loss: 2.5120\n",
      "Epoch [1/1], Step [38310/48549], Loss: 2.4347\n",
      "Epoch [1/1], Step [38320/48549], Loss: 2.4807\n",
      "Epoch [1/1], Step [38330/48549], Loss: 2.4454\n",
      "Epoch [1/1], Step [38340/48549], Loss: 2.4845\n",
      "Epoch [1/1], Step [38350/48549], Loss: 2.4293\n",
      "Epoch [1/1], Step [38360/48549], Loss: 2.4364\n",
      "Epoch [1/1], Step [38370/48549], Loss: 2.4235\n",
      "Epoch [1/1], Step [38380/48549], Loss: 2.4644\n",
      "Epoch [1/1], Step [38390/48549], Loss: 2.4411\n",
      "Epoch [1/1], Step [38400/48549], Loss: 2.4190\n",
      "Epoch [1/1], Step [38410/48549], Loss: 2.4540\n",
      "Epoch [1/1], Step [38420/48549], Loss: 2.4287\n",
      "Epoch [1/1], Step [38430/48549], Loss: 2.4431\n",
      "Epoch [1/1], Step [38440/48549], Loss: 2.4475\n",
      "Epoch [1/1], Step [38450/48549], Loss: 2.4726\n",
      "Epoch [1/1], Step [38460/48549], Loss: 2.4215\n",
      "Epoch [1/1], Step [38470/48549], Loss: 2.4831\n",
      "Epoch [1/1], Step [38480/48549], Loss: 2.4656\n",
      "Epoch [1/1], Step [38490/48549], Loss: 2.3899\n",
      "Epoch [1/1], Step [38500/48549], Loss: 2.4113\n",
      "Epoch [1/1], Step [38510/48549], Loss: 2.4573\n",
      "Epoch [1/1], Step [38520/48549], Loss: 2.4918\n",
      "Epoch [1/1], Step [38530/48549], Loss: 2.3698\n",
      "Epoch [1/1], Step [38540/48549], Loss: 2.4464\n",
      "Epoch [1/1], Step [38550/48549], Loss: 2.4117\n",
      "Epoch [1/1], Step [38560/48549], Loss: 2.3954\n",
      "Epoch [1/1], Step [38570/48549], Loss: 2.4094\n",
      "Epoch [1/1], Step [38580/48549], Loss: 2.4385\n",
      "Epoch [1/1], Step [38590/48549], Loss: 2.4399\n",
      "Epoch [1/1], Step [38600/48549], Loss: 2.5861\n",
      "Epoch [1/1], Step [38610/48549], Loss: 2.3969\n",
      "Epoch [1/1], Step [38620/48549], Loss: 2.4375\n",
      "Epoch [1/1], Step [38630/48549], Loss: 2.4572\n",
      "Epoch [1/1], Step [38640/48549], Loss: 2.4866\n",
      "Epoch [1/1], Step [38650/48549], Loss: 2.5221\n",
      "Epoch [1/1], Step [38660/48549], Loss: 2.4384\n",
      "Epoch [1/1], Step [38670/48549], Loss: 2.5825\n",
      "Epoch [1/1], Step [38680/48549], Loss: 2.4905\n",
      "Epoch [1/1], Step [38690/48549], Loss: 2.4314\n",
      "Epoch [1/1], Step [38700/48549], Loss: 2.4451\n",
      "Epoch [1/1], Step [38710/48549], Loss: 2.4009\n",
      "Epoch [1/1], Step [38720/48549], Loss: 2.4550\n",
      "Epoch [1/1], Step [38730/48549], Loss: 2.4421\n",
      "Epoch [1/1], Step [38740/48549], Loss: 2.4079\n",
      "Epoch [1/1], Step [38750/48549], Loss: 2.4337\n",
      "Epoch [1/1], Step [38760/48549], Loss: 2.4383\n",
      "Epoch [1/1], Step [38770/48549], Loss: 2.4465\n",
      "Epoch [1/1], Step [38780/48549], Loss: 2.4281\n",
      "Epoch [1/1], Step [38790/48549], Loss: 2.4606\n",
      "Epoch [1/1], Step [38800/48549], Loss: 2.4337\n",
      "Epoch [1/1], Step [38810/48549], Loss: 2.4437\n",
      "Epoch [1/1], Step [38820/48549], Loss: 2.4053\n",
      "Epoch [1/1], Step [38830/48549], Loss: 2.4238\n",
      "Epoch [1/1], Step [38840/48549], Loss: 2.4312\n",
      "Epoch [1/1], Step [38850/48549], Loss: 2.3852\n",
      "Epoch [1/1], Step [38860/48549], Loss: 2.4181\n",
      "Epoch [1/1], Step [38870/48549], Loss: 2.4721\n",
      "Epoch [1/1], Step [38880/48549], Loss: 2.4781\n",
      "Epoch [1/1], Step [38890/48549], Loss: 2.4497\n",
      "Epoch [1/1], Step [38900/48549], Loss: 2.4197\n",
      "Epoch [1/1], Step [38910/48549], Loss: 2.4205\n",
      "Epoch [1/1], Step [38920/48549], Loss: 2.4356\n",
      "Epoch [1/1], Step [38930/48549], Loss: 2.5035\n",
      "Epoch [1/1], Step [38940/48549], Loss: 2.4700\n",
      "Epoch [1/1], Step [38950/48549], Loss: 2.4744\n",
      "Epoch [1/1], Step [38960/48549], Loss: 2.4410\n",
      "Epoch [1/1], Step [38970/48549], Loss: 2.4167\n",
      "Epoch [1/1], Step [38980/48549], Loss: 2.4967\n",
      "Epoch [1/1], Step [38990/48549], Loss: 2.5383\n",
      "Epoch [1/1], Step [39000/48549], Loss: 2.4652\n",
      "Epoch [1/1], Step [39010/48549], Loss: 2.4484\n",
      "Epoch [1/1], Step [39020/48549], Loss: 2.4558\n",
      "Epoch [1/1], Step [39030/48549], Loss: 2.4392\n",
      "Epoch [1/1], Step [39040/48549], Loss: 2.4413\n",
      "Epoch [1/1], Step [39050/48549], Loss: 2.4648\n",
      "Epoch [1/1], Step [39060/48549], Loss: 2.4439\n",
      "Epoch [1/1], Step [39070/48549], Loss: 2.4358\n",
      "Epoch [1/1], Step [39080/48549], Loss: 2.4592\n",
      "Epoch [1/1], Step [39090/48549], Loss: 2.5255\n",
      "Epoch [1/1], Step [39100/48549], Loss: 2.4561\n",
      "Epoch [1/1], Step [39110/48549], Loss: 2.4202\n",
      "Epoch [1/1], Step [39120/48549], Loss: 2.4408\n",
      "Epoch [1/1], Step [39130/48549], Loss: 2.4169\n",
      "Epoch [1/1], Step [39140/48549], Loss: 2.3818\n",
      "Epoch [1/1], Step [39150/48549], Loss: 2.4153\n",
      "Epoch [1/1], Step [39160/48549], Loss: 2.4852\n",
      "Epoch [1/1], Step [39170/48549], Loss: 2.4545\n",
      "Epoch [1/1], Step [39180/48549], Loss: 2.4315\n",
      "Epoch [1/1], Step [39190/48549], Loss: 2.4106\n",
      "Epoch [1/1], Step [39200/48549], Loss: 2.4126\n",
      "Epoch [1/1], Step [39210/48549], Loss: 2.4386\n",
      "Epoch [1/1], Step [39220/48549], Loss: 2.4328\n",
      "Epoch [1/1], Step [39230/48549], Loss: 2.4371\n",
      "Epoch [1/1], Step [39240/48549], Loss: 2.4590\n",
      "Epoch [1/1], Step [39250/48549], Loss: 2.3667\n",
      "Epoch [1/1], Step [39260/48549], Loss: 2.4656\n",
      "Epoch [1/1], Step [39270/48549], Loss: 2.4433\n",
      "Epoch [1/1], Step [39280/48549], Loss: 2.4337\n",
      "Epoch [1/1], Step [39290/48549], Loss: 2.3843\n",
      "Epoch [1/1], Step [39300/48549], Loss: 2.4297\n",
      "Epoch [1/1], Step [39310/48549], Loss: 2.4261\n",
      "Epoch [1/1], Step [39320/48549], Loss: 2.4019\n",
      "Epoch [1/1], Step [39330/48549], Loss: 2.5888\n",
      "Epoch [1/1], Step [39340/48549], Loss: 2.4520\n",
      "Epoch [1/1], Step [39350/48549], Loss: 2.4450\n",
      "Epoch [1/1], Step [39360/48549], Loss: 2.4999\n",
      "Epoch [1/1], Step [39370/48549], Loss: 2.3914\n",
      "Epoch [1/1], Step [39380/48549], Loss: 2.4971\n",
      "Epoch [1/1], Step [39390/48549], Loss: 2.3948\n",
      "Epoch [1/1], Step [39400/48549], Loss: 2.4167\n",
      "Epoch [1/1], Step [39410/48549], Loss: 2.4337\n",
      "Epoch [1/1], Step [39420/48549], Loss: 2.5037\n",
      "Epoch [1/1], Step [39430/48549], Loss: 2.4367\n",
      "Epoch [1/1], Step [39440/48549], Loss: 2.4329\n",
      "Epoch [1/1], Step [39450/48549], Loss: 2.4461\n",
      "Epoch [1/1], Step [39460/48549], Loss: 2.4748\n",
      "Epoch [1/1], Step [39470/48549], Loss: 2.4814\n",
      "Epoch [1/1], Step [39480/48549], Loss: 2.5502\n",
      "Epoch [1/1], Step [39490/48549], Loss: 2.4073\n",
      "Epoch [1/1], Step [39500/48549], Loss: 2.4237\n",
      "Epoch [1/1], Step [39510/48549], Loss: 2.5164\n",
      "Epoch [1/1], Step [39520/48549], Loss: 2.4299\n",
      "Epoch [1/1], Step [39530/48549], Loss: 2.4213\n",
      "Epoch [1/1], Step [39540/48549], Loss: 2.4344\n",
      "Epoch [1/1], Step [39550/48549], Loss: 2.4511\n",
      "Epoch [1/1], Step [39560/48549], Loss: 2.4342\n",
      "Epoch [1/1], Step [39570/48549], Loss: 2.4256\n",
      "Epoch [1/1], Step [39580/48549], Loss: 2.4162\n",
      "Epoch [1/1], Step [39590/48549], Loss: 2.4225\n",
      "Epoch [1/1], Step [39600/48549], Loss: 2.4451\n",
      "Epoch [1/1], Step [39610/48549], Loss: 2.4730\n",
      "Epoch [1/1], Step [39620/48549], Loss: 2.4367\n",
      "Epoch [1/1], Step [39630/48549], Loss: 2.4403\n",
      "Epoch [1/1], Step [39640/48549], Loss: 2.5040\n",
      "Epoch [1/1], Step [39650/48549], Loss: 2.4094\n",
      "Epoch [1/1], Step [39660/48549], Loss: 2.4138\n",
      "Epoch [1/1], Step [39670/48549], Loss: 2.4663\n",
      "Epoch [1/1], Step [39680/48549], Loss: 2.4248\n",
      "Epoch [1/1], Step [39690/48549], Loss: 2.4796\n",
      "Epoch [1/1], Step [39700/48549], Loss: 2.5199\n",
      "Epoch [1/1], Step [39710/48549], Loss: 2.4354\n",
      "Epoch [1/1], Step [39720/48549], Loss: 2.4584\n",
      "Epoch [1/1], Step [39730/48549], Loss: 2.4386\n",
      "Epoch [1/1], Step [39740/48549], Loss: 2.5183\n",
      "Epoch [1/1], Step [39750/48549], Loss: 2.4692\n",
      "Epoch [1/1], Step [39760/48549], Loss: 2.4155\n",
      "Epoch [1/1], Step [39770/48549], Loss: 2.4757\n",
      "Epoch [1/1], Step [39780/48549], Loss: 2.3995\n",
      "Epoch [1/1], Step [39790/48549], Loss: 2.4700\n",
      "Epoch [1/1], Step [39800/48549], Loss: 2.4239\n",
      "Epoch [1/1], Step [39810/48549], Loss: 2.4508\n",
      "Epoch [1/1], Step [39820/48549], Loss: 2.4353\n",
      "Epoch [1/1], Step [39830/48549], Loss: 2.5105\n",
      "Epoch [1/1], Step [39840/48549], Loss: 2.4557\n",
      "Epoch [1/1], Step [39850/48549], Loss: 2.4849\n",
      "Epoch [1/1], Step [39860/48549], Loss: 2.4219\n",
      "Epoch [1/1], Step [39870/48549], Loss: 2.4816\n",
      "Epoch [1/1], Step [39880/48549], Loss: 2.4441\n",
      "Epoch [1/1], Step [39890/48549], Loss: 2.5143\n",
      "Epoch [1/1], Step [39900/48549], Loss: 2.4711\n",
      "Epoch [1/1], Step [39910/48549], Loss: 2.4773\n",
      "Epoch [1/1], Step [39920/48549], Loss: 2.3827\n",
      "Epoch [1/1], Step [39930/48549], Loss: 2.3833\n",
      "Epoch [1/1], Step [39940/48549], Loss: 2.4888\n",
      "Epoch [1/1], Step [39950/48549], Loss: 2.6097\n",
      "Epoch [1/1], Step [39960/48549], Loss: 2.4999\n",
      "Epoch [1/1], Step [39970/48549], Loss: 2.4176\n",
      "Epoch [1/1], Step [39980/48549], Loss: 2.4627\n",
      "Epoch [1/1], Step [39990/48549], Loss: 2.4188\n",
      "Epoch [1/1], Step [40000/48549], Loss: 2.4290\n",
      "Epoch [1/1], Step [40010/48549], Loss: 2.4140\n",
      "Epoch [1/1], Step [40020/48549], Loss: 2.4637\n",
      "Epoch [1/1], Step [40030/48549], Loss: 2.4737\n",
      "Epoch [1/1], Step [40040/48549], Loss: 2.4214\n",
      "Epoch [1/1], Step [40050/48549], Loss: 2.5097\n",
      "Epoch [1/1], Step [40060/48549], Loss: 2.4404\n",
      "Epoch [1/1], Step [40070/48549], Loss: 2.4327\n",
      "Epoch [1/1], Step [40080/48549], Loss: 2.3835\n",
      "Epoch [1/1], Step [40090/48549], Loss: 2.4566\n",
      "Epoch [1/1], Step [40100/48549], Loss: 2.4619\n",
      "Epoch [1/1], Step [40110/48549], Loss: 2.5281\n",
      "Epoch [1/1], Step [40120/48549], Loss: 2.4933\n",
      "Epoch [1/1], Step [40130/48549], Loss: 2.4641\n",
      "Epoch [1/1], Step [40140/48549], Loss: 2.4569\n",
      "Epoch [1/1], Step [40150/48549], Loss: 2.4612\n",
      "Epoch [1/1], Step [40160/48549], Loss: 2.3910\n",
      "Epoch [1/1], Step [40170/48549], Loss: 2.4234\n",
      "Epoch [1/1], Step [40180/48549], Loss: 2.4682\n",
      "Epoch [1/1], Step [40190/48549], Loss: 2.3998\n",
      "Epoch [1/1], Step [40200/48549], Loss: 2.4311\n",
      "Epoch [1/1], Step [40210/48549], Loss: 2.4518\n",
      "Epoch [1/1], Step [40220/48549], Loss: 2.4755\n",
      "Epoch [1/1], Step [40230/48549], Loss: 2.4280\n",
      "Epoch [1/1], Step [40240/48549], Loss: 2.4188\n",
      "Epoch [1/1], Step [40250/48549], Loss: 2.4806\n",
      "Epoch [1/1], Step [40260/48549], Loss: 2.4424\n",
      "Epoch [1/1], Step [40270/48549], Loss: 2.4444\n",
      "Epoch [1/1], Step [40280/48549], Loss: 2.5007\n",
      "Epoch [1/1], Step [40290/48549], Loss: 2.4373\n",
      "Epoch [1/1], Step [40300/48549], Loss: 2.3967\n",
      "Epoch [1/1], Step [40310/48549], Loss: 2.4355\n",
      "Epoch [1/1], Step [40320/48549], Loss: 2.4137\n",
      "Epoch [1/1], Step [40330/48549], Loss: 2.4341\n",
      "Epoch [1/1], Step [40340/48549], Loss: 2.4832\n",
      "Epoch [1/1], Step [40350/48549], Loss: 2.5075\n",
      "Epoch [1/1], Step [40360/48549], Loss: 2.4828\n",
      "Epoch [1/1], Step [40370/48549], Loss: 2.4587\n",
      "Epoch [1/1], Step [40380/48549], Loss: 2.3894\n",
      "Epoch [1/1], Step [40390/48549], Loss: 2.4689\n",
      "Epoch [1/1], Step [40400/48549], Loss: 2.4439\n",
      "Epoch [1/1], Step [40410/48549], Loss: 2.4464\n",
      "Epoch [1/1], Step [40420/48549], Loss: 2.4069\n",
      "Epoch [1/1], Step [40430/48549], Loss: 2.4438\n",
      "Epoch [1/1], Step [40440/48549], Loss: 2.4653\n",
      "Epoch [1/1], Step [40450/48549], Loss: 2.4332\n",
      "Epoch [1/1], Step [40460/48549], Loss: 2.4523\n",
      "Epoch [1/1], Step [40470/48549], Loss: 2.4467\n",
      "Epoch [1/1], Step [40480/48549], Loss: 2.4566\n",
      "Epoch [1/1], Step [40490/48549], Loss: 2.3588\n",
      "Epoch [1/1], Step [40500/48549], Loss: 2.4518\n",
      "Epoch [1/1], Step [40510/48549], Loss: 2.4661\n",
      "Epoch [1/1], Step [40520/48549], Loss: 2.4164\n",
      "Epoch [1/1], Step [40530/48549], Loss: 2.4369\n",
      "Epoch [1/1], Step [40540/48549], Loss: 2.4658\n",
      "Epoch [1/1], Step [40550/48549], Loss: 2.4613\n",
      "Epoch [1/1], Step [40560/48549], Loss: 2.5209\n",
      "Epoch [1/1], Step [40570/48549], Loss: 2.4089\n",
      "Epoch [1/1], Step [40580/48549], Loss: 2.4240\n",
      "Epoch [1/1], Step [40590/48549], Loss: 2.5213\n",
      "Epoch [1/1], Step [40600/48549], Loss: 2.3967\n",
      "Epoch [1/1], Step [40610/48549], Loss: 2.5795\n",
      "Epoch [1/1], Step [40620/48549], Loss: 2.3959\n",
      "Epoch [1/1], Step [40630/48549], Loss: 2.4211\n",
      "Epoch [1/1], Step [40640/48549], Loss: 2.4371\n",
      "Epoch [1/1], Step [40650/48549], Loss: 2.4413\n",
      "Epoch [1/1], Step [40660/48549], Loss: 2.4438\n",
      "Epoch [1/1], Step [40670/48549], Loss: 2.4754\n",
      "Epoch [1/1], Step [40680/48549], Loss: 2.4369\n",
      "Epoch [1/1], Step [40690/48549], Loss: 2.4402\n",
      "Epoch [1/1], Step [40700/48549], Loss: 2.4198\n",
      "Epoch [1/1], Step [40710/48549], Loss: 2.4332\n",
      "Epoch [1/1], Step [40720/48549], Loss: 2.4304\n",
      "Epoch [1/1], Step [40730/48549], Loss: 2.4074\n",
      "Epoch [1/1], Step [40740/48549], Loss: 2.5289\n",
      "Epoch [1/1], Step [40750/48549], Loss: 2.4429\n",
      "Epoch [1/1], Step [40760/48549], Loss: 2.4561\n",
      "Epoch [1/1], Step [40770/48549], Loss: 2.4246\n",
      "Epoch [1/1], Step [40780/48549], Loss: 2.4123\n",
      "Epoch [1/1], Step [40790/48549], Loss: 2.4283\n",
      "Epoch [1/1], Step [40800/48549], Loss: 2.4355\n",
      "Epoch [1/1], Step [40810/48549], Loss: 2.4661\n",
      "Epoch [1/1], Step [40820/48549], Loss: 2.4181\n",
      "Epoch [1/1], Step [40830/48549], Loss: 2.4729\n",
      "Epoch [1/1], Step [40840/48549], Loss: 2.4105\n",
      "Epoch [1/1], Step [40850/48549], Loss: 2.4174\n",
      "Epoch [1/1], Step [40860/48549], Loss: 2.4555\n",
      "Epoch [1/1], Step [40870/48549], Loss: 2.4342\n",
      "Epoch [1/1], Step [40880/48549], Loss: 2.4256\n",
      "Epoch [1/1], Step [40890/48549], Loss: 2.4656\n",
      "Epoch [1/1], Step [40900/48549], Loss: 2.4113\n",
      "Epoch [1/1], Step [40910/48549], Loss: 2.4672\n",
      "Epoch [1/1], Step [40920/48549], Loss: 2.6255\n",
      "Epoch [1/1], Step [40930/48549], Loss: 2.4371\n",
      "Epoch [1/1], Step [40940/48549], Loss: 2.3986\n",
      "Epoch [1/1], Step [40950/48549], Loss: 2.4691\n",
      "Epoch [1/1], Step [40960/48549], Loss: 2.5147\n",
      "Epoch [1/1], Step [40970/48549], Loss: 2.4857\n",
      "Epoch [1/1], Step [40980/48549], Loss: 2.4487\n",
      "Epoch [1/1], Step [40990/48549], Loss: 2.4616\n",
      "Epoch [1/1], Step [41000/48549], Loss: 2.4935\n",
      "Epoch [1/1], Step [41010/48549], Loss: 2.3996\n",
      "Epoch [1/1], Step [41020/48549], Loss: 2.4784\n",
      "Epoch [1/1], Step [41030/48549], Loss: 2.4016\n",
      "Epoch [1/1], Step [41040/48549], Loss: 2.4447\n",
      "Epoch [1/1], Step [41050/48549], Loss: 2.4067\n",
      "Epoch [1/1], Step [41060/48549], Loss: 2.4583\n",
      "Epoch [1/1], Step [41070/48549], Loss: 2.4308\n",
      "Epoch [1/1], Step [41080/48549], Loss: 2.4393\n",
      "Epoch [1/1], Step [41090/48549], Loss: 2.4046\n",
      "Epoch [1/1], Step [41100/48549], Loss: 2.5007\n",
      "Epoch [1/1], Step [41110/48549], Loss: 2.4695\n",
      "Epoch [1/1], Step [41120/48549], Loss: 2.3651\n",
      "Epoch [1/1], Step [41130/48549], Loss: 2.5505\n",
      "Epoch [1/1], Step [41140/48549], Loss: 2.4404\n",
      "Epoch [1/1], Step [41150/48549], Loss: 2.4291\n",
      "Epoch [1/1], Step [41160/48549], Loss: 2.4355\n",
      "Epoch [1/1], Step [41170/48549], Loss: 2.4394\n",
      "Epoch [1/1], Step [41180/48549], Loss: 2.4186\n",
      "Epoch [1/1], Step [41190/48549], Loss: 2.4521\n",
      "Epoch [1/1], Step [41200/48549], Loss: 2.4001\n",
      "Epoch [1/1], Step [41210/48549], Loss: 2.5349\n",
      "Epoch [1/1], Step [41220/48549], Loss: 2.4566\n",
      "Epoch [1/1], Step [41230/48549], Loss: 2.4576\n",
      "Epoch [1/1], Step [41240/48549], Loss: 2.4541\n",
      "Epoch [1/1], Step [41250/48549], Loss: 2.5864\n",
      "Epoch [1/1], Step [41260/48549], Loss: 2.4401\n",
      "Epoch [1/1], Step [41270/48549], Loss: 2.4832\n",
      "Epoch [1/1], Step [41280/48549], Loss: 2.4987\n",
      "Epoch [1/1], Step [41290/48549], Loss: 2.4719\n",
      "Epoch [1/1], Step [41300/48549], Loss: 2.4316\n",
      "Epoch [1/1], Step [41310/48549], Loss: 2.4302\n",
      "Epoch [1/1], Step [41320/48549], Loss: 2.5232\n",
      "Epoch [1/1], Step [41330/48549], Loss: 2.4811\n",
      "Epoch [1/1], Step [41340/48549], Loss: 2.4720\n",
      "Epoch [1/1], Step [41350/48549], Loss: 2.4699\n",
      "Epoch [1/1], Step [41360/48549], Loss: 2.4744\n",
      "Epoch [1/1], Step [41370/48549], Loss: 2.4208\n",
      "Epoch [1/1], Step [41380/48549], Loss: 2.4066\n",
      "Epoch [1/1], Step [41390/48549], Loss: 2.4002\n",
      "Epoch [1/1], Step [41400/48549], Loss: 2.4939\n",
      "Epoch [1/1], Step [41410/48549], Loss: 2.4429\n",
      "Epoch [1/1], Step [41420/48549], Loss: 2.4330\n",
      "Epoch [1/1], Step [41430/48549], Loss: 2.4036\n",
      "Epoch [1/1], Step [41440/48549], Loss: 2.4322\n",
      "Epoch [1/1], Step [41450/48549], Loss: 2.3928\n",
      "Epoch [1/1], Step [41460/48549], Loss: 2.5009\n",
      "Epoch [1/1], Step [41470/48549], Loss: 2.4467\n",
      "Epoch [1/1], Step [41480/48549], Loss: 2.4091\n",
      "Epoch [1/1], Step [41490/48549], Loss: 2.4455\n",
      "Epoch [1/1], Step [41500/48549], Loss: 2.4143\n",
      "Epoch [1/1], Step [41510/48549], Loss: 2.4841\n",
      "Epoch [1/1], Step [41520/48549], Loss: 2.4212\n",
      "Epoch [1/1], Step [41530/48549], Loss: 2.4362\n",
      "Epoch [1/1], Step [41540/48549], Loss: 2.4487\n",
      "Epoch [1/1], Step [41550/48549], Loss: 2.4573\n",
      "Epoch [1/1], Step [41560/48549], Loss: 2.4500\n",
      "Epoch [1/1], Step [41570/48549], Loss: 2.4841\n",
      "Epoch [1/1], Step [41580/48549], Loss: 2.4351\n",
      "Epoch [1/1], Step [41590/48549], Loss: 2.4870\n",
      "Epoch [1/1], Step [41600/48549], Loss: 2.4047\n",
      "Epoch [1/1], Step [41610/48549], Loss: 2.4888\n",
      "Epoch [1/1], Step [41620/48549], Loss: 2.4879\n",
      "Epoch [1/1], Step [41630/48549], Loss: 2.4433\n",
      "Epoch [1/1], Step [41640/48549], Loss: 2.4787\n",
      "Epoch [1/1], Step [41650/48549], Loss: 2.3751\n",
      "Epoch [1/1], Step [41660/48549], Loss: 2.4700\n",
      "Epoch [1/1], Step [41670/48549], Loss: 2.4848\n",
      "Epoch [1/1], Step [41680/48549], Loss: 2.4521\n",
      "Epoch [1/1], Step [41690/48549], Loss: 2.4293\n",
      "Epoch [1/1], Step [41700/48549], Loss: 2.4363\n",
      "Epoch [1/1], Step [41710/48549], Loss: 2.4650\n",
      "Epoch [1/1], Step [41720/48549], Loss: 2.4322\n",
      "Epoch [1/1], Step [41730/48549], Loss: 2.4823\n",
      "Epoch [1/1], Step [41740/48549], Loss: 2.3815\n",
      "Epoch [1/1], Step [41750/48549], Loss: 2.4174\n",
      "Epoch [1/1], Step [41760/48549], Loss: 2.3858\n",
      "Epoch [1/1], Step [41770/48549], Loss: 2.4843\n",
      "Epoch [1/1], Step [41780/48549], Loss: 2.4363\n",
      "Epoch [1/1], Step [41790/48549], Loss: 2.4823\n",
      "Epoch [1/1], Step [41800/48549], Loss: 2.4614\n",
      "Epoch [1/1], Step [41810/48549], Loss: 2.4184\n",
      "Epoch [1/1], Step [41820/48549], Loss: 2.4747\n",
      "Epoch [1/1], Step [41830/48549], Loss: 2.3938\n",
      "Epoch [1/1], Step [41840/48549], Loss: 2.4430\n",
      "Epoch [1/1], Step [41850/48549], Loss: 2.4122\n",
      "Epoch [1/1], Step [41860/48549], Loss: 2.4056\n",
      "Epoch [1/1], Step [41870/48549], Loss: 2.4893\n",
      "Epoch [1/1], Step [41880/48549], Loss: 2.4178\n",
      "Epoch [1/1], Step [41890/48549], Loss: 2.4755\n",
      "Epoch [1/1], Step [41900/48549], Loss: 2.4010\n",
      "Epoch [1/1], Step [41910/48549], Loss: 2.4273\n",
      "Epoch [1/1], Step [41920/48549], Loss: 2.3627\n",
      "Epoch [1/1], Step [41930/48549], Loss: 2.4695\n",
      "Epoch [1/1], Step [41940/48549], Loss: 2.3962\n",
      "Epoch [1/1], Step [41950/48549], Loss: 2.4558\n",
      "Epoch [1/1], Step [41960/48549], Loss: 2.4227\n",
      "Epoch [1/1], Step [41970/48549], Loss: 2.4652\n",
      "Epoch [1/1], Step [41980/48549], Loss: 2.4709\n",
      "Epoch [1/1], Step [41990/48549], Loss: 2.4874\n",
      "Epoch [1/1], Step [42000/48549], Loss: 2.5353\n",
      "Epoch [1/1], Step [42010/48549], Loss: 2.4370\n",
      "Epoch [1/1], Step [42020/48549], Loss: 2.5780\n",
      "Epoch [1/1], Step [42030/48549], Loss: 2.4465\n",
      "Epoch [1/1], Step [42040/48549], Loss: 2.4558\n",
      "Epoch [1/1], Step [42050/48549], Loss: 2.4204\n",
      "Epoch [1/1], Step [42060/48549], Loss: 2.4605\n",
      "Epoch [1/1], Step [42070/48549], Loss: 2.4942\n",
      "Epoch [1/1], Step [42080/48549], Loss: 2.4587\n",
      "Epoch [1/1], Step [42090/48549], Loss: 2.4759\n",
      "Epoch [1/1], Step [42100/48549], Loss: 2.4387\n",
      "Epoch [1/1], Step [42110/48549], Loss: 2.4339\n",
      "Epoch [1/1], Step [42120/48549], Loss: 2.3883\n",
      "Epoch [1/1], Step [42130/48549], Loss: 2.4693\n",
      "Epoch [1/1], Step [42140/48549], Loss: 2.4101\n",
      "Epoch [1/1], Step [42150/48549], Loss: 2.4640\n",
      "Epoch [1/1], Step [42160/48549], Loss: 2.4773\n",
      "Epoch [1/1], Step [42170/48549], Loss: 2.4741\n",
      "Epoch [1/1], Step [42180/48549], Loss: 2.4216\n",
      "Epoch [1/1], Step [42190/48549], Loss: 2.4175\n",
      "Epoch [1/1], Step [42200/48549], Loss: 2.4327\n",
      "Epoch [1/1], Step [42210/48549], Loss: 2.5986\n",
      "Epoch [1/1], Step [42220/48549], Loss: 2.4123\n",
      "Epoch [1/1], Step [42230/48549], Loss: 2.4351\n",
      "Epoch [1/1], Step [42240/48549], Loss: 2.4319\n",
      "Epoch [1/1], Step [42250/48549], Loss: 2.4024\n",
      "Epoch [1/1], Step [42260/48549], Loss: 2.4034\n",
      "Epoch [1/1], Step [42270/48549], Loss: 2.5183\n",
      "Epoch [1/1], Step [42280/48549], Loss: 2.4125\n",
      "Epoch [1/1], Step [42290/48549], Loss: 2.4162\n",
      "Epoch [1/1], Step [42300/48549], Loss: 2.4677\n",
      "Epoch [1/1], Step [42310/48549], Loss: 2.4083\n",
      "Epoch [1/1], Step [42320/48549], Loss: 2.4951\n",
      "Epoch [1/1], Step [42330/48549], Loss: 2.4744\n",
      "Epoch [1/1], Step [42340/48549], Loss: 2.4017\n",
      "Epoch [1/1], Step [42350/48549], Loss: 2.4688\n",
      "Epoch [1/1], Step [42360/48549], Loss: 2.4499\n",
      "Epoch [1/1], Step [42370/48549], Loss: 2.4307\n",
      "Epoch [1/1], Step [42380/48549], Loss: 2.4598\n",
      "Epoch [1/1], Step [42390/48549], Loss: 2.4676\n",
      "Epoch [1/1], Step [42400/48549], Loss: 2.4710\n",
      "Epoch [1/1], Step [42410/48549], Loss: 2.5116\n",
      "Epoch [1/1], Step [42420/48549], Loss: 2.4241\n",
      "Epoch [1/1], Step [42430/48549], Loss: 2.3751\n",
      "Epoch [1/1], Step [42440/48549], Loss: 2.4467\n",
      "Epoch [1/1], Step [42450/48549], Loss: 2.4316\n",
      "Epoch [1/1], Step [42460/48549], Loss: 2.5298\n",
      "Epoch [1/1], Step [42470/48549], Loss: 2.4305\n",
      "Epoch [1/1], Step [42480/48549], Loss: 2.4452\n",
      "Epoch [1/1], Step [42490/48549], Loss: 2.5108\n",
      "Epoch [1/1], Step [42500/48549], Loss: 2.4119\n",
      "Epoch [1/1], Step [42510/48549], Loss: 2.4459\n",
      "Epoch [1/1], Step [42520/48549], Loss: 2.5323\n",
      "Epoch [1/1], Step [42530/48549], Loss: 2.4676\n",
      "Epoch [1/1], Step [42540/48549], Loss: 2.4293\n",
      "Epoch [1/1], Step [42550/48549], Loss: 2.4511\n",
      "Epoch [1/1], Step [42560/48549], Loss: 2.4560\n",
      "Epoch [1/1], Step [42570/48549], Loss: 2.4392\n",
      "Epoch [1/1], Step [42580/48549], Loss: 2.4253\n",
      "Epoch [1/1], Step [42590/48549], Loss: 2.4055\n",
      "Epoch [1/1], Step [42600/48549], Loss: 2.4732\n",
      "Epoch [1/1], Step [42610/48549], Loss: 2.4346\n",
      "Epoch [1/1], Step [42620/48549], Loss: 2.4690\n",
      "Epoch [1/1], Step [42630/48549], Loss: 2.4786\n",
      "Epoch [1/1], Step [42640/48549], Loss: 2.4141\n",
      "Epoch [1/1], Step [42650/48549], Loss: 2.4165\n",
      "Epoch [1/1], Step [42660/48549], Loss: 2.4681\n",
      "Epoch [1/1], Step [42670/48549], Loss: 2.4223\n",
      "Epoch [1/1], Step [42680/48549], Loss: 2.4185\n",
      "Epoch [1/1], Step [42690/48549], Loss: 2.4596\n",
      "Epoch [1/1], Step [42700/48549], Loss: 2.4650\n",
      "Epoch [1/1], Step [42710/48549], Loss: 2.4542\n",
      "Epoch [1/1], Step [42720/48549], Loss: 2.4439\n",
      "Epoch [1/1], Step [42730/48549], Loss: 2.4129\n",
      "Epoch [1/1], Step [42740/48549], Loss: 2.4840\n",
      "Epoch [1/1], Step [42750/48549], Loss: 2.3994\n",
      "Epoch [1/1], Step [42760/48549], Loss: 2.4059\n",
      "Epoch [1/1], Step [42770/48549], Loss: 2.4397\n",
      "Epoch [1/1], Step [42780/48549], Loss: 2.4213\n",
      "Epoch [1/1], Step [42790/48549], Loss: 2.4433\n",
      "Epoch [1/1], Step [42800/48549], Loss: 2.4367\n",
      "Epoch [1/1], Step [42810/48549], Loss: 2.4431\n",
      "Epoch [1/1], Step [42820/48549], Loss: 2.4911\n",
      "Epoch [1/1], Step [42830/48549], Loss: 2.4355\n",
      "Epoch [1/1], Step [42840/48549], Loss: 2.4238\n",
      "Epoch [1/1], Step [42850/48549], Loss: 2.4549\n",
      "Epoch [1/1], Step [42860/48549], Loss: 2.5313\n",
      "Epoch [1/1], Step [42870/48549], Loss: 2.4010\n",
      "Epoch [1/1], Step [42880/48549], Loss: 2.4386\n",
      "Epoch [1/1], Step [42890/48549], Loss: 2.4929\n",
      "Epoch [1/1], Step [42900/48549], Loss: 2.4111\n",
      "Epoch [1/1], Step [42910/48549], Loss: 2.4192\n",
      "Epoch [1/1], Step [42920/48549], Loss: 2.4040\n",
      "Epoch [1/1], Step [42930/48549], Loss: 2.4809\n",
      "Epoch [1/1], Step [42940/48549], Loss: 2.4988\n",
      "Epoch [1/1], Step [42950/48549], Loss: 2.3739\n",
      "Epoch [1/1], Step [42960/48549], Loss: 2.4487\n",
      "Epoch [1/1], Step [42970/48549], Loss: 2.4788\n",
      "Epoch [1/1], Step [42980/48549], Loss: 2.4250\n",
      "Epoch [1/1], Step [42990/48549], Loss: 2.5333\n",
      "Epoch [1/1], Step [43000/48549], Loss: 2.4450\n",
      "Epoch [1/1], Step [43010/48549], Loss: 2.4475\n",
      "Epoch [1/1], Step [43020/48549], Loss: 2.4172\n",
      "Epoch [1/1], Step [43030/48549], Loss: 2.4310\n",
      "Epoch [1/1], Step [43040/48549], Loss: 2.4674\n",
      "Epoch [1/1], Step [43050/48549], Loss: 2.4979\n",
      "Epoch [1/1], Step [43060/48549], Loss: 2.4226\n",
      "Epoch [1/1], Step [43070/48549], Loss: 2.4426\n",
      "Epoch [1/1], Step [43080/48549], Loss: 2.4539\n",
      "Epoch [1/1], Step [43090/48549], Loss: 2.5419\n",
      "Epoch [1/1], Step [43100/48549], Loss: 2.4000\n",
      "Epoch [1/1], Step [43110/48549], Loss: 2.4160\n",
      "Epoch [1/1], Step [43120/48549], Loss: 2.5057\n",
      "Epoch [1/1], Step [43130/48549], Loss: 2.4392\n",
      "Epoch [1/1], Step [43140/48549], Loss: 2.4550\n",
      "Epoch [1/1], Step [43150/48549], Loss: 2.4481\n",
      "Epoch [1/1], Step [43160/48549], Loss: 2.4791\n",
      "Epoch [1/1], Step [43170/48549], Loss: 2.4006\n",
      "Epoch [1/1], Step [43180/48549], Loss: 2.4636\n",
      "Epoch [1/1], Step [43190/48549], Loss: 2.4878\n",
      "Epoch [1/1], Step [43200/48549], Loss: 2.4719\n",
      "Epoch [1/1], Step [43210/48549], Loss: 2.4211\n",
      "Epoch [1/1], Step [43220/48549], Loss: 2.4153\n",
      "Epoch [1/1], Step [43230/48549], Loss: 2.4165\n",
      "Epoch [1/1], Step [43240/48549], Loss: 2.4296\n",
      "Epoch [1/1], Step [43250/48549], Loss: 2.4409\n",
      "Epoch [1/1], Step [43260/48549], Loss: 2.4210\n",
      "Epoch [1/1], Step [43270/48549], Loss: 2.4326\n",
      "Epoch [1/1], Step [43280/48549], Loss: 2.4521\n",
      "Epoch [1/1], Step [43290/48549], Loss: 2.5735\n",
      "Epoch [1/1], Step [43300/48549], Loss: 2.4144\n",
      "Epoch [1/1], Step [43310/48549], Loss: 2.4325\n",
      "Epoch [1/1], Step [43320/48549], Loss: 2.4847\n",
      "Epoch [1/1], Step [43330/48549], Loss: 2.4512\n",
      "Epoch [1/1], Step [43340/48549], Loss: 2.4186\n",
      "Epoch [1/1], Step [43350/48549], Loss: 2.4593\n",
      "Epoch [1/1], Step [43360/48549], Loss: 2.4139\n",
      "Epoch [1/1], Step [43370/48549], Loss: 2.4991\n",
      "Epoch [1/1], Step [43380/48549], Loss: 2.4764\n",
      "Epoch [1/1], Step [43390/48549], Loss: 2.4596\n",
      "Epoch [1/1], Step [43400/48549], Loss: 2.4415\n",
      "Epoch [1/1], Step [43410/48549], Loss: 2.4456\n",
      "Epoch [1/1], Step [43420/48549], Loss: 2.3824\n",
      "Epoch [1/1], Step [43430/48549], Loss: 2.3887\n",
      "Epoch [1/1], Step [43440/48549], Loss: 2.3974\n",
      "Epoch [1/1], Step [43450/48549], Loss: 2.4143\n",
      "Epoch [1/1], Step [43460/48549], Loss: 2.3912\n",
      "Epoch [1/1], Step [43470/48549], Loss: 2.4486\n",
      "Epoch [1/1], Step [43480/48549], Loss: 2.4142\n",
      "Epoch [1/1], Step [43490/48549], Loss: 2.4382\n",
      "Epoch [1/1], Step [43500/48549], Loss: 2.6305\n",
      "Epoch [1/1], Step [43510/48549], Loss: 2.4298\n",
      "Epoch [1/1], Step [43520/48549], Loss: 2.4419\n",
      "Epoch [1/1], Step [43530/48549], Loss: 2.4162\n",
      "Epoch [1/1], Step [43540/48549], Loss: 2.4182\n",
      "Epoch [1/1], Step [43550/48549], Loss: 2.3704\n",
      "Epoch [1/1], Step [43560/48549], Loss: 2.4707\n",
      "Epoch [1/1], Step [43570/48549], Loss: 2.4334\n",
      "Epoch [1/1], Step [43580/48549], Loss: 2.5073\n",
      "Epoch [1/1], Step [43590/48549], Loss: 2.4314\n",
      "Epoch [1/1], Step [43600/48549], Loss: 2.3884\n",
      "Epoch [1/1], Step [43610/48549], Loss: 2.4464\n",
      "Epoch [1/1], Step [43620/48549], Loss: 2.4474\n",
      "Epoch [1/1], Step [43630/48549], Loss: 2.4278\n",
      "Epoch [1/1], Step [43640/48549], Loss: 2.4445\n",
      "Epoch [1/1], Step [43650/48549], Loss: 2.4447\n",
      "Epoch [1/1], Step [43660/48549], Loss: 2.4286\n",
      "Epoch [1/1], Step [43670/48549], Loss: 2.4099\n",
      "Epoch [1/1], Step [43680/48549], Loss: 2.4470\n",
      "Epoch [1/1], Step [43690/48549], Loss: 2.4187\n",
      "Epoch [1/1], Step [43700/48549], Loss: 2.4196\n",
      "Epoch [1/1], Step [43710/48549], Loss: 2.4700\n",
      "Epoch [1/1], Step [43720/48549], Loss: 2.4192\n",
      "Epoch [1/1], Step [43730/48549], Loss: 2.4351\n",
      "Epoch [1/1], Step [43740/48549], Loss: 2.4822\n",
      "Epoch [1/1], Step [43750/48549], Loss: 2.4548\n",
      "Epoch [1/1], Step [43760/48549], Loss: 2.4731\n",
      "Epoch [1/1], Step [43770/48549], Loss: 2.3864\n",
      "Epoch [1/1], Step [43780/48549], Loss: 2.4632\n",
      "Epoch [1/1], Step [43790/48549], Loss: 2.4120\n",
      "Epoch [1/1], Step [43800/48549], Loss: 2.4271\n",
      "Epoch [1/1], Step [43810/48549], Loss: 2.4396\n",
      "Epoch [1/1], Step [43820/48549], Loss: 2.5013\n",
      "Epoch [1/1], Step [43830/48549], Loss: 2.4738\n",
      "Epoch [1/1], Step [43840/48549], Loss: 2.4761\n",
      "Epoch [1/1], Step [43850/48549], Loss: 2.4204\n",
      "Epoch [1/1], Step [43860/48549], Loss: 2.4444\n",
      "Epoch [1/1], Step [43870/48549], Loss: 2.5261\n",
      "Epoch [1/1], Step [43880/48549], Loss: 2.5986\n",
      "Epoch [1/1], Step [43890/48549], Loss: 2.4366\n",
      "Epoch [1/1], Step [43900/48549], Loss: 2.3822\n",
      "Epoch [1/1], Step [43910/48549], Loss: 2.4526\n",
      "Epoch [1/1], Step [43920/48549], Loss: 2.4246\n",
      "Epoch [1/1], Step [43930/48549], Loss: 2.4339\n",
      "Epoch [1/1], Step [43940/48549], Loss: 2.4366\n",
      "Epoch [1/1], Step [43950/48549], Loss: 2.4497\n",
      "Epoch [1/1], Step [43960/48549], Loss: 2.4650\n",
      "Epoch [1/1], Step [43970/48549], Loss: 2.4532\n",
      "Epoch [1/1], Step [43980/48549], Loss: 2.4072\n",
      "Epoch [1/1], Step [43990/48549], Loss: 2.4517\n",
      "Epoch [1/1], Step [44000/48549], Loss: 2.4338\n",
      "Epoch [1/1], Step [44010/48549], Loss: 2.4278\n",
      "Epoch [1/1], Step [44020/48549], Loss: 2.4182\n",
      "Epoch [1/1], Step [44030/48549], Loss: 2.4769\n",
      "Epoch [1/1], Step [44040/48549], Loss: 2.4694\n",
      "Epoch [1/1], Step [44050/48549], Loss: 2.5681\n",
      "Epoch [1/1], Step [44060/48549], Loss: 2.4564\n",
      "Epoch [1/1], Step [44070/48549], Loss: 2.4616\n",
      "Epoch [1/1], Step [44080/48549], Loss: 2.4398\n",
      "Epoch [1/1], Step [44090/48549], Loss: 2.4574\n",
      "Epoch [1/1], Step [44100/48549], Loss: 2.4456\n",
      "Epoch [1/1], Step [44110/48549], Loss: 2.4574\n",
      "Epoch [1/1], Step [44120/48549], Loss: 2.5222\n",
      "Epoch [1/1], Step [44130/48549], Loss: 2.4838\n",
      "Epoch [1/1], Step [44140/48549], Loss: 2.5073\n",
      "Epoch [1/1], Step [44150/48549], Loss: 2.4970\n",
      "Epoch [1/1], Step [44160/48549], Loss: 2.3981\n",
      "Epoch [1/1], Step [44170/48549], Loss: 2.4879\n",
      "Epoch [1/1], Step [44180/48549], Loss: 2.4558\n",
      "Epoch [1/1], Step [44190/48549], Loss: 2.4577\n",
      "Epoch [1/1], Step [44200/48549], Loss: 2.4083\n",
      "Epoch [1/1], Step [44210/48549], Loss: 2.4217\n",
      "Epoch [1/1], Step [44220/48549], Loss: 2.3786\n",
      "Epoch [1/1], Step [44230/48549], Loss: 2.5427\n",
      "Epoch [1/1], Step [44240/48549], Loss: 2.4178\n",
      "Epoch [1/1], Step [44250/48549], Loss: 2.4890\n",
      "Epoch [1/1], Step [44260/48549], Loss: 2.4613\n",
      "Epoch [1/1], Step [44270/48549], Loss: 2.4533\n",
      "Epoch [1/1], Step [44280/48549], Loss: 2.5989\n",
      "Epoch [1/1], Step [44290/48549], Loss: 2.4755\n",
      "Epoch [1/1], Step [44300/48549], Loss: 2.4701\n",
      "Epoch [1/1], Step [44310/48549], Loss: 2.4541\n",
      "Epoch [1/1], Step [44320/48549], Loss: 2.4600\n",
      "Epoch [1/1], Step [44330/48549], Loss: 2.4323\n",
      "Epoch [1/1], Step [44340/48549], Loss: 2.4497\n",
      "Epoch [1/1], Step [44350/48549], Loss: 2.4496\n",
      "Epoch [1/1], Step [44360/48549], Loss: 2.4307\n",
      "Epoch [1/1], Step [44370/48549], Loss: 2.4173\n",
      "Epoch [1/1], Step [44380/48549], Loss: 2.4434\n",
      "Epoch [1/1], Step [44390/48549], Loss: 2.4398\n",
      "Epoch [1/1], Step [44400/48549], Loss: 2.4307\n",
      "Epoch [1/1], Step [44410/48549], Loss: 2.4475\n",
      "Epoch [1/1], Step [44420/48549], Loss: 2.4750\n",
      "Epoch [1/1], Step [44430/48549], Loss: 2.4768\n",
      "Epoch [1/1], Step [44440/48549], Loss: 2.4355\n",
      "Epoch [1/1], Step [44450/48549], Loss: 2.4820\n",
      "Epoch [1/1], Step [44460/48549], Loss: 2.4478\n",
      "Epoch [1/1], Step [44470/48549], Loss: 2.4216\n",
      "Epoch [1/1], Step [44480/48549], Loss: 2.5318\n",
      "Epoch [1/1], Step [44490/48549], Loss: 2.4311\n",
      "Epoch [1/1], Step [44500/48549], Loss: 2.4184\n",
      "Epoch [1/1], Step [44510/48549], Loss: 2.4094\n",
      "Epoch [1/1], Step [44520/48549], Loss: 2.4147\n",
      "Epoch [1/1], Step [44530/48549], Loss: 2.5646\n",
      "Epoch [1/1], Step [44540/48549], Loss: 2.4621\n",
      "Epoch [1/1], Step [44550/48549], Loss: 2.4359\n",
      "Epoch [1/1], Step [44560/48549], Loss: 2.4193\n",
      "Epoch [1/1], Step [44570/48549], Loss: 2.4533\n",
      "Epoch [1/1], Step [44580/48549], Loss: 2.4523\n",
      "Epoch [1/1], Step [44590/48549], Loss: 2.6011\n",
      "Epoch [1/1], Step [44600/48549], Loss: 2.4252\n",
      "Epoch [1/1], Step [44610/48549], Loss: 2.4179\n",
      "Epoch [1/1], Step [44620/48549], Loss: 2.4393\n",
      "Epoch [1/1], Step [44630/48549], Loss: 2.4472\n",
      "Epoch [1/1], Step [44640/48549], Loss: 2.4355\n",
      "Epoch [1/1], Step [44650/48549], Loss: 2.4977\n",
      "Epoch [1/1], Step [44660/48549], Loss: 2.4237\n",
      "Epoch [1/1], Step [44670/48549], Loss: 2.4336\n",
      "Epoch [1/1], Step [44680/48549], Loss: 2.4200\n",
      "Epoch [1/1], Step [44690/48549], Loss: 2.3972\n",
      "Epoch [1/1], Step [44700/48549], Loss: 2.4824\n",
      "Epoch [1/1], Step [44710/48549], Loss: 2.4370\n",
      "Epoch [1/1], Step [44720/48549], Loss: 2.4321\n",
      "Epoch [1/1], Step [44730/48549], Loss: 2.4495\n",
      "Epoch [1/1], Step [44740/48549], Loss: 2.4723\n",
      "Epoch [1/1], Step [44750/48549], Loss: 2.3771\n",
      "Epoch [1/1], Step [44760/48549], Loss: 2.4684\n",
      "Epoch [1/1], Step [44770/48549], Loss: 2.5029\n",
      "Epoch [1/1], Step [44780/48549], Loss: 2.5085\n",
      "Epoch [1/1], Step [44790/48549], Loss: 2.5018\n",
      "Epoch [1/1], Step [44800/48549], Loss: 2.4365\n",
      "Epoch [1/1], Step [44810/48549], Loss: 2.5436\n",
      "Epoch [1/1], Step [44820/48549], Loss: 2.4735\n",
      "Epoch [1/1], Step [44830/48549], Loss: 2.4438\n",
      "Epoch [1/1], Step [44840/48549], Loss: 2.4574\n",
      "Epoch [1/1], Step [44850/48549], Loss: 2.4197\n",
      "Epoch [1/1], Step [44860/48549], Loss: 2.4142\n",
      "Epoch [1/1], Step [44870/48549], Loss: 2.4417\n",
      "Epoch [1/1], Step [44880/48549], Loss: 2.5264\n",
      "Epoch [1/1], Step [44890/48549], Loss: 2.4211\n",
      "Epoch [1/1], Step [44900/48549], Loss: 2.5444\n",
      "Epoch [1/1], Step [44910/48549], Loss: 2.4202\n",
      "Epoch [1/1], Step [44920/48549], Loss: 2.5089\n",
      "Epoch [1/1], Step [44930/48549], Loss: 2.4976\n",
      "Epoch [1/1], Step [44940/48549], Loss: 2.4010\n",
      "Epoch [1/1], Step [44950/48549], Loss: 2.4443\n",
      "Epoch [1/1], Step [44960/48549], Loss: 2.4901\n",
      "Epoch [1/1], Step [44970/48549], Loss: 2.4718\n",
      "Epoch [1/1], Step [44980/48549], Loss: 2.4721\n",
      "Epoch [1/1], Step [44990/48549], Loss: 2.4706\n",
      "Epoch [1/1], Step [45000/48549], Loss: 2.3869\n",
      "Epoch [1/1], Step [45010/48549], Loss: 2.4950\n",
      "Epoch [1/1], Step [45020/48549], Loss: 2.3831\n",
      "Epoch [1/1], Step [45030/48549], Loss: 2.3856\n",
      "Epoch [1/1], Step [45040/48549], Loss: 2.4565\n",
      "Epoch [1/1], Step [45050/48549], Loss: 2.4201\n",
      "Epoch [1/1], Step [45060/48549], Loss: 2.4889\n",
      "Epoch [1/1], Step [45070/48549], Loss: 2.4418\n",
      "Epoch [1/1], Step [45080/48549], Loss: 2.4746\n",
      "Epoch [1/1], Step [45090/48549], Loss: 2.4105\n",
      "Epoch [1/1], Step [45100/48549], Loss: 2.4563\n",
      "Epoch [1/1], Step [45110/48549], Loss: 2.3962\n",
      "Epoch [1/1], Step [45120/48549], Loss: 2.4442\n",
      "Epoch [1/1], Step [45130/48549], Loss: 2.4283\n",
      "Epoch [1/1], Step [45140/48549], Loss: 2.4416\n",
      "Epoch [1/1], Step [45150/48549], Loss: 2.4786\n",
      "Epoch [1/1], Step [45160/48549], Loss: 2.4093\n",
      "Epoch [1/1], Step [45170/48549], Loss: 2.4359\n",
      "Epoch [1/1], Step [45180/48549], Loss: 2.4547\n",
      "Epoch [1/1], Step [45190/48549], Loss: 2.4522\n",
      "Epoch [1/1], Step [45200/48549], Loss: 2.4557\n",
      "Epoch [1/1], Step [45210/48549], Loss: 2.4008\n",
      "Epoch [1/1], Step [45220/48549], Loss: 2.3965\n",
      "Epoch [1/1], Step [45230/48549], Loss: 2.4388\n",
      "Epoch [1/1], Step [45240/48549], Loss: 2.4345\n",
      "Epoch [1/1], Step [45250/48549], Loss: 2.4471\n",
      "Epoch [1/1], Step [45260/48549], Loss: 2.4472\n",
      "Epoch [1/1], Step [45270/48549], Loss: 2.4890\n",
      "Epoch [1/1], Step [45280/48549], Loss: 2.4169\n",
      "Epoch [1/1], Step [45290/48549], Loss: 2.4617\n",
      "Epoch [1/1], Step [45300/48549], Loss: 2.4431\n",
      "Epoch [1/1], Step [45310/48549], Loss: 2.4419\n",
      "Epoch [1/1], Step [45320/48549], Loss: 2.4218\n",
      "Epoch [1/1], Step [45330/48549], Loss: 2.4519\n",
      "Epoch [1/1], Step [45340/48549], Loss: 2.4187\n",
      "Epoch [1/1], Step [45350/48549], Loss: 2.4442\n",
      "Epoch [1/1], Step [45360/48549], Loss: 2.4891\n",
      "Epoch [1/1], Step [45370/48549], Loss: 2.3881\n",
      "Epoch [1/1], Step [45380/48549], Loss: 2.4694\n",
      "Epoch [1/1], Step [45390/48549], Loss: 2.4640\n",
      "Epoch [1/1], Step [45400/48549], Loss: 2.4209\n",
      "Epoch [1/1], Step [45410/48549], Loss: 2.4229\n",
      "Epoch [1/1], Step [45420/48549], Loss: 2.4487\n",
      "Epoch [1/1], Step [45430/48549], Loss: 2.4883\n",
      "Epoch [1/1], Step [45440/48549], Loss: 2.4396\n",
      "Epoch [1/1], Step [45450/48549], Loss: 2.4372\n",
      "Epoch [1/1], Step [45460/48549], Loss: 2.4500\n",
      "Epoch [1/1], Step [45470/48549], Loss: 2.4644\n",
      "Epoch [1/1], Step [45480/48549], Loss: 2.3844\n",
      "Epoch [1/1], Step [45490/48549], Loss: 2.3915\n",
      "Epoch [1/1], Step [45500/48549], Loss: 2.4682\n",
      "Epoch [1/1], Step [45510/48549], Loss: 2.4183\n",
      "Epoch [1/1], Step [45520/48549], Loss: 2.4889\n",
      "Epoch [1/1], Step [45530/48549], Loss: 2.4208\n",
      "Epoch [1/1], Step [45540/48549], Loss: 2.4424\n",
      "Epoch [1/1], Step [45550/48549], Loss: 2.4651\n",
      "Epoch [1/1], Step [45560/48549], Loss: 2.6665\n",
      "Epoch [1/1], Step [45570/48549], Loss: 2.5027\n",
      "Epoch [1/1], Step [45580/48549], Loss: 2.4128\n",
      "Epoch [1/1], Step [45590/48549], Loss: 2.4311\n",
      "Epoch [1/1], Step [45600/48549], Loss: 2.4782\n",
      "Epoch [1/1], Step [45610/48549], Loss: 2.4722\n",
      "Epoch [1/1], Step [45620/48549], Loss: 2.4292\n",
      "Epoch [1/1], Step [45630/48549], Loss: 2.3952\n",
      "Epoch [1/1], Step [45640/48549], Loss: 2.4517\n",
      "Epoch [1/1], Step [45650/48549], Loss: 2.5075\n",
      "Epoch [1/1], Step [45660/48549], Loss: 2.4425\n",
      "Epoch [1/1], Step [45670/48549], Loss: 2.5094\n",
      "Epoch [1/1], Step [45680/48549], Loss: 2.4139\n",
      "Epoch [1/1], Step [45690/48549], Loss: 2.4658\n",
      "Epoch [1/1], Step [45700/48549], Loss: 2.4417\n",
      "Epoch [1/1], Step [45710/48549], Loss: 2.4294\n",
      "Epoch [1/1], Step [45720/48549], Loss: 2.4778\n",
      "Epoch [1/1], Step [45730/48549], Loss: 2.4578\n",
      "Epoch [1/1], Step [45740/48549], Loss: 2.5081\n",
      "Epoch [1/1], Step [45750/48549], Loss: 2.4418\n",
      "Epoch [1/1], Step [45760/48549], Loss: 2.4865\n",
      "Epoch [1/1], Step [45770/48549], Loss: 2.4843\n",
      "Epoch [1/1], Step [45780/48549], Loss: 2.5029\n",
      "Epoch [1/1], Step [45790/48549], Loss: 2.4315\n",
      "Epoch [1/1], Step [45800/48549], Loss: 2.4389\n",
      "Epoch [1/1], Step [45810/48549], Loss: 2.4344\n",
      "Epoch [1/1], Step [45820/48549], Loss: 2.5227\n",
      "Epoch [1/1], Step [45830/48549], Loss: 2.4471\n",
      "Epoch [1/1], Step [45840/48549], Loss: 2.4157\n",
      "Epoch [1/1], Step [45850/48549], Loss: 2.4146\n",
      "Epoch [1/1], Step [45860/48549], Loss: 2.4131\n",
      "Epoch [1/1], Step [45870/48549], Loss: 2.4092\n",
      "Epoch [1/1], Step [45880/48549], Loss: 2.4711\n",
      "Epoch [1/1], Step [45890/48549], Loss: 2.4475\n",
      "Epoch [1/1], Step [45900/48549], Loss: 2.4145\n",
      "Epoch [1/1], Step [45910/48549], Loss: 2.5214\n",
      "Epoch [1/1], Step [45920/48549], Loss: 2.4712\n",
      "Epoch [1/1], Step [45930/48549], Loss: 2.4328\n",
      "Epoch [1/1], Step [45940/48549], Loss: 2.4466\n",
      "Epoch [1/1], Step [45950/48549], Loss: 2.4573\n",
      "Epoch [1/1], Step [45960/48549], Loss: 2.4594\n",
      "Epoch [1/1], Step [45970/48549], Loss: 2.4609\n",
      "Epoch [1/1], Step [45980/48549], Loss: 2.3971\n",
      "Epoch [1/1], Step [45990/48549], Loss: 2.4526\n",
      "Epoch [1/1], Step [46000/48549], Loss: 2.4399\n",
      "Epoch [1/1], Step [46010/48549], Loss: 2.4758\n",
      "Epoch [1/1], Step [46020/48549], Loss: 2.4838\n",
      "Epoch [1/1], Step [46030/48549], Loss: 2.4430\n",
      "Epoch [1/1], Step [46040/48549], Loss: 2.5942\n",
      "Epoch [1/1], Step [46050/48549], Loss: 2.4186\n",
      "Epoch [1/1], Step [46060/48549], Loss: 2.4553\n",
      "Epoch [1/1], Step [46070/48549], Loss: 2.3807\n",
      "Epoch [1/1], Step [46080/48549], Loss: 2.4090\n",
      "Epoch [1/1], Step [46090/48549], Loss: 2.4281\n",
      "Epoch [1/1], Step [46100/48549], Loss: 2.4352\n",
      "Epoch [1/1], Step [46110/48549], Loss: 2.4430\n",
      "Epoch [1/1], Step [46120/48549], Loss: 2.4985\n",
      "Epoch [1/1], Step [46130/48549], Loss: 2.3955\n",
      "Epoch [1/1], Step [46140/48549], Loss: 2.4680\n",
      "Epoch [1/1], Step [46150/48549], Loss: 2.3871\n",
      "Epoch [1/1], Step [46160/48549], Loss: 2.3905\n",
      "Epoch [1/1], Step [46170/48549], Loss: 2.5186\n",
      "Epoch [1/1], Step [46180/48549], Loss: 2.4344\n",
      "Epoch [1/1], Step [46190/48549], Loss: 2.4835\n",
      "Epoch [1/1], Step [46200/48549], Loss: 2.4483\n",
      "Epoch [1/1], Step [46210/48549], Loss: 2.4615\n",
      "Epoch [1/1], Step [46220/48549], Loss: 2.4284\n",
      "Epoch [1/1], Step [46230/48549], Loss: 2.4946\n",
      "Epoch [1/1], Step [46240/48549], Loss: 2.4327\n",
      "Epoch [1/1], Step [46250/48549], Loss: 2.4929\n",
      "Epoch [1/1], Step [46260/48549], Loss: 2.5013\n",
      "Epoch [1/1], Step [46270/48549], Loss: 2.4144\n",
      "Epoch [1/1], Step [46280/48549], Loss: 2.3573\n",
      "Epoch [1/1], Step [46290/48549], Loss: 2.4399\n",
      "Epoch [1/1], Step [46300/48549], Loss: 2.4917\n",
      "Epoch [1/1], Step [46310/48549], Loss: 2.4726\n",
      "Epoch [1/1], Step [46320/48549], Loss: 2.4746\n",
      "Epoch [1/1], Step [46330/48549], Loss: 2.4017\n",
      "Epoch [1/1], Step [46340/48549], Loss: 2.4494\n",
      "Epoch [1/1], Step [46350/48549], Loss: 2.4336\n",
      "Epoch [1/1], Step [46360/48549], Loss: 2.4666\n",
      "Epoch [1/1], Step [46370/48549], Loss: 2.4304\n",
      "Epoch [1/1], Step [46380/48549], Loss: 2.4956\n",
      "Epoch [1/1], Step [46390/48549], Loss: 2.5639\n",
      "Epoch [1/1], Step [46400/48549], Loss: 2.4655\n",
      "Epoch [1/1], Step [46410/48549], Loss: 2.4411\n",
      "Epoch [1/1], Step [46420/48549], Loss: 2.4615\n",
      "Epoch [1/1], Step [46430/48549], Loss: 2.4271\n",
      "Epoch [1/1], Step [46440/48549], Loss: 2.4419\n",
      "Epoch [1/1], Step [46450/48549], Loss: 2.4574\n",
      "Epoch [1/1], Step [46460/48549], Loss: 2.5478\n",
      "Epoch [1/1], Step [46470/48549], Loss: 2.4714\n",
      "Epoch [1/1], Step [46480/48549], Loss: 2.4298\n",
      "Epoch [1/1], Step [46490/48549], Loss: 2.3963\n",
      "Epoch [1/1], Step [46500/48549], Loss: 2.4560\n",
      "Epoch [1/1], Step [46510/48549], Loss: 2.4665\n",
      "Epoch [1/1], Step [46520/48549], Loss: 2.4202\n",
      "Epoch [1/1], Step [46530/48549], Loss: 2.4613\n",
      "Epoch [1/1], Step [46540/48549], Loss: 2.4549\n",
      "Epoch [1/1], Step [46550/48549], Loss: 2.4158\n",
      "Epoch [1/1], Step [46560/48549], Loss: 2.4630\n",
      "Epoch [1/1], Step [46570/48549], Loss: 2.4268\n",
      "Epoch [1/1], Step [46580/48549], Loss: 2.5032\n",
      "Epoch [1/1], Step [46590/48549], Loss: 2.4391\n",
      "Epoch [1/1], Step [46600/48549], Loss: 2.4221\n",
      "Epoch [1/1], Step [46610/48549], Loss: 2.4806\n",
      "Epoch [1/1], Step [46620/48549], Loss: 2.3979\n",
      "Epoch [1/1], Step [46630/48549], Loss: 2.4541\n",
      "Epoch [1/1], Step [46640/48549], Loss: 2.4096\n",
      "Epoch [1/1], Step [46650/48549], Loss: 2.4243\n",
      "Epoch [1/1], Step [46660/48549], Loss: 2.4488\n",
      "Epoch [1/1], Step [46670/48549], Loss: 2.4471\n",
      "Epoch [1/1], Step [46680/48549], Loss: 2.4827\n",
      "Epoch [1/1], Step [46690/48549], Loss: 2.4555\n",
      "Epoch [1/1], Step [46700/48549], Loss: 2.4392\n",
      "Epoch [1/1], Step [46710/48549], Loss: 2.4731\n",
      "Epoch [1/1], Step [46720/48549], Loss: 2.4325\n",
      "Epoch [1/1], Step [46730/48549], Loss: 2.5841\n",
      "Epoch [1/1], Step [46740/48549], Loss: 2.4656\n",
      "Epoch [1/1], Step [46750/48549], Loss: 2.4264\n",
      "Epoch [1/1], Step [46760/48549], Loss: 2.4635\n",
      "Epoch [1/1], Step [46770/48549], Loss: 2.5145\n",
      "Epoch [1/1], Step [46780/48549], Loss: 2.4612\n",
      "Epoch [1/1], Step [46790/48549], Loss: 2.4408\n",
      "Epoch [1/1], Step [46800/48549], Loss: 2.4242\n",
      "Epoch [1/1], Step [46810/48549], Loss: 2.4252\n",
      "Epoch [1/1], Step [46820/48549], Loss: 2.4497\n",
      "Epoch [1/1], Step [46830/48549], Loss: 2.4878\n",
      "Epoch [1/1], Step [46840/48549], Loss: 2.5195\n",
      "Epoch [1/1], Step [46850/48549], Loss: 2.4732\n",
      "Epoch [1/1], Step [46860/48549], Loss: 2.4450\n",
      "Epoch [1/1], Step [46870/48549], Loss: 2.4653\n",
      "Epoch [1/1], Step [46880/48549], Loss: 2.4258\n",
      "Epoch [1/1], Step [46890/48549], Loss: 2.4282\n",
      "Epoch [1/1], Step [46900/48549], Loss: 2.4732\n",
      "Epoch [1/1], Step [46910/48549], Loss: 2.4338\n",
      "Epoch [1/1], Step [46920/48549], Loss: 2.4607\n",
      "Epoch [1/1], Step [46930/48549], Loss: 2.4848\n",
      "Epoch [1/1], Step [46940/48549], Loss: 2.3857\n",
      "Epoch [1/1], Step [46950/48549], Loss: 2.4836\n",
      "Epoch [1/1], Step [46960/48549], Loss: 2.4694\n",
      "Epoch [1/1], Step [46970/48549], Loss: 2.4537\n",
      "Epoch [1/1], Step [46980/48549], Loss: 2.4104\n",
      "Epoch [1/1], Step [46990/48549], Loss: 2.4541\n",
      "Epoch [1/1], Step [47000/48549], Loss: 2.4680\n",
      "Epoch [1/1], Step [47010/48549], Loss: 2.4021\n",
      "Epoch [1/1], Step [47020/48549], Loss: 2.4554\n",
      "Epoch [1/1], Step [47030/48549], Loss: 2.4407\n",
      "Epoch [1/1], Step [47040/48549], Loss: 2.4010\n",
      "Epoch [1/1], Step [47050/48549], Loss: 2.4124\n",
      "Epoch [1/1], Step [47060/48549], Loss: 2.4145\n",
      "Epoch [1/1], Step [47070/48549], Loss: 2.4785\n",
      "Epoch [1/1], Step [47080/48549], Loss: 2.4315\n",
      "Epoch [1/1], Step [47090/48549], Loss: 2.4774\n",
      "Epoch [1/1], Step [47100/48549], Loss: 2.4589\n",
      "Epoch [1/1], Step [47110/48549], Loss: 2.4188\n",
      "Epoch [1/1], Step [47120/48549], Loss: 2.4211\n",
      "Epoch [1/1], Step [47130/48549], Loss: 2.4158\n",
      "Epoch [1/1], Step [47140/48549], Loss: 2.4670\n",
      "Epoch [1/1], Step [47150/48549], Loss: 2.4013\n",
      "Epoch [1/1], Step [47160/48549], Loss: 2.4172\n",
      "Epoch [1/1], Step [47170/48549], Loss: 2.4378\n",
      "Epoch [1/1], Step [47180/48549], Loss: 2.4427\n",
      "Epoch [1/1], Step [47190/48549], Loss: 2.4010\n",
      "Epoch [1/1], Step [47200/48549], Loss: 2.4708\n",
      "Epoch [1/1], Step [47210/48549], Loss: 2.4859\n",
      "Epoch [1/1], Step [47220/48549], Loss: 2.4094\n",
      "Epoch [1/1], Step [47230/48549], Loss: 2.5013\n",
      "Epoch [1/1], Step [47240/48549], Loss: 2.4518\n",
      "Epoch [1/1], Step [47250/48549], Loss: 2.4120\n",
      "Epoch [1/1], Step [47260/48549], Loss: 2.4095\n",
      "Epoch [1/1], Step [47270/48549], Loss: 2.4704\n",
      "Epoch [1/1], Step [47280/48549], Loss: 2.4811\n",
      "Epoch [1/1], Step [47290/48549], Loss: 2.4343\n",
      "Epoch [1/1], Step [47300/48549], Loss: 2.4159\n",
      "Epoch [1/1], Step [47310/48549], Loss: 2.4546\n",
      "Epoch [1/1], Step [47320/48549], Loss: 2.4523\n",
      "Epoch [1/1], Step [47330/48549], Loss: 2.4698\n",
      "Epoch [1/1], Step [47340/48549], Loss: 2.4245\n",
      "Epoch [1/1], Step [47350/48549], Loss: 2.5041\n",
      "Epoch [1/1], Step [47360/48549], Loss: 2.4192\n",
      "Epoch [1/1], Step [47370/48549], Loss: 2.4728\n",
      "Epoch [1/1], Step [47380/48549], Loss: 2.4257\n",
      "Epoch [1/1], Step [47390/48549], Loss: 2.4508\n",
      "Epoch [1/1], Step [47400/48549], Loss: 2.4332\n",
      "Epoch [1/1], Step [47410/48549], Loss: 2.4893\n",
      "Epoch [1/1], Step [47420/48549], Loss: 2.4900\n",
      "Epoch [1/1], Step [47430/48549], Loss: 2.4516\n",
      "Epoch [1/1], Step [47440/48549], Loss: 2.6216\n",
      "Epoch [1/1], Step [47450/48549], Loss: 2.3846\n",
      "Epoch [1/1], Step [47460/48549], Loss: 2.3925\n",
      "Epoch [1/1], Step [47470/48549], Loss: 2.4181\n",
      "Epoch [1/1], Step [47480/48549], Loss: 2.3925\n",
      "Epoch [1/1], Step [47490/48549], Loss: 2.3659\n",
      "Epoch [1/1], Step [47500/48549], Loss: 2.4414\n",
      "Epoch [1/1], Step [47510/48549], Loss: 2.4135\n",
      "Epoch [1/1], Step [47520/48549], Loss: 2.4253\n",
      "Epoch [1/1], Step [47530/48549], Loss: 2.4567\n",
      "Epoch [1/1], Step [47540/48549], Loss: 2.4780\n",
      "Epoch [1/1], Step [47550/48549], Loss: 2.4296\n",
      "Epoch [1/1], Step [47560/48549], Loss: 2.5032\n",
      "Epoch [1/1], Step [47570/48549], Loss: 2.3910\n",
      "Epoch [1/1], Step [47580/48549], Loss: 2.4454\n",
      "Epoch [1/1], Step [47590/48549], Loss: 2.4705\n",
      "Epoch [1/1], Step [47600/48549], Loss: 2.4348\n",
      "Epoch [1/1], Step [47610/48549], Loss: 2.4122\n",
      "Epoch [1/1], Step [47620/48549], Loss: 2.4395\n",
      "Epoch [1/1], Step [47630/48549], Loss: 2.4208\n",
      "Epoch [1/1], Step [47640/48549], Loss: 2.4606\n",
      "Epoch [1/1], Step [47650/48549], Loss: 2.4067\n",
      "Epoch [1/1], Step [47660/48549], Loss: 2.3847\n",
      "Epoch [1/1], Step [47670/48549], Loss: 2.5046\n",
      "Epoch [1/1], Step [47680/48549], Loss: 2.4479\n",
      "Epoch [1/1], Step [47690/48549], Loss: 2.4846\n",
      "Epoch [1/1], Step [47700/48549], Loss: 2.4040\n",
      "Epoch [1/1], Step [47710/48549], Loss: 2.4951\n",
      "Epoch [1/1], Step [47720/48549], Loss: 2.4646\n",
      "Epoch [1/1], Step [47730/48549], Loss: 2.3900\n",
      "Epoch [1/1], Step [47740/48549], Loss: 2.4414\n",
      "Epoch [1/1], Step [47750/48549], Loss: 2.4469\n",
      "Epoch [1/1], Step [47760/48549], Loss: 2.4390\n",
      "Epoch [1/1], Step [47770/48549], Loss: 2.4638\n",
      "Epoch [1/1], Step [47780/48549], Loss: 2.4606\n",
      "Epoch [1/1], Step [47790/48549], Loss: 2.5630\n",
      "Epoch [1/1], Step [47800/48549], Loss: 2.4194\n",
      "Epoch [1/1], Step [47810/48549], Loss: 2.4452\n",
      "Epoch [1/1], Step [47820/48549], Loss: 2.3987\n",
      "Epoch [1/1], Step [47830/48549], Loss: 2.4522\n",
      "Epoch [1/1], Step [47840/48549], Loss: 2.4409\n",
      "Epoch [1/1], Step [47850/48549], Loss: 2.4164\n",
      "Epoch [1/1], Step [47860/48549], Loss: 2.4688\n",
      "Epoch [1/1], Step [47870/48549], Loss: 2.4659\n",
      "Epoch [1/1], Step [47880/48549], Loss: 2.5101\n",
      "Epoch [1/1], Step [47890/48549], Loss: 2.4207\n",
      "Epoch [1/1], Step [47900/48549], Loss: 2.4594\n",
      "Epoch [1/1], Step [47910/48549], Loss: 2.4496\n",
      "Epoch [1/1], Step [47920/48549], Loss: 2.4050\n",
      "Epoch [1/1], Step [47930/48549], Loss: 2.4329\n",
      "Epoch [1/1], Step [47940/48549], Loss: 2.4209\n",
      "Epoch [1/1], Step [47950/48549], Loss: 2.4236\n",
      "Epoch [1/1], Step [47960/48549], Loss: 2.4141\n",
      "Epoch [1/1], Step [47970/48549], Loss: 2.4005\n",
      "Epoch [1/1], Step [47980/48549], Loss: 2.3499\n",
      "Epoch [1/1], Step [47990/48549], Loss: 2.4835\n",
      "Epoch [1/1], Step [48000/48549], Loss: 2.4337\n",
      "Epoch [1/1], Step [48010/48549], Loss: 2.4615\n",
      "Epoch [1/1], Step [48020/48549], Loss: 2.4385\n",
      "Epoch [1/1], Step [48030/48549], Loss: 2.4347\n",
      "Epoch [1/1], Step [48040/48549], Loss: 2.4376\n",
      "Epoch [1/1], Step [48050/48549], Loss: 2.4748\n",
      "Epoch [1/1], Step [48060/48549], Loss: 2.4164\n",
      "Epoch [1/1], Step [48070/48549], Loss: 2.4437\n",
      "Epoch [1/1], Step [48080/48549], Loss: 2.3770\n",
      "Epoch [1/1], Step [48090/48549], Loss: 2.4255\n",
      "Epoch [1/1], Step [48100/48549], Loss: 2.4493\n",
      "Epoch [1/1], Step [48110/48549], Loss: 2.4945\n",
      "Epoch [1/1], Step [48120/48549], Loss: 2.5012\n",
      "Epoch [1/1], Step [48130/48549], Loss: 2.4138\n",
      "Epoch [1/1], Step [48140/48549], Loss: 2.4747\n",
      "Epoch [1/1], Step [48150/48549], Loss: 2.4518\n",
      "Epoch [1/1], Step [48160/48549], Loss: 2.4287\n",
      "Epoch [1/1], Step [48170/48549], Loss: 2.4977\n",
      "Epoch [1/1], Step [48180/48549], Loss: 2.4565\n",
      "Epoch [1/1], Step [48190/48549], Loss: 2.5355\n",
      "Epoch [1/1], Step [48200/48549], Loss: 2.5225\n",
      "Epoch [1/1], Step [48210/48549], Loss: 2.4157\n",
      "Epoch [1/1], Step [48220/48549], Loss: 2.4083\n",
      "Epoch [1/1], Step [48230/48549], Loss: 2.3799\n",
      "Epoch [1/1], Step [48240/48549], Loss: 2.4759\n",
      "Epoch [1/1], Step [48250/48549], Loss: 2.4590\n",
      "Epoch [1/1], Step [48260/48549], Loss: 2.4240\n",
      "Epoch [1/1], Step [48270/48549], Loss: 2.4350\n",
      "Epoch [1/1], Step [48280/48549], Loss: 2.4578\n",
      "Epoch [1/1], Step [48290/48549], Loss: 2.4178\n",
      "Epoch [1/1], Step [48300/48549], Loss: 2.4633\n",
      "Epoch [1/1], Step [48310/48549], Loss: 2.4687\n",
      "Epoch [1/1], Step [48320/48549], Loss: 2.4657\n",
      "Epoch [1/1], Step [48330/48549], Loss: 2.4606\n",
      "Epoch [1/1], Step [48340/48549], Loss: 2.4776\n",
      "Epoch [1/1], Step [48350/48549], Loss: 2.4561\n",
      "Epoch [1/1], Step [48360/48549], Loss: 2.4404\n",
      "Epoch [1/1], Step [48370/48549], Loss: 2.4512\n",
      "Epoch [1/1], Step [48380/48549], Loss: 2.4515\n",
      "Epoch [1/1], Step [48390/48549], Loss: 2.4795\n",
      "Epoch [1/1], Step [48400/48549], Loss: 2.4369\n",
      "Epoch [1/1], Step [48410/48549], Loss: 2.4673\n",
      "Epoch [1/1], Step [48420/48549], Loss: 2.3894\n",
      "Epoch [1/1], Step [48430/48549], Loss: 2.4860\n",
      "Epoch [1/1], Step [48440/48549], Loss: 2.5988\n",
      "Epoch [1/1], Step [48450/48549], Loss: 2.4560\n",
      "Epoch [1/1], Step [48460/48549], Loss: 2.4393\n",
      "Epoch [1/1], Step [48470/48549], Loss: 2.4827\n",
      "Epoch [1/1], Step [48480/48549], Loss: 2.4318\n",
      "Epoch [1/1], Step [48490/48549], Loss: 2.5266\n",
      "Epoch [1/1], Step [48500/48549], Loss: 2.4262\n",
      "Epoch [1/1], Step [48510/48549], Loss: 2.4394\n",
      "Epoch [1/1], Step [48520/48549], Loss: 2.4699\n",
      "Epoch [1/1], Step [48530/48549], Loss: 2.5084\n",
      "Epoch [1/1], Step [48540/48549], Loss: 2.4163\n",
      "Epoch [1/1], Average Loss: 2.4510\n"
     ]
    }
   ],
   "source": [
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_hidden_dim, max_seq_len, dropout=0.1):\n",
    "        super(TransformerLanguageModel, self).__init__()\n",
    "\n",
    "        # Token and positional embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "\n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_hidden_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Final output projection\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_len = input_ids.size(1)\n",
    "        device = input_ids.device\n",
    "\n",
    "        # Create position IDs (0 to seq_len-1)\n",
    "        position_ids = torch.arange(seq_len, device=device).unsqueeze(0).expand_as(input_ids)\n",
    "\n",
    "        # Token and position embeddings\n",
    "        token_embeddings = self.token_embedding(input_ids)\n",
    "        position_embeddings = self.position_embedding(position_ids)\n",
    "        embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "        # Pass through the Transformer encoder\n",
    "        transformer_output = self.transformer_encoder(embeddings)\n",
    "\n",
    "        # Project to vocab size for next-token predictions\n",
    "        logits = self.output_layer(transformer_output)\n",
    "        return logits\n",
    "\n",
    "# Preparing the text dataset\n",
    "text_file = \"C:\\\\Users\\\\dhuma\\\\Desktop\\\\book.txt\"\n",
    "with open(text_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Building a vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab = {char: idx for idx, char in enumerate(chars)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Hyperparameters\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "ff_hidden_dim = 100\n",
    "max_seq_len = 100\n",
    "dropout = 0.1\n",
    "seq_len = max_seq_len\n",
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Hyperparameters\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "ff_hidden_dim = 100\n",
    "max_seq_len = 100\n",
    "dropout = 0.1\n",
    "seq_len = max_seq_len\n",
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Prepare dataset and dataloader\n",
    "dataset = TextDataset(text, seq_len, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    ff_hidden_dim=ff_hidden_dim,\n",
    "    max_seq_len=max_seq_len,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "\n",
    "        # Reshape logits and targets for loss computation\n",
    "        logits = logits.view(-1, vocab_size)\n",
    "        y = y.view(-1)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / len(dataloader):.4f}\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3047a90a-2178-4fd6-9991-e346e4c26e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "hey how are you its me shivam  t  htne t  stn tte tt n nett tt    t ttt  t   t  t t    t  tttt t   tt ttt tt t tttt tt    t ttt  \n"
     ]
    }
   ],
   "source": [
    "# Text generation function\n",
    "def generate_text(model, vocab, max_len, start_text, device='cpu'):\n",
    "    model.eval()\n",
    "    idx_to_char = {idx: char for char, idx in vocab.items()}\n",
    "    input_ids = [vocab[char] for char in start_text if char in vocab]\n",
    "\n",
    "    input_ids = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    generated_sequence = input_ids.tolist()[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(input_ids)\n",
    "            predicted_id = torch.argmax(output[:, 1, :], dim=1).item()\n",
    "            generated_sequence.append(predicted_id)\n",
    "            input_ids = torch.tensor([generated_sequence[-len(input_ids[0]):]]).to(device)\n",
    "\n",
    "    generated_text = ''.join([idx_to_char[i] for i in generated_sequence])\n",
    "    return generated_text\n",
    "\n",
    "# Generate text\n",
    "start_text = \"hey how are you its me shivam\"\n",
    "generated_text = generate_text(model, vocab, max_len=100, start_text=start_text)\n",
    "print(f\"Generated Text:\\n{generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8ca5ef-1295-49da-9c56-915978998240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe62fe1d-74a6-4cba-b609-d2121e3a39b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab1b141-4479-42a8-b37f-89f3c8121f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d6474a-9793-46b7-a1bf-654963995958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
